2025/06/23 16:49:36 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: win32
    Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 6592899
    GPU 0: NVIDIA GeForce GTX 1660
    CUDA_HOME: C:\Users\Sagi\Miniconda3\envs\gcp-env\Library
    NVCC: Not Available
    MSVC: Оптимизирующий компилятор Microsoft (R) C/C++ версии 19.43.34810 для x64
    GCC: n/a
    PyTorch: 2.3.0
    PyTorch compiling details: PyTorch built with:
  - C++ Version: 201703
  - MSVC 192930151
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.8.1  (built against CUDA 12.0)
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.8.1, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.18.0
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 6592899
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/06/23 16:49:37 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=2, enable=True)
backend_args = None
batch_augments = [
    dict(
        img_pad_value=0,
        mask_pad_value=0,
        pad_mask=True,
        pad_seg=True,
        seg_pad_value=255,
        size=(
            512,
            512,
        ),
        type='BatchFixedSizePad'),
]
crop_size = (
    512,
    512,
)
data_preprocessor = dict(
    batch_augments=[
        dict(
            img_pad_value=0,
            mask_pad_value=0,
            pad_mask=True,
            pad_seg=True,
            seg_pad_value=255,
            size=(
                512,
                512,
            ),
            type='BatchFixedSizePad'),
    ],
    bgr_to_rgb=True,
    mask_pad_value=0,
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    pad_mask=True,
    pad_seg=True,
    pad_size_divisor=32,
    seg_pad_value=255,
    std=[
        58.395,
        57.12,
        57.375,
    ],
    type='DetDataPreprocessor')
data_root = 'data/kostanai'
dataset_type = 'WHUMixVectorDataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=True,
        interval=1,
        max_keep_ckpts=1,
        save_last=True,
        type='CheckpointHook'),
    logger=dict(interval=10, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(draw=True, interval=10, type='TanmlhVisualizationHook'))
default_scope = 'mmdet'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
img_norm_cfg = dict(
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    std=[
        58.395,
        57.12,
        57.375,
    ],
    to_rgb=True)
launcher = 'none'
load_from = 'checkpoints/mask2former_r50_pretrained_50e_whu-mix-vector.pth'
log_config = dict(hooks=[
    dict(type='TextLoggerHook'),
    dict(
        init_kwargs=dict(
            allow_val_change=True,
            group='mask2former_training',
            name='mask2former_e15_lre-4_kostanai_afs',
            project='building-segmentation-gcp',
            resume='never'),
        interval=10,
        log_checkpoint=True,
        log_checkpoint_metadata=True,
        num_eval_images=10,
        type='MMDetWandbHook'),
])
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=10)
max_epochs = 10
model = dict(
    backbone=dict(
        depth=50,
        frozen_stages=-1,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=False, type='BN'),
        norm_eval=True,
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mask_pad_value=0,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=True,
        pad_seg=True,
        pad_size_divisor=32,
        seg_pad_value=255,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    init_cfg=None,
    panoptic_fusion_head=dict(
        init_cfg=None,
        loss_panoptic=None,
        num_stuff_classes=0,
        num_things_classes=1,
        type='MaskFormerFusionHead'),
    panoptic_head=dict(
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=True),
        num_queries=300,
        num_stuff_classes=0,
        num_things_classes=1,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(
        filter_low_score=False,
        instance_on=True,
        iou_thr=0.8,
        max_per_image=200,
        panoptic_on=False,
        semantic_on=False),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='ClassificationCost', weight=2.0),
                dict(
                    type='CrossEntropyLossCost', use_sigmoid=True, weight=5.0),
                dict(eps=1.0, pred_act=True, type='DiceCost', weight=5.0),
            ],
            type='HungarianAssigner'),
        importance_sample_ratio=0.75,
        num_points=12544,
        oversample_ratio=3.0,
        sampler=dict(type='MaskPseudoSampler')),
    type='Mask2Former')
num_classes = 1
num_stuff_classes = 0
num_things_classes = 1
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(decay_mult=1.0, lr_mult=0.1),
            level_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_feat=dict(decay_mult=0.0, lr_mult=1.0)),
        norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0, by_epoch=False, end=1000, start_factor=0.001,
        type='LinearLR'),
    dict(
        begin=0,
        by_epoch=True,
        end=10,
        gamma=0.1,
        milestones=[
            40,
        ],
        type='MultiStepLR'),
]
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='test/test.json',
        backend_args=None,
        data_prefix=dict(img='test/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=1,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = [
    dict(
        ann_file='data/kostanai/test/test.json',
        backend_args=None,
        metric=[
            'segm',
        ],
        type='CocoMetric'),
]
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=False,
        with_mask=True,
        with_poly_json=False),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=10, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=1,
    dataset=dict(
        ann_file='train/train.json',
        backend_args=None,
        data_prefix=dict(img='train/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True,
                with_poly_json=False),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                direction=[
                    'horizontal',
                    'vertical',
                    'diagonal',
                ],
                prob=0.75,
                type='RandomFlip'),
            dict(prob=0.75, type='Rotate90'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        type='WHUMixVectorDataset'),
    num_workers=1,
    persistent_workers=False,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True,
        with_poly_json=False),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        direction=[
            'horizontal',
            'vertical',
            'diagonal',
        ],
        prob=0.75,
        type='RandomFlip'),
    dict(prob=0.75, type='Rotate90'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='val/val.json',
        backend_args=None,
        data_prefix=dict(img='val/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=1,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = [
    dict(
        ann_file='data/kostanai/val/val.json',
        backend_args=None,
        metric=[
            'segm',
        ],
        type='CocoMetric'),
]
vis_backends = [
    dict(
        init_kwargs=dict(
            allow_val_change=True,
            group='mask2former_training',
            name='mask2former_e15_lre-4_kostanai_afs',
            project='building-segmentation-gcp',
            resume='never'),
        save_dir=
        'work_dir\\mask2former_training\\mask2former_e15_lre-4_kostanai_afs\\wandb',
        type='WandbVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='TanmlhVisualizer',
    vis_backends=[
        dict(
            init_kwargs=dict(
                allow_val_change=True,
                group='mask2former_training',
                name='mask2former_e15_lre-4_kostanai_afs',
                project='building-segmentation-gcp',
                resume='never'),
            save_dir=
            'work_dir\\mask2former_training\\mask2former_e15_lre-4_kostanai_afs\\wandb',
            type='WandbVisBackend'),
    ])
work_dir = 'work_dir\\mask2former_training\\mask2former_e15_lre-4_kostanai_afs'

2025/06/23 16:49:44 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/06/23 16:49:44 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.0.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.0.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.0.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.0.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.0.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.0.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.1.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.1.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.1.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.1.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.1.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.1.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.2.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.2.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.2.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.2.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.2.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer1.2.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.0.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.0.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.0.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.0.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.0.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.0.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.1.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.1.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.1.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.1.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.1.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.1.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.2.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.2.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.2.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.2.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.2.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.2.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.3.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.3.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.3.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.3.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.3.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer2.3.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.0.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.0.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.0.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.0.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.0.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.0.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.1.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.1.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.1.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.1.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.1.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.1.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.2.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.2.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.2.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.2.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.2.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.2.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.3.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.3.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.3.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.3.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.3.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.3.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.4.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.4.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.4.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.4.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.4.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.4.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.5.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.5.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.5.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.5.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.5.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer3.5.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.0.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.0.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.0.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.0.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.0.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.0.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.1.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.1.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.1.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.1.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.1.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.1.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.2.bn1.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.2.bn1.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.2.bn2.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.2.bn2.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1e-05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.1
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.2.bn3.weight is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - WARNING - backbone.layer4.2.bn3.bias is skipped since its requires_grad=False
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.bias:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr=0.0001
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr_mult=1.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:decay_mult=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr=0.0001
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr_mult=1.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:decay_mult=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr=0.0001
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:weight_decay=0.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr_mult=1.0
2025/06/23 16:49:55 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:decay_mult=0.0
2025/06/23 16:49:55 - mmengine - INFO - LR is set based on batch size of 2 and the current batch size is 1. Scaling the original LR by 0.5.
2025/06/23 16:49:58 - mmengine - INFO - load model from: torchvision://resnet50
2025/06/23 16:49:58 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
2025/06/23 16:49:58 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

panoptic_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 512, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.query_embed.weight - torch.Size([300, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.query_feat.weight - torch.Size([300, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.cls_embed.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.cls_embed.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  
2025/06/23 16:50:03 - mmengine - INFO - Load checkpoint from checkpoints/mask2former_r50_pretrained_50e_whu-mix-vector.pth
2025/06/23 16:50:04 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/06/23 16:50:04 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/06/23 16:50:04 - mmengine - INFO - Checkpoints will be saved to D:\Sagi\GCP\GCP\work_dir\mask2former_training\mask2former_e15_lre-4_kostanai_afs.
2025/06/23 16:50:13 - mmengine - INFO - Epoch(train)  [1][  10/3769]  base_lr: 1.0000e-06 lr: 5.0000e-08  eta: 9:48:48  time: 0.9376  data_time: 0.4788  memory: 2272  grad_norm: 67.7279  loss: 0.8212  loss_cls: 0.1415  loss_mask: 0.0468  loss_dice: 0.6329
2025/06/23 16:50:16 - mmengine - INFO - Epoch(train)  [1][  20/3769]  base_lr: 2.0000e-06 lr: 1.0000e-07  eta: 6:40:35  time: 0.3385  data_time: 0.0036  memory: 2505  grad_norm: 27.0955  loss: 1.2511  loss_cls: 0.3037  loss_mask: 0.1138  loss_dice: 0.8336
2025/06/23 16:50:20 - mmengine - INFO - Epoch(train)  [1][  30/3769]  base_lr: 3.0000e-06 lr: 1.5000e-07  eta: 5:34:35  time: 0.3231  data_time: 0.0018  memory: 2106  grad_norm: 23.1141  loss: 0.8906  loss_cls: 0.2369  loss_mask: 0.0836  loss_dice: 0.5700
2025/06/23 16:50:23 - mmengine - INFO - Epoch(train)  [1][  40/3769]  base_lr: 4.0000e-06 lr: 2.0000e-07  eta: 5:01:57  time: 0.3256  data_time: 0.0079  memory: 2343  grad_norm: 29.0202  loss: 1.1134  loss_cls: 0.3777  loss_mask: 0.0871  loss_dice: 0.6486
2025/06/23 16:50:26 - mmengine - INFO - Epoch(train)  [1][  50/3769]  base_lr: 5.0000e-06 lr: 2.5000e-07  eta: 4:43:16  time: 0.3329  data_time: 0.0040  memory: 2541  grad_norm: 223.4874  loss: 1.1715  loss_cls: 0.3280  loss_mask: 0.1071  loss_dice: 0.7365
2025/06/23 16:50:29 - mmengine - INFO - Epoch(train)  [1][  60/3769]  base_lr: 6.0000e-06 lr: 3.0000e-07  eta: 4:29:54  time: 0.3243  data_time: 0.0058  memory: 2293  grad_norm: 23.4538  loss: 1.1041  loss_cls: 0.1342  loss_mask: 0.0589  loss_dice: 0.9111
2025/06/23 16:50:33 - mmengine - INFO - Epoch(train)  [1][  70/3769]  base_lr: 7.0000e-06 lr: 3.5000e-07  eta: 4:20:56  time: 0.3311  data_time: 0.0020  memory: 2402  grad_norm: 80.9238  loss: 1.2274  loss_cls: 0.4087  loss_mask: 0.1011  loss_dice: 0.7177
2025/06/23 16:50:36 - mmengine - INFO - Epoch(train)  [1][  80/3769]  base_lr: 8.0000e-06 lr: 4.0000e-07  eta: 4:14:03  time: 0.3293  data_time: 0.0042  memory: 2093  grad_norm: 64.9373  loss: 1.0305  loss_cls: 0.1837  loss_mask: 0.1254  loss_dice: 0.7214
2025/06/23 16:50:39 - mmengine - INFO - Epoch(train)  [1][  90/3769]  base_lr: 9.0000e-06 lr: 4.5000e-07  eta: 4:08:40  time: 0.3289  data_time: 0.0034  memory: 2519  grad_norm: 38.7657  loss: 1.1619  loss_cls: 0.2946  loss_mask: 0.1037  loss_dice: 0.7636
2025/06/23 16:50:42 - mmengine - INFO - Epoch(train)  [1][ 100/3769]  base_lr: 1.0000e-05 lr: 5.0000e-07  eta: 4:04:00  time: 0.3235  data_time: 0.0042  memory: 2439  grad_norm: 48.4658  loss: 0.8744  loss_cls: 0.2921  loss_mask: 0.0566  loss_dice: 0.5258
2025/06/23 16:50:46 - mmengine - INFO - Epoch(train)  [1][ 110/3769]  base_lr: 1.1000e-05 lr: 5.5000e-07  eta: 4:00:34  time: 0.3304  data_time: 0.0070  memory: 2372  grad_norm: 57.9115  loss: 1.1106  loss_cls: 0.2796  loss_mask: 0.0962  loss_dice: 0.7348
2025/06/23 16:50:49 - mmengine - INFO - Epoch(train)  [1][ 120/3769]  base_lr: 1.2000e-05 lr: 6.0000e-07  eta: 3:57:54  time: 0.3340  data_time: 0.0022  memory: 2483  grad_norm: 37.0121  loss: 0.8781  loss_cls: 0.2580  loss_mask: 0.0633  loss_dice: 0.5568
2025/06/23 16:50:53 - mmengine - INFO - Epoch(train)  [1][ 130/3769]  base_lr: 1.3000e-05 lr: 6.5000e-07  eta: 3:56:11  time: 0.3456  data_time: 0.0027  memory: 2321  grad_norm: 21.8160  loss: 0.9214  loss_cls: 0.1930  loss_mask: 0.0718  loss_dice: 0.6566
2025/06/23 16:50:56 - mmengine - INFO - Epoch(train)  [1][ 140/3769]  base_lr: 1.4000e-05 lr: 7.0000e-07  eta: 3:54:04  time: 0.3314  data_time: 0.0031  memory: 2343  grad_norm: 862.6765  loss: 1.0324  loss_cls: 0.2513  loss_mask: 0.2141  loss_dice: 0.5671
2025/06/23 16:50:59 - mmengine - INFO - Epoch(train)  [1][ 150/3769]  base_lr: 1.5000e-05 lr: 7.5000e-07  eta: 3:51:58  time: 0.3254  data_time: 0.0013  memory: 2425  grad_norm: 49.3902  loss: 0.9345  loss_cls: 0.2432  loss_mask: 0.1064  loss_dice: 0.5849
2025/06/23 16:51:02 - mmengine - INFO - Epoch(train)  [1][ 160/3769]  base_lr: 1.6000e-05 lr: 8.0000e-07  eta: 3:50:13  time: 0.3274  data_time: 0.0019  memory: 2257  grad_norm: 39.5268  loss: 0.8532  loss_cls: 0.2000  loss_mask: 0.0672  loss_dice: 0.5860
2025/06/23 16:51:06 - mmengine - INFO - Epoch(train)  [1][ 170/3769]  base_lr: 1.7000e-05 lr: 8.5000e-07  eta: 3:48:49  time: 0.3316  data_time: 0.0018  memory: 2314  grad_norm: 33.8171  loss: 1.0341  loss_cls: 0.2752  loss_mask: 0.0950  loss_dice: 0.6639
2025/06/23 16:51:09 - mmengine - INFO - Epoch(train)  [1][ 180/3769]  base_lr: 1.8000e-05 lr: 9.0000e-07  eta: 3:47:36  time: 0.3325  data_time: 0.0023  memory: 2409  grad_norm: 32.5831  loss: 1.3733  loss_cls: 0.3353  loss_mask: 0.0891  loss_dice: 0.9489
2025/06/23 16:51:12 - mmengine - INFO - Epoch(train)  [1][ 190/3769]  base_lr: 1.9000e-05 lr: 9.5000e-07  eta: 3:46:14  time: 0.3247  data_time: 0.0069  memory: 2263  grad_norm: 62.2096  loss: 0.9065  loss_cls: 0.1826  loss_mask: 0.0976  loss_dice: 0.6263
2025/06/23 16:51:16 - mmengine - INFO - Epoch(train)  [1][ 200/3769]  base_lr: 2.0000e-05 lr: 1.0000e-06  eta: 3:45:09  time: 0.3291  data_time: 0.0045  memory: 2372  grad_norm: 13.5811  loss: 0.4440  loss_cls: 0.0858  loss_mask: 0.0350  loss_dice: 0.3232
2025/06/23 16:51:19 - mmengine - INFO - Epoch(train)  [1][ 210/3769]  base_lr: 2.1000e-05 lr: 1.0500e-06  eta: 3:44:11  time: 0.3300  data_time: 0.0023  memory: 2278  grad_norm: 38.6712  loss: 1.3493  loss_cls: 0.2181  loss_mask: 0.0998  loss_dice: 1.0315
2025/06/23 16:51:22 - mmengine - INFO - Epoch(train)  [1][ 220/3769]  base_lr: 2.2000e-05 lr: 1.1000e-06  eta: 3:43:13  time: 0.3267  data_time: 0.0040  memory: 2314  grad_norm: 46.4052  loss: 1.2395  loss_cls: 0.2486  loss_mask: 0.1777  loss_dice: 0.8132
2025/06/23 16:51:25 - mmengine - INFO - Epoch(train)  [1][ 230/3769]  base_lr: 2.3000e-05 lr: 1.1500e-06  eta: 3:42:22  time: 0.3288  data_time: 0.0045  memory: 2409  grad_norm: 102.5912  loss: 1.2137  loss_cls: 0.2839  loss_mask: 0.1372  loss_dice: 0.7926
2025/06/23 16:51:29 - mmengine - INFO - Epoch(train)  [1][ 240/3769]  base_lr: 2.4000e-05 lr: 1.2000e-06  eta: 3:41:35  time: 0.3283  data_time: 0.0038  memory: 2380  grad_norm: 37.3238  loss: 1.5074  loss_cls: 0.2630  loss_mask: 0.1110  loss_dice: 1.1334
2025/06/23 16:51:32 - mmengine - INFO - Epoch(train)  [1][ 250/3769]  base_lr: 2.5000e-05 lr: 1.2500e-06  eta: 3:40:49  time: 0.3262  data_time: 0.0030  memory: 2285  grad_norm: 19.6004  loss: 0.9167  loss_cls: 0.1678  loss_mask: 0.0714  loss_dice: 0.6774
2025/06/23 16:51:35 - mmengine - INFO - Epoch(train)  [1][ 260/3769]  base_lr: 2.6000e-05 lr: 1.3000e-06  eta: 3:40:02  time: 0.3243  data_time: 0.0041  memory: 2257  grad_norm: 49.3004  loss: 0.9207  loss_cls: 0.2039  loss_mask: 0.0645  loss_dice: 0.6523
2025/06/23 16:51:39 - mmengine - INFO - Epoch(train)  [1][ 270/3769]  base_lr: 2.7000e-05 lr: 1.3500e-06  eta: 3:39:23  time: 0.3269  data_time: 0.0026  memory: 2534  grad_norm: 61.7497  loss: 0.8723  loss_cls: 0.2220  loss_mask: 0.0987  loss_dice: 0.5516
2025/06/23 16:51:42 - mmengine - INFO - Epoch(train)  [1][ 280/3769]  base_lr: 2.8000e-05 lr: 1.4000e-06  eta: 3:38:48  time: 0.3278  data_time: 0.0015  memory: 2432  grad_norm: 30.3753  loss: 1.0637  loss_cls: 0.2561  loss_mask: 0.1050  loss_dice: 0.7026
2025/06/23 16:51:45 - mmengine - INFO - Epoch(train)  [1][ 290/3769]  base_lr: 2.9000e-05 lr: 1.4500e-06  eta: 3:38:13  time: 0.3265  data_time: 0.0003  memory: 2505  grad_norm: 41.5991  loss: 0.7974  loss_cls: 0.2778  loss_mask: 0.0443  loss_dice: 0.4753
2025/06/23 16:51:48 - mmengine - INFO - Epoch(train)  [1][ 300/3769]  base_lr: 3.0000e-05 lr: 1.5000e-06  eta: 3:37:39  time: 0.3263  data_time: 0.0022  memory: 2358  grad_norm: 17.7417  loss: 0.9568  loss_cls: 0.1757  loss_mask: 0.0670  loss_dice: 0.7140
2025/06/23 16:51:52 - mmengine - INFO - Epoch(train)  [1][ 310/3769]  base_lr: 3.1000e-05 lr: 1.5500e-06  eta: 3:37:22  time: 0.3381  data_time: 0.0051  memory: 2387  grad_norm: 55.5380  loss: 1.0891  loss_cls: 0.3607  loss_mask: 0.0791  loss_dice: 0.6492
2025/06/23 16:51:55 - mmengine - INFO - Epoch(train)  [1][ 320/3769]  base_lr: 3.2000e-05 lr: 1.6000e-06  eta: 3:36:59  time: 0.3315  data_time: 0.0024  memory: 2417  grad_norm: 101.5970  loss: 1.2025  loss_cls: 0.3166  loss_mask: 0.1121  loss_dice: 0.7738
2025/06/23 16:51:58 - mmengine - INFO - Epoch(train)  [1][ 330/3769]  base_lr: 3.3000e-05 lr: 1.6500e-06  eta: 3:36:35  time: 0.3307  data_time: 0.0015  memory: 2490  grad_norm: 84.8163  loss: 1.4000  loss_cls: 0.4593  loss_mask: 0.1698  loss_dice: 0.7709
2025/06/23 16:52:02 - mmengine - INFO - Epoch(train)  [1][ 340/3769]  base_lr: 3.4000e-05 lr: 1.7000e-06  eta: 3:36:13  time: 0.3309  data_time: 0.0069  memory: 2505  grad_norm: 107.2344  loss: 1.0389  loss_cls: 0.2511  loss_mask: 0.0879  loss_dice: 0.6999
2025/06/23 16:52:05 - mmengine - INFO - Epoch(train)  [1][ 350/3769]  base_lr: 3.5000e-05 lr: 1.7500e-06  eta: 3:35:46  time: 0.3249  data_time: 0.0042  memory: 2278  grad_norm: 22.9084  loss: 0.9006  loss_cls: 0.1799  loss_mask: 0.1278  loss_dice: 0.5929
2025/06/23 16:52:08 - mmengine - INFO - Epoch(train)  [1][ 360/3769]  base_lr: 3.6000e-05 lr: 1.8000e-06  eta: 3:35:27  time: 0.3320  data_time: 0.0038  memory: 2409  grad_norm: 60.8106  loss: 1.2703  loss_cls: 0.3019  loss_mask: 0.1220  loss_dice: 0.8464
2025/06/23 16:52:11 - mmengine - INFO - Epoch(train)  [1][ 370/3769]  base_lr: 3.7000e-05 lr: 1.8500e-06  eta: 3:35:05  time: 0.3278  data_time: 0.0094  memory: 2300  grad_norm: 23.5419  loss: 1.0924  loss_cls: 0.2420  loss_mask: 0.0930  loss_dice: 0.7574
2025/06/23 16:52:15 - mmengine - INFO - Epoch(train)  [1][ 380/3769]  base_lr: 3.8000e-05 lr: 1.9000e-06  eta: 3:34:46  time: 0.3304  data_time: 0.0042  memory: 2468  grad_norm: 46.2804  loss: 0.9990  loss_cls: 0.2265  loss_mask: 0.0876  loss_dice: 0.6849
2025/06/23 16:52:18 - mmengine - INFO - Epoch(train)  [1][ 390/3769]  base_lr: 3.9000e-05 lr: 1.9500e-06  eta: 3:34:26  time: 0.3282  data_time: 0.0045  memory: 2271  grad_norm: 20.3180  loss: 0.7669  loss_cls: 0.2523  loss_mask: 0.0593  loss_dice: 0.4553
2025/06/23 16:52:21 - mmengine - INFO - Epoch(train)  [1][ 400/3769]  base_lr: 4.0000e-05 lr: 2.0000e-06  eta: 3:34:16  time: 0.3375  data_time: 0.0038  memory: 2478  grad_norm: 30.5305  loss: 0.9956  loss_cls: 0.2544  loss_mask: 0.0688  loss_dice: 0.6724
2025/06/23 16:52:25 - mmengine - INFO - Epoch(train)  [1][ 410/3769]  base_lr: 4.1000e-05 lr: 2.0500e-06  eta: 3:34:01  time: 0.3319  data_time: 0.0047  memory: 2351  grad_norm: 120.2955  loss: 1.3373  loss_cls: 0.3319  loss_mask: 0.2287  loss_dice: 0.7767
2025/06/23 16:52:28 - mmengine - INFO - Epoch(train)  [1][ 420/3769]  base_lr: 4.2000e-05 lr: 2.1000e-06  eta: 3:33:47  time: 0.3330  data_time: 0.0036  memory: 2380  grad_norm: 44.2303  loss: 1.0507  loss_cls: 0.3521  loss_mask: 0.0730  loss_dice: 0.6255
2025/06/23 16:52:31 - mmengine - INFO - Epoch(train)  [1][ 430/3769]  base_lr: 4.3000e-05 lr: 2.1500e-06  eta: 3:33:31  time: 0.3292  data_time: 0.0032  memory: 2099  grad_norm: 19.6954  loss: 0.9197  loss_cls: 0.1527  loss_mask: 0.1126  loss_dice: 0.6544
2025/06/23 16:52:35 - mmengine - INFO - Epoch(train)  [1][ 440/3769]  base_lr: 4.4000e-05 lr: 2.2000e-06  eta: 3:33:15  time: 0.3290  data_time: 0.0018  memory: 2271  grad_norm: 99.6549  loss: 1.2662  loss_cls: 0.3686  loss_mask: 0.0856  loss_dice: 0.8120
2025/06/23 16:52:38 - mmengine - INFO - Epoch(train)  [1][ 450/3769]  base_lr: 4.5000e-05 lr: 2.2500e-06  eta: 3:32:59  time: 0.3286  data_time: 0.0053  memory: 2227  grad_norm: 40.5382  loss: 0.7999  loss_cls: 0.1737  loss_mask: 0.0653  loss_dice: 0.5609
2025/06/23 16:52:41 - mmengine - INFO - Epoch(train)  [1][ 460/3769]  base_lr: 4.6000e-05 lr: 2.3000e-06  eta: 3:32:49  time: 0.3355  data_time: 0.0029  memory: 2387  grad_norm: 26.5893  loss: 0.9835  loss_cls: 0.1804  loss_mask: 0.0714  loss_dice: 0.7317
2025/06/23 16:52:45 - mmengine - INFO - Epoch(train)  [1][ 470/3769]  base_lr: 4.7000e-05 lr: 2.3500e-06  eta: 3:32:40  time: 0.3350  data_time: 0.0074  memory: 2409  grad_norm: 49.4127  loss: 1.0731  loss_cls: 0.3001  loss_mask: 0.1127  loss_dice: 0.6603
2025/06/23 16:52:48 - mmengine - INFO - Epoch(train)  [1][ 480/3769]  base_lr: 4.8000e-05 lr: 2.4000e-06  eta: 3:32:35  time: 0.3418  data_time: 0.0043  memory: 2394  grad_norm: 209.0359  loss: 1.2827  loss_cls: 0.2639  loss_mask: 0.1441  loss_dice: 0.8746
2025/06/23 16:52:51 - mmengine - INFO - Epoch(train)  [1][ 490/3769]  base_lr: 4.9000e-05 lr: 2.4500e-06  eta: 3:32:30  time: 0.3401  data_time: 0.0030  memory: 2336  grad_norm: 129.7507  loss: 1.3409  loss_cls: 0.3719  loss_mask: 0.1303  loss_dice: 0.8387
2025/06/23 16:52:55 - mmengine - INFO - Epoch(train)  [1][ 500/3769]  base_lr: 5.0000e-05 lr: 2.5000e-06  eta: 3:32:16  time: 0.3290  data_time: 0.0019  memory: 2387  grad_norm: 17.0721  loss: 0.8727  loss_cls: 0.1878  loss_mask: 0.1350  loss_dice: 0.5499
2025/06/23 16:52:58 - mmengine - INFO - Epoch(train)  [1][ 510/3769]  base_lr: 5.1000e-05 lr: 2.5500e-06  eta: 3:31:59  time: 0.3235  data_time: 0.0026  memory: 2285  grad_norm: 21.6457  loss: 0.7051  loss_cls: 0.1833  loss_mask: 0.0364  loss_dice: 0.4854
2025/06/23 16:53:01 - mmengine - INFO - Epoch(train)  [1][ 520/3769]  base_lr: 5.2000e-05 lr: 2.6000e-06  eta: 3:31:54  time: 0.3397  data_time: 0.0029  memory: 2166  grad_norm: 28.1728  loss: 1.2111  loss_cls: 0.2426  loss_mask: 0.1549  loss_dice: 0.8137
2025/06/23 16:53:05 - mmengine - INFO - Epoch(train)  [1][ 530/3769]  base_lr: 5.3000e-05 lr: 2.6500e-06  eta: 3:31:43  time: 0.3314  data_time: 0.0054  memory: 2300  grad_norm: 56.8027  loss: 1.2747  loss_cls: 0.4346  loss_mask: 0.0936  loss_dice: 0.7466
2025/06/23 16:53:08 - mmengine - INFO - Epoch(train)  [1][ 540/3769]  base_lr: 5.4000e-05 lr: 2.7000e-06  eta: 3:31:35  time: 0.3346  data_time: 0.0020  memory: 2365  grad_norm: 43.2922  loss: 0.8855  loss_cls: 0.1946  loss_mask: 0.0647  loss_dice: 0.6263
2025/06/23 16:53:11 - mmengine - INFO - Epoch(train)  [1][ 550/3769]  base_lr: 5.5000e-05 lr: 2.7500e-06  eta: 3:31:26  time: 0.3343  data_time: 0.0044  memory: 2461  grad_norm: 48.9475  loss: 1.1526  loss_cls: 0.2810  loss_mask: 0.0828  loss_dice: 0.7887
2025/06/23 16:53:15 - mmengine - INFO - Epoch(train)  [1][ 560/3769]  base_lr: 5.6000e-05 lr: 2.8000e-06  eta: 3:31:14  time: 0.3281  data_time: 0.0036  memory: 2178  grad_norm: 14.8334  loss: 0.9170  loss_cls: 0.1839  loss_mask: 0.0736  loss_dice: 0.6595
2025/06/23 16:53:18 - mmengine - INFO - Epoch(train)  [1][ 570/3769]  base_lr: 5.7000e-05 lr: 2.8500e-06  eta: 3:31:03  time: 0.3303  data_time: 0.0055  memory: 2278  grad_norm: 47.5214  loss: 1.0379  loss_cls: 0.2090  loss_mask: 0.1779  loss_dice: 0.6510
2025/06/23 16:53:21 - mmengine - INFO - Epoch(train)  [1][ 580/3769]  base_lr: 5.8000e-05 lr: 2.9000e-06  eta: 3:30:53  time: 0.3304  data_time: 0.0043  memory: 2468  grad_norm: 12.0768  loss: 0.7789  loss_cls: 0.1147  loss_mask: 0.0831  loss_dice: 0.5812
2025/06/23 16:53:25 - mmengine - INFO - Epoch(train)  [1][ 590/3769]  base_lr: 5.9000e-05 lr: 2.9500e-06  eta: 3:30:43  time: 0.3306  data_time: 0.0036  memory: 2358  grad_norm: 111.2468  loss: 1.1823  loss_cls: 0.2408  loss_mask: 0.0749  loss_dice: 0.8666
2025/06/23 16:53:28 - mmengine - INFO - Epoch(train)  [1][ 600/3769]  base_lr: 6.0000e-05 lr: 3.0000e-06  eta: 3:30:30  time: 0.3257  data_time: 0.0002  memory: 2099  grad_norm: 20.0685  loss: 0.8037  loss_cls: 0.1717  loss_mask: 0.0813  loss_dice: 0.5506
2025/06/23 16:53:31 - mmengine - INFO - Epoch(train)  [1][ 610/3769]  base_lr: 6.1000e-05 lr: 3.0500e-06  eta: 3:30:25  time: 0.3370  data_time: 0.0029  memory: 2081  grad_norm: 177.9066  loss: 1.0788  loss_cls: 0.2527  loss_mask: 0.2275  loss_dice: 0.5986
2025/06/23 16:53:35 - mmengine - INFO - Epoch(train)  [1][ 620/3769]  base_lr: 6.2000e-05 lr: 3.1000e-06  eta: 3:30:25  time: 0.3460  data_time: 0.0030  memory: 2425  grad_norm: 40.1460  loss: 1.0277  loss_cls: 0.2632  loss_mask: 0.0615  loss_dice: 0.7030
2025/06/23 16:53:38 - mmengine - INFO - Epoch(train)  [1][ 630/3769]  base_lr: 6.3000e-05 lr: 3.1500e-06  eta: 3:30:15  time: 0.3304  data_time: 0.0032  memory: 2314  grad_norm: 39.4347  loss: 0.9455  loss_cls: 0.2649  loss_mask: 0.0715  loss_dice: 0.6091
2025/06/23 16:53:41 - mmengine - INFO - Epoch(train)  [1][ 640/3769]  base_lr: 6.4000e-05 lr: 3.2000e-06  eta: 3:30:08  time: 0.3345  data_time: 0.0033  memory: 2343  grad_norm: 45.1231  loss: 1.1981  loss_cls: 0.2288  loss_mask: 0.1082  loss_dice: 0.8610
2025/06/23 16:53:45 - mmengine - INFO - Epoch(train)  [1][ 650/3769]  base_lr: 6.5000e-05 lr: 3.2500e-06  eta: 3:29:58  time: 0.3289  data_time: 0.0027  memory: 2461  grad_norm: 90.2110  loss: 0.9807  loss_cls: 0.2153  loss_mask: 0.0597  loss_dice: 0.7058
2025/06/23 16:53:48 - mmengine - INFO - Epoch(train)  [1][ 660/3769]  base_lr: 6.6000e-05 lr: 3.3000e-06  eta: 3:29:49  time: 0.3300  data_time: 0.0034  memory: 2308  grad_norm: 456.8805  loss: 1.0590  loss_cls: 0.2333  loss_mask: 0.1053  loss_dice: 0.7204
2025/06/23 16:53:51 - mmengine - INFO - Epoch(train)  [1][ 670/3769]  base_lr: 6.7000e-05 lr: 3.3500e-06  eta: 3:29:41  time: 0.3304  data_time: 0.0042  memory: 2387  grad_norm: 27.7272  loss: 1.1297  loss_cls: 0.2359  loss_mask: 0.1260  loss_dice: 0.7678
2025/06/23 16:53:54 - mmengine - INFO - Epoch(train)  [1][ 680/3769]  base_lr: 6.8000e-05 lr: 3.4000e-06  eta: 3:29:30  time: 0.3261  data_time: 0.0028  memory: 2191  grad_norm: 104.6688  loss: 1.0126  loss_cls: 0.2910  loss_mask: 0.0841  loss_dice: 0.6375
2025/06/23 16:53:58 - mmengine - INFO - Epoch(train)  [1][ 690/3769]  base_lr: 6.9000e-05 lr: 3.4500e-06  eta: 3:29:20  time: 0.3286  data_time: 0.0032  memory: 2271  grad_norm: 15.7161  loss: 1.1457  loss_cls: 0.2435  loss_mask: 0.0621  loss_dice: 0.8400
2025/06/23 16:54:01 - mmengine - INFO - Epoch(train)  [1][ 700/3769]  base_lr: 7.0000e-05 lr: 3.5000e-06  eta: 3:29:11  time: 0.3289  data_time: 0.0067  memory: 2365  grad_norm: 32.8284  loss: 1.9070  loss_cls: 0.2670  loss_mask: 0.7801  loss_dice: 0.8599
2025/06/23 16:54:04 - mmengine - INFO - Epoch(train)  [1][ 710/3769]  base_lr: 7.1000e-05 lr: 3.5500e-06  eta: 3:29:02  time: 0.3277  data_time: 0.0026  memory: 2184  grad_norm: 25.1814  loss: 1.0397  loss_cls: 0.2822  loss_mask: 0.0899  loss_dice: 0.6676
2025/06/23 16:54:08 - mmengine - INFO - Epoch(train)  [1][ 720/3769]  base_lr: 7.2000e-05 lr: 3.6000e-06  eta: 3:28:54  time: 0.3294  data_time: 0.0050  memory: 2519  grad_norm: 30.6017  loss: 1.1183  loss_cls: 0.2560  loss_mask: 0.1014  loss_dice: 0.7608
2025/06/23 16:54:11 - mmengine - INFO - Epoch(train)  [1][ 730/3769]  base_lr: 7.3000e-05 lr: 3.6500e-06  eta: 3:28:45  time: 0.3290  data_time: 0.0065  memory: 2454  grad_norm: 48.1933  loss: 1.0136  loss_cls: 0.2427  loss_mask: 0.1229  loss_dice: 0.6480
2025/06/23 16:54:14 - mmengine - INFO - Epoch(train)  [1][ 740/3769]  base_lr: 7.4000e-05 lr: 3.7000e-06  eta: 3:28:36  time: 0.3270  data_time: 0.0057  memory: 2380  grad_norm: 138.1560  loss: 0.9956  loss_cls: 0.3470  loss_mask: 0.0706  loss_dice: 0.5780
2025/06/23 16:54:17 - mmengine - INFO - Epoch(train)  [1][ 750/3769]  base_lr: 7.5000e-05 lr: 3.7500e-06  eta: 3:28:28  time: 0.3302  data_time: 0.0033  memory: 2439  grad_norm: 287.0264  loss: 1.4907  loss_cls: 0.3934  loss_mask: 0.1580  loss_dice: 0.9393
2025/06/23 16:54:21 - mmengine - INFO - Epoch(train)  [1][ 760/3769]  base_lr: 7.6000e-05 lr: 3.8000e-06  eta: 3:28:21  time: 0.3305  data_time: 0.0071  memory: 2372  grad_norm: 31.0915  loss: 1.1120  loss_cls: 0.2915  loss_mask: 0.1077  loss_dice: 0.7127
2025/06/23 16:54:24 - mmengine - INFO - Epoch(train)  [1][ 770/3769]  base_lr: 7.7000e-05 lr: 3.8500e-06  eta: 3:28:11  time: 0.3246  data_time: 0.0044  memory: 2336  grad_norm: 57.4916  loss: 0.7456  loss_cls: 0.1399  loss_mask: 0.0541  loss_dice: 0.5516
2025/06/23 16:54:27 - mmengine - INFO - Epoch(train)  [1][ 780/3769]  base_lr: 7.8000e-05 lr: 3.9000e-06  eta: 3:28:04  time: 0.3302  data_time: 0.0048  memory: 2329  grad_norm: 25.6045  loss: 0.9315  loss_cls: 0.2037  loss_mask: 0.0783  loss_dice: 0.6494
2025/06/23 16:54:31 - mmengine - INFO - Epoch(train)  [1][ 790/3769]  base_lr: 7.9000e-05 lr: 3.9500e-06  eta: 3:27:55  time: 0.3264  data_time: 0.0089  memory: 2314  grad_norm: 124.3107  loss: 0.8087  loss_cls: 0.1783  loss_mask: 0.0867  loss_dice: 0.5437
2025/06/23 16:54:34 - mmengine - INFO - Epoch(train)  [1][ 800/3769]  base_lr: 8.0000e-05 lr: 4.0000e-06  eta: 3:27:49  time: 0.3324  data_time: 0.0036  memory: 2372  grad_norm: 133.3899  loss: 1.2663  loss_cls: 0.2757  loss_mask: 0.1563  loss_dice: 0.8342
2025/06/23 16:54:37 - mmengine - INFO - Epoch(train)  [1][ 810/3769]  base_lr: 8.1000e-05 lr: 4.0500e-06  eta: 3:27:42  time: 0.3310  data_time: 0.0032  memory: 2358  grad_norm: 175.9633  loss: 1.2423  loss_cls: 0.3697  loss_mask: 0.1057  loss_dice: 0.7669
2025/06/23 16:54:41 - mmengine - INFO - Epoch(train)  [1][ 820/3769]  base_lr: 8.2000e-05 lr: 4.1000e-06  eta: 3:27:34  time: 0.3281  data_time: 0.0056  memory: 2358  grad_norm: 208.7651  loss: 1.5779  loss_cls: 0.4772  loss_mask: 0.0916  loss_dice: 1.0091
2025/06/23 16:54:44 - mmengine - INFO - Epoch(train)  [1][ 830/3769]  base_lr: 8.3000e-05 lr: 4.1500e-06  eta: 3:27:27  time: 0.3284  data_time: 0.0038  memory: 2314  grad_norm: 16.9557  loss: 1.0365  loss_cls: 0.2482  loss_mask: 0.0834  loss_dice: 0.7048
2025/06/23 16:54:47 - mmengine - INFO - Epoch(train)  [1][ 840/3769]  base_lr: 8.4000e-05 lr: 4.2000e-06  eta: 3:27:20  time: 0.3294  data_time: 0.0073  memory: 2358  grad_norm: 31.9947  loss: 0.9146  loss_cls: 0.2197  loss_mask: 0.0673  loss_dice: 0.6276
2025/06/23 16:54:50 - mmengine - INFO - Epoch(train)  [1][ 850/3769]  base_lr: 8.5000e-05 lr: 4.2500e-06  eta: 3:27:11  time: 0.3265  data_time: 0.0025  memory: 2519  grad_norm: 30.4273  loss: 0.7894  loss_cls: 0.1539  loss_mask: 0.1405  loss_dice: 0.4951
2025/06/23 16:54:54 - mmengine - INFO - Epoch(train)  [1][ 860/3769]  base_lr: 8.6000e-05 lr: 4.3000e-06  eta: 3:27:06  time: 0.3321  data_time: 0.0041  memory: 2468  grad_norm: 151.7399  loss: 1.3196  loss_cls: 0.3409  loss_mask: 0.1503  loss_dice: 0.8285
2025/06/23 16:54:57 - mmengine - INFO - Epoch(train)  [1][ 870/3769]  base_lr: 8.7000e-05 lr: 4.3500e-06  eta: 3:26:58  time: 0.3265  data_time: 0.0022  memory: 2321  grad_norm: 57.0571  loss: 1.0157  loss_cls: 0.2272  loss_mask: 0.0576  loss_dice: 0.7310
2025/06/23 16:55:00 - mmengine - INFO - Epoch(train)  [1][ 880/3769]  base_lr: 8.8000e-05 lr: 4.4000e-06  eta: 3:26:50  time: 0.3266  data_time: 0.0034  memory: 2300  grad_norm: 20.4044  loss: 0.8245  loss_cls: 0.1591  loss_mask: 0.0840  loss_dice: 0.5814
2025/06/23 16:55:03 - mmengine - INFO - Epoch(train)  [1][ 890/3769]  base_lr: 8.9000e-05 lr: 4.4500e-06  eta: 3:26:42  time: 0.3260  data_time: 0.0029  memory: 2166  grad_norm: 290.5660  loss: 0.7770  loss_cls: 0.1727  loss_mask: 0.0720  loss_dice: 0.5323
2025/06/23 16:55:07 - mmengine - INFO - Epoch(train)  [1][ 900/3769]  base_lr: 9.0000e-05 lr: 4.5000e-06  eta: 3:26:36  time: 0.3307  data_time: 0.0058  memory: 2365  grad_norm: 26.4862  loss: 1.2003  loss_cls: 0.2750  loss_mask: 0.1017  loss_dice: 0.8236
2025/06/23 16:55:10 - mmengine - INFO - Epoch(train)  [1][ 910/3769]  base_lr: 9.1000e-05 lr: 4.5500e-06  eta: 3:26:30  time: 0.3291  data_time: 0.0016  memory: 2432  grad_norm: 97.8129  loss: 1.4836  loss_cls: 0.3444  loss_mask: 0.3557  loss_dice: 0.7835
2025/06/23 16:55:13 - mmengine - INFO - Epoch(train)  [1][ 920/3769]  base_lr: 9.2000e-05 lr: 4.6000e-06  eta: 3:26:23  time: 0.3295  data_time: 0.0042  memory: 2351  grad_norm: 16.7089  loss: 0.8481  loss_cls: 0.1562  loss_mask: 0.1011  loss_dice: 0.5907
2025/06/23 16:55:17 - mmengine - INFO - Epoch(train)  [1][ 930/3769]  base_lr: 9.3000e-05 lr: 4.6500e-06  eta: 3:26:19  time: 0.3337  data_time: 0.0011  memory: 2534  grad_norm: 58.0189  loss: 0.9735  loss_cls: 0.1763  loss_mask: 0.0444  loss_dice: 0.7528
2025/06/23 16:55:20 - mmengine - INFO - Epoch(train)  [1][ 940/3769]  base_lr: 9.4000e-05 lr: 4.7000e-06  eta: 3:26:12  time: 0.3285  data_time: 0.0046  memory: 2372  grad_norm: 53.2340  loss: 1.2048  loss_cls: 0.2912  loss_mask: 0.1489  loss_dice: 0.7648
2025/06/23 16:55:23 - mmengine - INFO - Epoch(train)  [1][ 950/3769]  base_lr: 9.5000e-05 lr: 4.7500e-06  eta: 3:26:06  time: 0.3289  data_time: 0.0041  memory: 2498  grad_norm: 9.8931  loss: 0.9601  loss_cls: 0.1790  loss_mask: 0.0611  loss_dice: 0.7200
2025/06/23 16:55:27 - mmengine - INFO - Epoch(train)  [1][ 960/3769]  base_lr: 9.6000e-05 lr: 4.8000e-06  eta: 3:25:59  time: 0.3275  data_time: 0.0044  memory: 2351  grad_norm: 42.8735  loss: 0.9928  loss_cls: 0.1448  loss_mask: 0.1023  loss_dice: 0.7457
2025/06/23 16:55:30 - mmengine - INFO - Epoch(train)  [1][ 970/3769]  base_lr: 9.7000e-05 lr: 4.8500e-06  eta: 3:25:52  time: 0.3270  data_time: 0.0048  memory: 2251  grad_norm: 71.0337  loss: 1.4392  loss_cls: 0.3225  loss_mask: 0.2934  loss_dice: 0.8233
2025/06/23 16:55:33 - mmengine - INFO - Epoch(train)  [1][ 980/3769]  base_lr: 9.8000e-05 lr: 4.9000e-06  eta: 3:25:46  time: 0.3298  data_time: 0.0039  memory: 2285  grad_norm: 465.2854  loss: 1.0153  loss_cls: 0.2282  loss_mask: 0.1006  loss_dice: 0.6865
2025/06/23 16:55:36 - mmengine - INFO - Epoch(train)  [1][ 990/3769]  base_lr: 9.9000e-05 lr: 4.9500e-06  eta: 3:25:39  time: 0.3275  data_time: 0.0047  memory: 2490  grad_norm: 19.1262  loss: 0.9386  loss_cls: 0.2240  loss_mask: 0.0683  loss_dice: 0.6463
2025/06/23 16:55:40 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 16:55:40 - mmengine - INFO - Epoch(train)  [1][1000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:25:34  time: 0.3304  data_time: 0.0048  memory: 2239  grad_norm: 37.0332  loss: 0.9507  loss_cls: 0.1613  loss_mask: 0.0801  loss_dice: 0.7093
2025/06/23 16:55:43 - mmengine - INFO - Epoch(train)  [1][1010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:25:27  time: 0.3256  data_time: 0.0036  memory: 2191  grad_norm: 39.2653  loss: 0.7558  loss_cls: 0.2390  loss_mask: 0.0810  loss_dice: 0.4359
2025/06/23 16:55:46 - mmengine - INFO - Epoch(train)  [1][1020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:25:20  time: 0.3275  data_time: 0.0033  memory: 2409  grad_norm: 44.8201  loss: 1.2236  loss_cls: 0.3823  loss_mask: 0.0663  loss_dice: 0.7750
2025/06/23 16:55:50 - mmengine - INFO - Epoch(train)  [1][1030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:25:15  time: 0.3294  data_time: 0.0021  memory: 2527  grad_norm: 139.0819  loss: 1.0590  loss_cls: 0.3465  loss_mask: 0.0980  loss_dice: 0.6146
2025/06/23 16:55:53 - mmengine - INFO - Epoch(train)  [1][1040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:25:10  time: 0.3318  data_time: 0.0020  memory: 2483  grad_norm: 44.5602  loss: 1.3663  loss_cls: 0.3192  loss_mask: 0.1102  loss_dice: 0.9368
2025/06/23 16:55:56 - mmengine - INFO - Epoch(train)  [1][1050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:25:04  time: 0.3286  data_time: 0.0046  memory: 2351  grad_norm: 49.7576  loss: 1.2440  loss_cls: 0.2997  loss_mask: 0.1607  loss_dice: 0.7835
2025/06/23 16:55:59 - mmengine - INFO - Epoch(train)  [1][1060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:57  time: 0.3264  data_time: 0.0023  memory: 2124  grad_norm: 28.8202  loss: 1.0351  loss_cls: 0.1931  loss_mask: 0.0599  loss_dice: 0.7821
2025/06/23 16:56:03 - mmengine - INFO - Epoch(train)  [1][1070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:50  time: 0.3246  data_time: 0.0022  memory: 2336  grad_norm: 39.3012  loss: 0.8867  loss_cls: 0.1110  loss_mask: 0.1215  loss_dice: 0.6542
2025/06/23 16:56:06 - mmengine - INFO - Epoch(train)  [1][1080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:44  time: 0.3264  data_time: 0.0014  memory: 2417  grad_norm: 69.2324  loss: 1.1558  loss_cls: 0.2019  loss_mask: 0.0896  loss_dice: 0.8642
2025/06/23 16:56:09 - mmengine - INFO - Epoch(train)  [1][1090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:38  time: 0.3282  data_time: 0.0031  memory: 2402  grad_norm: 246.9872  loss: 1.3236  loss_cls: 0.3556  loss_mask: 0.1107  loss_dice: 0.8574
2025/06/23 16:56:12 - mmengine - INFO - Epoch(train)  [1][1100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:32  time: 0.3284  data_time: 0.0051  memory: 2257  grad_norm: 86.2240  loss: 1.4777  loss_cls: 0.3285  loss_mask: 0.1945  loss_dice: 0.9547
2025/06/23 16:56:16 - mmengine - INFO - Epoch(train)  [1][1110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:24  time: 0.3225  data_time: 0.0024  memory: 2058  grad_norm: 55.3343  loss: 1.3227  loss_cls: 0.2474  loss_mask: 0.2057  loss_dice: 0.8696
2025/06/23 16:56:19 - mmengine - INFO - Epoch(train)  [1][1120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:19  time: 0.3297  data_time: 0.0013  memory: 2454  grad_norm: 89.6460  loss: 1.1542  loss_cls: 0.2445  loss_mask: 0.1470  loss_dice: 0.7627
2025/06/23 16:56:22 - mmengine - INFO - Epoch(train)  [1][1130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:12  time: 0.3234  data_time: 0.0048  memory: 2336  grad_norm: 47.7468  loss: 1.0329  loss_cls: 0.2132  loss_mask: 0.0833  loss_dice: 0.7365
2025/06/23 16:56:26 - mmengine - INFO - Epoch(train)  [1][1140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:06  time: 0.3278  data_time: 0.0049  memory: 2263  grad_norm: 125.0876  loss: 1.4502  loss_cls: 0.2942  loss_mask: 0.1570  loss_dice: 0.9990
2025/06/23 16:56:29 - mmengine - INFO - Epoch(train)  [1][1150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:24:00  time: 0.3260  data_time: 0.0005  memory: 2257  grad_norm: 62.2217  loss: 0.8763  loss_cls: 0.1361  loss_mask: 0.1198  loss_dice: 0.6204
2025/06/23 16:56:32 - mmengine - INFO - Epoch(train)  [1][1160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:54  time: 0.3255  data_time: 0.0028  memory: 2285  grad_norm: 39.8702  loss: 1.3510  loss_cls: 0.2783  loss_mask: 0.2221  loss_dice: 0.8505
2025/06/23 16:56:35 - mmengine - INFO - Epoch(train)  [1][1170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:49  time: 0.3291  data_time: 0.0026  memory: 2468  grad_norm: 46.0201  loss: 1.8805  loss_cls: 0.4425  loss_mask: 0.1750  loss_dice: 1.2631
2025/06/23 16:56:39 - mmengine - INFO - Epoch(train)  [1][1180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:43  time: 0.3266  data_time: 0.0051  memory: 2300  grad_norm: 18.3919  loss: 1.1109  loss_cls: 0.1450  loss_mask: 0.1148  loss_dice: 0.8512
2025/06/23 16:56:42 - mmengine - INFO - Epoch(train)  [1][1190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:36  time: 0.3257  data_time: 0.0022  memory: 2351  grad_norm: 43.4288  loss: 1.1678  loss_cls: 0.2376  loss_mask: 0.1585  loss_dice: 0.7716
2025/06/23 16:56:45 - mmengine - INFO - Epoch(train)  [1][1200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:31  time: 0.3275  data_time: 0.0066  memory: 2380  grad_norm: 90.4186  loss: 1.2524  loss_cls: 0.2615  loss_mask: 0.0870  loss_dice: 0.9039
2025/06/23 16:56:48 - mmengine - INFO - Epoch(train)  [1][1210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:25  time: 0.3256  data_time: 0.0013  memory: 2184  grad_norm: 29.9534  loss: 1.0935  loss_cls: 0.3288  loss_mask: 0.1019  loss_dice: 0.6627
2025/06/23 16:56:52 - mmengine - INFO - Epoch(train)  [1][1220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:20  time: 0.3285  data_time: 0.0027  memory: 2454  grad_norm: 47.6375  loss: 1.1317  loss_cls: 0.3087  loss_mask: 0.0905  loss_dice: 0.7326
2025/06/23 16:56:55 - mmengine - INFO - Epoch(train)  [1][1230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:13  time: 0.3230  data_time: 0.0019  memory: 2112  grad_norm: 62.7937  loss: 1.0434  loss_cls: 0.1668  loss_mask: 0.2363  loss_dice: 0.6403
2025/06/23 16:56:58 - mmengine - INFO - Epoch(train)  [1][1240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:07  time: 0.3259  data_time: 0.0050  memory: 2409  grad_norm: 74.2199  loss: 0.7375  loss_cls: 0.1853  loss_mask: 0.0424  loss_dice: 0.5098
2025/06/23 16:57:01 - mmengine - INFO - Epoch(train)  [1][1250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:02  time: 0.3268  data_time: 0.0030  memory: 2233  grad_norm: 119.2254  loss: 1.3762  loss_cls: 0.3959  loss_mask: 0.1653  loss_dice: 0.8149
2025/06/23 16:57:05 - mmengine - INFO - Epoch(train)  [1][1260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:55  time: 0.3236  data_time: 0.0006  memory: 2308  grad_norm: 129.7532  loss: 0.7319  loss_cls: 0.1652  loss_mask: 0.0800  loss_dice: 0.4868
2025/06/23 16:57:08 - mmengine - INFO - Epoch(train)  [1][1270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:49  time: 0.3232  data_time: 0.0057  memory: 2184  grad_norm: 16.5016  loss: 1.0988  loss_cls: 0.1851  loss_mask: 0.1230  loss_dice: 0.7907
2025/06/23 16:57:11 - mmengine - INFO - Epoch(train)  [1][1280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:44  time: 0.3295  data_time: 0.0033  memory: 2454  grad_norm: 124.5548  loss: 1.6193  loss_cls: 0.2921  loss_mask: 0.3363  loss_dice: 0.9909
2025/06/23 16:57:14 - mmengine - INFO - Epoch(train)  [1][1290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:37  time: 0.3227  data_time: 0.0003  memory: 2483  grad_norm: 33.0341  loss: 1.0353  loss_cls: 0.1865  loss_mask: 0.0822  loss_dice: 0.7667
2025/06/23 16:57:18 - mmengine - INFO - Epoch(train)  [1][1300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:32  time: 0.3268  data_time: 0.0057  memory: 2293  grad_norm: 199.9579  loss: 1.0990  loss_cls: 0.2807  loss_mask: 0.1049  loss_dice: 0.7133
2025/06/23 16:57:21 - mmengine - INFO - Epoch(train)  [1][1310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:27  time: 0.3286  data_time: 0.0022  memory: 2365  grad_norm: 75.2177  loss: 1.6649  loss_cls: 0.2473  loss_mask: 0.0991  loss_dice: 1.3186
2025/06/23 16:57:24 - mmengine - INFO - Epoch(train)  [1][1320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:23  time: 0.3315  data_time: 0.0060  memory: 2336  grad_norm: 79.0936  loss: 1.1952  loss_cls: 0.2533  loss_mask: 0.1333  loss_dice: 0.8086
2025/06/23 16:57:28 - mmengine - INFO - Epoch(train)  [1][1330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:17  time: 0.3222  data_time: 0.0004  memory: 2402  grad_norm: 11.6779  loss: 0.6595  loss_cls: 0.1281  loss_mask: 0.0405  loss_dice: 0.4908
2025/06/23 16:57:31 - mmengine - INFO - Epoch(train)  [1][1340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:11  time: 0.3234  data_time: 0.0010  memory: 2227  grad_norm: 20.9056  loss: 0.8257  loss_cls: 0.1743  loss_mask: 0.0607  loss_dice: 0.5906
2025/06/23 16:57:34 - mmengine - INFO - Epoch(train)  [1][1350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:05  time: 0.3261  data_time: 0.0042  memory: 2498  grad_norm: 62.4041  loss: 0.9799  loss_cls: 0.2434  loss_mask: 0.1001  loss_dice: 0.6364
2025/06/23 16:57:37 - mmengine - INFO - Epoch(train)  [1][1360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:00  time: 0.3269  data_time: 0.0017  memory: 2285  grad_norm: 73.0180  loss: 0.8838  loss_cls: 0.2117  loss_mask: 0.0934  loss_dice: 0.5787
2025/06/23 16:57:41 - mmengine - INFO - Epoch(train)  [1][1370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:54  time: 0.3236  data_time: 0.0009  memory: 2154  grad_norm: 82.2381  loss: 1.0880  loss_cls: 0.2184  loss_mask: 0.1946  loss_dice: 0.6750
2025/06/23 16:57:44 - mmengine - INFO - Epoch(train)  [1][1380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:49  time: 0.3260  data_time: 0.0042  memory: 2293  grad_norm: 48.2542  loss: 1.2277  loss_cls: 0.3881  loss_mask: 0.1104  loss_dice: 0.7292
2025/06/23 16:57:47 - mmengine - INFO - Epoch(train)  [1][1390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:45  time: 0.3313  data_time: 0.0019  memory: 2483  grad_norm: 23.3388  loss: 1.1774  loss_cls: 0.2501  loss_mask: 0.0855  loss_dice: 0.8417
2025/06/23 16:57:50 - mmengine - INFO - Epoch(train)  [1][1400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:40  time: 0.3296  data_time: 0.0035  memory: 2478  grad_norm: 268.8355  loss: 1.1752  loss_cls: 0.1774  loss_mask: 0.1452  loss_dice: 0.8526
2025/06/23 16:57:54 - mmengine - INFO - Epoch(train)  [1][1410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:35  time: 0.3266  data_time: 0.0035  memory: 2432  grad_norm: 56.5443  loss: 1.2534  loss_cls: 0.2883  loss_mask: 0.1264  loss_dice: 0.8387
2025/06/23 16:57:57 - mmengine - INFO - Epoch(train)  [1][1420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:30  time: 0.3266  data_time: 0.0040  memory: 2314  grad_norm: 54.4395  loss: 0.9779  loss_cls: 0.2611  loss_mask: 0.0698  loss_dice: 0.6470
2025/06/23 16:58:00 - mmengine - INFO - Epoch(train)  [1][1430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:26  time: 0.3278  data_time: 0.0011  memory: 2483  grad_norm: 57.2371  loss: 0.9661  loss_cls: 0.2007  loss_mask: 0.1053  loss_dice: 0.6600
2025/06/23 16:58:03 - mmengine - INFO - Epoch(train)  [1][1440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:21  time: 0.3291  data_time: 0.0053  memory: 2563  grad_norm: 80.8841  loss: 1.0593  loss_cls: 0.2055  loss_mask: 0.0694  loss_dice: 0.7844
2025/06/23 16:58:07 - mmengine - INFO - Epoch(train)  [1][1450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:16  time: 0.3265  data_time: 0.0026  memory: 2285  grad_norm: 50.8250  loss: 0.7382  loss_cls: 0.1533  loss_mask: 0.0508  loss_dice: 0.5341
2025/06/23 16:58:10 - mmengine - INFO - Epoch(train)  [1][1460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:12  time: 0.3291  data_time: 0.0042  memory: 2490  grad_norm: 224.6489  loss: 1.1650  loss_cls: 0.2017  loss_mask: 0.1274  loss_dice: 0.8359
2025/06/23 16:58:13 - mmengine - INFO - Epoch(train)  [1][1470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:08  time: 0.3316  data_time: 0.0035  memory: 2239  grad_norm: 52.6281  loss: 1.6225  loss_cls: 0.4507  loss_mask: 0.1136  loss_dice: 1.0582
2025/06/23 16:58:17 - mmengine - INFO - Epoch(train)  [1][1480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:03  time: 0.3279  data_time: 0.0034  memory: 2293  grad_norm: 79.7976  loss: 1.7460  loss_cls: 0.4935  loss_mask: 0.1190  loss_dice: 1.1334
2025/06/23 16:58:20 - mmengine - INFO - Epoch(train)  [1][1490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:58  time: 0.3264  data_time: 0.0018  memory: 2099  grad_norm: 303.5479  loss: 1.2056  loss_cls: 0.2370  loss_mask: 0.1880  loss_dice: 0.7806
2025/06/23 16:58:23 - mmengine - INFO - Epoch(train)  [1][1500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:54  time: 0.3294  data_time: 0.0025  memory: 2380  grad_norm: 22.0205  loss: 1.3156  loss_cls: 0.2900  loss_mask: 0.1062  loss_dice: 0.9194
2025/06/23 16:58:26 - mmengine - INFO - Epoch(train)  [1][1510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:49  time: 0.3251  data_time: 0.0052  memory: 2203  grad_norm: 28.7219  loss: 1.0654  loss_cls: 0.1534  loss_mask: 0.1238  loss_dice: 0.7882
2025/06/23 16:58:30 - mmengine - INFO - Epoch(train)  [1][1520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:44  time: 0.3269  data_time: 0.0054  memory: 2432  grad_norm: 23.6177  loss: 0.8492  loss_cls: 0.1144  loss_mask: 0.0853  loss_dice: 0.6495
2025/06/23 16:58:33 - mmengine - INFO - Epoch(train)  [1][1530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:43  time: 0.3405  data_time: 0.0034  memory: 2454  grad_norm: 61.5209  loss: 1.4833  loss_cls: 0.2750  loss_mask: 0.1714  loss_dice: 1.0368
2025/06/23 16:58:36 - mmengine - INFO - Epoch(train)  [1][1540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:40  time: 0.3369  data_time: 0.0038  memory: 2468  grad_norm: 32.5692  loss: 1.3282  loss_cls: 0.2752  loss_mask: 0.1024  loss_dice: 0.9506
2025/06/23 16:58:40 - mmengine - INFO - Epoch(train)  [1][1550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:36  time: 0.3294  data_time: 0.0024  memory: 2308  grad_norm: 34.6752  loss: 1.0116  loss_cls: 0.1865  loss_mask: 0.0962  loss_dice: 0.7289
2025/06/23 16:58:43 - mmengine - INFO - Epoch(train)  [1][1560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:32  time: 0.3285  data_time: 0.0018  memory: 2314  grad_norm: 30.9652  loss: 0.9119  loss_cls: 0.1649  loss_mask: 0.1286  loss_dice: 0.6183
2025/06/23 16:58:46 - mmengine - INFO - Epoch(train)  [1][1570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:28  time: 0.3333  data_time: 0.0027  memory: 2527  grad_norm: 138.4404  loss: 1.3317  loss_cls: 0.2803  loss_mask: 0.0779  loss_dice: 0.9734
2025/06/23 16:58:50 - mmengine - INFO - Epoch(train)  [1][1580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:24  time: 0.3289  data_time: 0.0023  memory: 2203  grad_norm: 68.3297  loss: 1.4380  loss_cls: 0.2634  loss_mask: 0.1640  loss_dice: 1.0106
2025/06/23 16:58:53 - mmengine - INFO - Epoch(train)  [1][1590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:21  time: 0.3355  data_time: 0.0066  memory: 2490  grad_norm: 25.9212  loss: 1.0264  loss_cls: 0.1621  loss_mask: 0.1554  loss_dice: 0.7089
2025/06/23 16:58:56 - mmengine - INFO - Epoch(train)  [1][1600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:16  time: 0.3244  data_time: 0.0036  memory: 2329  grad_norm: 59.5220  loss: 0.5827  loss_cls: 0.1430  loss_mask: 0.0486  loss_dice: 0.3910
2025/06/23 16:59:00 - mmengine - INFO - Epoch(train)  [1][1610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:13  time: 0.3328  data_time: 0.0022  memory: 2446  grad_norm: 20.5306  loss: 1.1138  loss_cls: 0.2180  loss_mask: 0.1073  loss_dice: 0.7885
2025/06/23 16:59:03 - mmengine - INFO - Epoch(train)  [1][1620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:08  time: 0.3279  data_time: 0.0043  memory: 2227  grad_norm: 14.4772  loss: 0.9893  loss_cls: 0.2440  loss_mask: 0.0948  loss_dice: 0.6505
2025/06/23 16:59:06 - mmengine - INFO - Epoch(train)  [1][1630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:05  time: 0.3334  data_time: 0.0026  memory: 2351  grad_norm: 225.0257  loss: 1.1975  loss_cls: 0.3030  loss_mask: 0.1090  loss_dice: 0.7854
2025/06/23 16:59:09 - mmengine - INFO - Epoch(train)  [1][1640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:00  time: 0.3253  data_time: 0.0020  memory: 2203  grad_norm: 19.8393  loss: 0.8752  loss_cls: 0.1874  loss_mask: 0.1868  loss_dice: 0.5010
2025/06/23 16:59:13 - mmengine - INFO - Epoch(train)  [1][1650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:56  time: 0.3321  data_time: 0.0020  memory: 2534  grad_norm: 38.7213  loss: 1.4474  loss_cls: 0.3030  loss_mask: 0.1133  loss_dice: 1.0311
2025/06/23 16:59:16 - mmengine - INFO - Epoch(train)  [1][1660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:54  time: 0.3361  data_time: 0.0054  memory: 2227  grad_norm: 62.8824  loss: 1.0329  loss_cls: 0.1960  loss_mask: 0.2811  loss_dice: 0.5558
2025/06/23 16:59:19 - mmengine - INFO - Epoch(train)  [1][1670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:51  time: 0.3341  data_time: 0.0032  memory: 2239  grad_norm: 52.2549  loss: 1.1469  loss_cls: 0.2259  loss_mask: 0.0871  loss_dice: 0.8338
2025/06/23 16:59:23 - mmengine - INFO - Epoch(train)  [1][1680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:48  time: 0.3357  data_time: 0.0020  memory: 2534  grad_norm: 32.7458  loss: 1.1113  loss_cls: 0.2759  loss_mask: 0.1215  loss_dice: 0.7138
2025/06/23 16:59:26 - mmengine - INFO - Epoch(train)  [1][1690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:44  time: 0.3287  data_time: 0.0025  memory: 2118  grad_norm: 57.5007  loss: 0.9185  loss_cls: 0.1391  loss_mask: 0.0921  loss_dice: 0.6873
2025/06/23 16:59:30 - mmengine - INFO - Epoch(train)  [1][1700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:41  time: 0.3377  data_time: 0.0035  memory: 2351  grad_norm: 20.4229  loss: 1.1315  loss_cls: 0.2881  loss_mask: 0.0672  loss_dice: 0.7762
2025/06/23 16:59:33 - mmengine - INFO - Epoch(train)  [1][1710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:39  time: 0.3370  data_time: 0.0023  memory: 2439  grad_norm: 27.9896  loss: 1.3314  loss_cls: 0.3215  loss_mask: 0.0917  loss_dice: 0.9182
2025/06/23 16:59:36 - mmengine - INFO - Epoch(train)  [1][1720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:35  time: 0.3309  data_time: 0.0041  memory: 2191  grad_norm: 90.3561  loss: 1.1327  loss_cls: 0.2440  loss_mask: 0.0618  loss_dice: 0.8270
2025/06/23 16:59:40 - mmengine - INFO - Epoch(train)  [1][1730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:32  time: 0.3341  data_time: 0.0021  memory: 2365  grad_norm: 49.4152  loss: 0.7614  loss_cls: 0.1617  loss_mask: 0.0691  loss_dice: 0.5305
2025/06/23 16:59:43 - mmengine - INFO - Epoch(train)  [1][1740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:29  time: 0.3336  data_time: 0.0037  memory: 2446  grad_norm: 219.9626  loss: 1.0181  loss_cls: 0.1695  loss_mask: 0.1144  loss_dice: 0.7342
2025/06/23 16:59:46 - mmengine - INFO - Epoch(train)  [1][1750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:25  time: 0.3286  data_time: 0.0022  memory: 2221  grad_norm: 19.6680  loss: 1.1483  loss_cls: 0.2597  loss_mask: 0.1205  loss_dice: 0.7682
2025/06/23 16:59:49 - mmengine - INFO - Epoch(train)  [1][1760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:20  time: 0.3258  data_time: 0.0007  memory: 2124  grad_norm: 88.3719  loss: 0.6994  loss_cls: 0.1333  loss_mask: 0.0650  loss_dice: 0.5012
2025/06/23 16:59:53 - mmengine - INFO - Epoch(train)  [1][1770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:16  time: 0.3319  data_time: 0.0029  memory: 2166  grad_norm: 36.3540  loss: 0.8010  loss_cls: 0.1617  loss_mask: 0.1152  loss_dice: 0.5241
2025/06/23 16:59:56 - mmengine - INFO - Epoch(train)  [1][1780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:14  time: 0.3399  data_time: 0.0031  memory: 2215  grad_norm: 40.9576  loss: 1.0757  loss_cls: 0.2806  loss_mask: 0.0607  loss_dice: 0.7344
2025/06/23 17:00:00 - mmengine - INFO - Epoch(train)  [1][1790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:12  time: 0.3377  data_time: 0.0029  memory: 2541  grad_norm: 182.3666  loss: 1.1390  loss_cls: 0.2930  loss_mask: 0.1111  loss_dice: 0.7350
2025/06/23 17:00:03 - mmengine - INFO - Epoch(train)  [1][1800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:09  time: 0.3331  data_time: 0.0018  memory: 2278  grad_norm: 43.8389  loss: 0.9610  loss_cls: 0.2688  loss_mask: 0.0611  loss_dice: 0.6311
2025/06/23 17:00:06 - mmengine - INFO - Epoch(train)  [1][1810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:05  time: 0.3320  data_time: 0.0036  memory: 2197  grad_norm: 34.3018  loss: 0.9359  loss_cls: 0.1742  loss_mask: 0.1629  loss_dice: 0.5989
2025/06/23 17:00:09 - mmengine - INFO - Epoch(train)  [1][1820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:01  time: 0.3291  data_time: 0.0026  memory: 2478  grad_norm: 66.7147  loss: 1.2971  loss_cls: 0.2721  loss_mask: 0.1212  loss_dice: 0.9038
2025/06/23 17:00:13 - mmengine - INFO - Epoch(train)  [1][1830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:56  time: 0.3245  data_time: 0.0012  memory: 2178  grad_norm: 28.9522  loss: 0.8836  loss_cls: 0.2629  loss_mask: 0.0683  loss_dice: 0.5524
2025/06/23 17:00:16 - mmengine - INFO - Epoch(train)  [1][1840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:51  time: 0.3257  data_time: 0.0052  memory: 2172  grad_norm: 103.6733  loss: 2.3042  loss_cls: 0.1677  loss_mask: 1.1702  loss_dice: 0.9663
2025/06/23 17:00:19 - mmengine - INFO - Epoch(train)  [1][1850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:47  time: 0.3271  data_time: 0.0017  memory: 2329  grad_norm: 63.2202  loss: 1.0877  loss_cls: 0.2297  loss_mask: 0.0744  loss_dice: 0.7837
2025/06/23 17:00:22 - mmengine - INFO - Epoch(train)  [1][1860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:43  time: 0.3272  data_time: 0.0045  memory: 2571  grad_norm: 12.7488  loss: 0.7665  loss_cls: 0.1615  loss_mask: 0.0699  loss_dice: 0.5351
2025/06/23 17:00:26 - mmengine - INFO - Epoch(train)  [1][1870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:38  time: 0.3283  data_time: 0.0030  memory: 2227  grad_norm: 35.2858  loss: 0.8550  loss_cls: 0.2116  loss_mask: 0.0830  loss_dice: 0.5603
2025/06/23 17:00:29 - mmengine - INFO - Epoch(train)  [1][1880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:34  time: 0.3269  data_time: 0.0018  memory: 2293  grad_norm: 54.7203  loss: 1.1461  loss_cls: 0.2599  loss_mask: 0.0944  loss_dice: 0.7918
2025/06/23 17:00:32 - mmengine - INFO - Epoch(train)  [1][1890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:29  time: 0.3254  data_time: 0.0024  memory: 2191  grad_norm: 38.0475  loss: 0.8753  loss_cls: 0.1650  loss_mask: 0.0949  loss_dice: 0.6155
2025/06/23 17:00:36 - mmengine - INFO - Epoch(train)  [1][1900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:25  time: 0.3271  data_time: 0.0010  memory: 2321  grad_norm: 16.5504  loss: 1.1893  loss_cls: 0.2397  loss_mask: 0.0860  loss_dice: 0.8636
2025/06/23 17:00:39 - mmengine - INFO - Epoch(train)  [1][1910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:21  time: 0.3270  data_time: 0.0028  memory: 2278  grad_norm: 34.1656  loss: 0.7803  loss_cls: 0.1671  loss_mask: 0.1587  loss_dice: 0.4545
2025/06/23 17:00:42 - mmengine - INFO - Epoch(train)  [1][1920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:17  time: 0.3311  data_time: 0.0004  memory: 2483  grad_norm: 51.4576  loss: 1.2056  loss_cls: 0.3264  loss_mask: 0.0703  loss_dice: 0.8089
2025/06/23 17:00:45 - mmengine - INFO - Epoch(train)  [1][1930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:13  time: 0.3285  data_time: 0.0013  memory: 2336  grad_norm: 36.7500  loss: 1.2281  loss_cls: 0.2527  loss_mask: 0.0727  loss_dice: 0.9027
2025/06/23 17:00:49 - mmengine - INFO - Epoch(train)  [1][1940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:09  time: 0.3320  data_time: 0.0054  memory: 2343  grad_norm: 39.8690  loss: 1.4159  loss_cls: 0.2910  loss_mask: 0.0904  loss_dice: 1.0345
2025/06/23 17:00:52 - mmengine - INFO - Epoch(train)  [1][1950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:06  time: 0.3307  data_time: 0.0038  memory: 2166  grad_norm: 240.6729  loss: 1.1968  loss_cls: 0.2343  loss_mask: 0.1651  loss_dice: 0.7974
2025/06/23 17:00:55 - mmengine - INFO - Epoch(train)  [1][1960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:02  time: 0.3284  data_time: 0.0047  memory: 2417  grad_norm: 60.3245  loss: 1.2090  loss_cls: 0.2700  loss_mask: 0.0675  loss_dice: 0.8715
2025/06/23 17:00:59 - mmengine - INFO - Epoch(train)  [1][1970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:58  time: 0.3332  data_time: 0.0053  memory: 2209  grad_norm: 22.2983  loss: 0.8077  loss_cls: 0.1573  loss_mask: 0.0826  loss_dice: 0.5678
2025/06/23 17:01:02 - mmengine - INFO - Epoch(train)  [1][1980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:55  time: 0.3328  data_time: 0.0037  memory: 2387  grad_norm: 43.9133  loss: 1.0788  loss_cls: 0.2462  loss_mask: 0.0899  loss_dice: 0.7427
2025/06/23 17:01:05 - mmengine - INFO - Epoch(train)  [1][1990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:52  time: 0.3336  data_time: 0.0015  memory: 2160  grad_norm: 67.8282  loss: 1.0618  loss_cls: 0.2883  loss_mask: 0.0708  loss_dice: 0.7027
2025/06/23 17:01:09 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:01:09 - mmengine - INFO - Epoch(train)  [1][2000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:49  time: 0.3365  data_time: 0.0029  memory: 2336  grad_norm: 108.9514  loss: 0.9307  loss_cls: 0.1674  loss_mask: 0.1662  loss_dice: 0.5971
2025/06/23 17:01:12 - mmengine - INFO - Epoch(train)  [1][2010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:49  time: 0.3512  data_time: 0.0070  memory: 2336  grad_norm: 35.2171  loss: 0.6768  loss_cls: 0.1791  loss_mask: 0.0756  loss_dice: 0.4222
2025/06/23 17:01:16 - mmengine - INFO - Epoch(train)  [1][2020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:47  time: 0.3380  data_time: 0.0022  memory: 2432  grad_norm: 72.4746  loss: 1.2274  loss_cls: 0.2708  loss_mask: 0.0777  loss_dice: 0.8789
2025/06/23 17:01:19 - mmengine - INFO - Epoch(train)  [1][2030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:44  time: 0.3334  data_time: 0.0024  memory: 2505  grad_norm: 29.2161  loss: 1.2716  loss_cls: 0.3154  loss_mask: 0.1544  loss_dice: 0.8017
2025/06/23 17:01:22 - mmengine - INFO - Epoch(train)  [1][2040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:42  time: 0.3422  data_time: 0.0029  memory: 2387  grad_norm: 10.7443  loss: 0.5317  loss_cls: 0.1017  loss_mask: 0.0565  loss_dice: 0.3735
2025/06/23 17:01:26 - mmengine - INFO - Epoch(train)  [1][2050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:40  time: 0.3413  data_time: 0.0034  memory: 2221  grad_norm: 59.5278  loss: 1.3980  loss_cls: 0.2483  loss_mask: 0.1322  loss_dice: 1.0176
2025/06/23 17:01:29 - mmengine - INFO - Epoch(train)  [1][2060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:36  time: 0.3300  data_time: 0.0033  memory: 2527  grad_norm: 34.4752  loss: 1.1619  loss_cls: 0.2263  loss_mask: 0.0913  loss_dice: 0.8443
2025/06/23 17:01:32 - mmengine - INFO - Epoch(train)  [1][2070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:33  time: 0.3332  data_time: 0.0026  memory: 2343  grad_norm: 21.4790  loss: 0.9780  loss_cls: 0.1946  loss_mask: 0.0950  loss_dice: 0.6885
2025/06/23 17:01:36 - mmengine - INFO - Epoch(train)  [1][2080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:30  time: 0.3315  data_time: 0.0053  memory: 2251  grad_norm: 57.2349  loss: 0.9604  loss_cls: 0.2101  loss_mask: 0.0545  loss_dice: 0.6958
2025/06/23 17:01:39 - mmengine - INFO - Epoch(train)  [1][2090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:26  time: 0.3294  data_time: 0.0015  memory: 2329  grad_norm: 79.9789  loss: 1.4333  loss_cls: 0.3839  loss_mask: 0.0994  loss_dice: 0.9499
2025/06/23 17:01:42 - mmengine - INFO - Epoch(train)  [1][2100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:22  time: 0.3286  data_time: 0.0038  memory: 2293  grad_norm: 277.9264  loss: 1.0640  loss_cls: 0.3680  loss_mask: 0.0535  loss_dice: 0.6425
2025/06/23 17:01:46 - mmengine - INFO - Epoch(train)  [1][2110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:18  time: 0.3321  data_time: 0.0018  memory: 2358  grad_norm: 54.4295  loss: 1.5914  loss_cls: 0.4130  loss_mask: 0.1080  loss_dice: 1.0704
2025/06/23 17:01:49 - mmengine - INFO - Epoch(train)  [1][2120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:15  time: 0.3306  data_time: 0.0017  memory: 2563  grad_norm: 48.6031  loss: 1.2744  loss_cls: 0.3324  loss_mask: 0.1862  loss_dice: 0.7558
2025/06/23 17:01:52 - mmengine - INFO - Epoch(train)  [1][2130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:10  time: 0.3260  data_time: 0.0029  memory: 2336  grad_norm: 21.7266  loss: 0.8229  loss_cls: 0.2459  loss_mask: 0.0757  loss_dice: 0.5013
2025/06/23 17:01:56 - mmengine - INFO - Epoch(train)  [1][2140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:09  time: 0.3435  data_time: 0.0032  memory: 2454  grad_norm: 77.0256  loss: 1.0027  loss_cls: 0.2460  loss_mask: 0.0811  loss_dice: 0.6756
2025/06/23 17:01:59 - mmengine - INFO - Epoch(train)  [1][2150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:05  time: 0.3280  data_time: 0.0033  memory: 2069  grad_norm: 128.4466  loss: 1.4024  loss_cls: 0.3229  loss_mask: 0.1562  loss_dice: 0.9232
2025/06/23 17:02:02 - mmengine - INFO - Epoch(train)  [1][2160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:00  time: 0.3281  data_time: 0.0013  memory: 2160  grad_norm: 37.8908  loss: 1.0415  loss_cls: 0.2462  loss_mask: 0.0804  loss_dice: 0.7148
2025/06/23 17:02:06 - mmengine - INFO - Epoch(train)  [1][2170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:57  time: 0.3311  data_time: 0.0032  memory: 2365  grad_norm: 52.6531  loss: 0.9795  loss_cls: 0.2383  loss_mask: 0.0698  loss_dice: 0.6713
2025/06/23 17:02:09 - mmengine - INFO - Epoch(train)  [1][2180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:53  time: 0.3322  data_time: 0.0041  memory: 2372  grad_norm: 65.6726  loss: 1.2425  loss_cls: 0.3609  loss_mask: 0.1154  loss_dice: 0.7663
2025/06/23 17:02:12 - mmengine - INFO - Epoch(train)  [1][2190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:50  time: 0.3311  data_time: 0.0009  memory: 2365  grad_norm: 51.0763  loss: 0.9749  loss_cls: 0.2252  loss_mask: 0.0646  loss_dice: 0.6851
2025/06/23 17:02:15 - mmengine - INFO - Epoch(train)  [1][2200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:46  time: 0.3273  data_time: 0.0031  memory: 2239  grad_norm: 47.5811  loss: 1.0564  loss_cls: 0.2746  loss_mask: 0.0741  loss_dice: 0.7077
2025/06/23 17:02:19 - mmengine - INFO - Epoch(train)  [1][2210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:42  time: 0.3337  data_time: 0.0023  memory: 2358  grad_norm: 93.5948  loss: 1.0007  loss_cls: 0.1949  loss_mask: 0.0961  loss_dice: 0.7097
2025/06/23 17:02:22 - mmengine - INFO - Epoch(train)  [1][2220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:39  time: 0.3333  data_time: 0.0052  memory: 2300  grad_norm: 20.7587  loss: 0.6320  loss_cls: 0.1287  loss_mask: 0.0880  loss_dice: 0.4154
2025/06/23 17:02:25 - mmengine - INFO - Epoch(train)  [1][2230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:36  time: 0.3361  data_time: 0.0022  memory: 2432  grad_norm: 189.5691  loss: 1.0749  loss_cls: 0.2314  loss_mask: 0.2322  loss_dice: 0.6113
2025/06/23 17:02:29 - mmengine - INFO - Epoch(train)  [1][2240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:33  time: 0.3335  data_time: 0.0013  memory: 2372  grad_norm: 35.6017  loss: 1.1885  loss_cls: 0.3462  loss_mask: 0.0981  loss_dice: 0.7443
2025/06/23 17:02:32 - mmengine - INFO - Epoch(train)  [1][2250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:29  time: 0.3280  data_time: 0.0049  memory: 2314  grad_norm: 45.2345  loss: 1.3443  loss_cls: 0.1608  loss_mask: 0.1454  loss_dice: 1.0381
2025/06/23 17:02:35 - mmengine - INFO - Epoch(train)  [1][2260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:26  time: 0.3334  data_time: 0.0019  memory: 2483  grad_norm: 44.0342  loss: 1.3589  loss_cls: 0.2999  loss_mask: 0.1003  loss_dice: 0.9587
2025/06/23 17:02:39 - mmengine - INFO - Epoch(train)  [1][2270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:23  time: 0.3325  data_time: 0.0024  memory: 2058  grad_norm: 30.7739  loss: 0.7401  loss_cls: 0.1414  loss_mask: 0.0552  loss_dice: 0.5434
2025/06/23 17:02:42 - mmengine - INFO - Epoch(train)  [1][2280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:19  time: 0.3292  data_time: 0.0012  memory: 2278  grad_norm: 45.2119  loss: 1.5224  loss_cls: 0.2411  loss_mask: 0.1032  loss_dice: 1.1780
2025/06/23 17:02:45 - mmengine - INFO - Epoch(train)  [1][2290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:16  time: 0.3343  data_time: 0.0060  memory: 2505  grad_norm: 61.7208  loss: 1.5267  loss_cls: 0.3124  loss_mask: 0.1662  loss_dice: 1.0482
2025/06/23 17:02:49 - mmengine - INFO - Epoch(train)  [1][2300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:11  time: 0.3263  data_time: 0.0050  memory: 2285  grad_norm: 41.4250  loss: 0.9465  loss_cls: 0.2532  loss_mask: 0.0494  loss_dice: 0.6439
2025/06/23 17:02:52 - mmengine - INFO - Epoch(train)  [1][2310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:07  time: 0.3259  data_time: 0.0042  memory: 2446  grad_norm: 34.1827  loss: 0.9467  loss_cls: 0.1664  loss_mask: 0.0524  loss_dice: 0.7279
2025/06/23 17:02:55 - mmengine - INFO - Epoch(train)  [1][2320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:03  time: 0.3289  data_time: 0.0036  memory: 2308  grad_norm: 45.1181  loss: 1.3046  loss_cls: 0.3395  loss_mask: 0.1078  loss_dice: 0.8573
2025/06/23 17:02:58 - mmengine - INFO - Epoch(train)  [1][2330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:59  time: 0.3264  data_time: 0.0021  memory: 2351  grad_norm: 30.1743  loss: 0.9150  loss_cls: 0.2066  loss_mask: 0.0785  loss_dice: 0.6299
2025/06/23 17:03:02 - mmengine - INFO - Epoch(train)  [1][2340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:55  time: 0.3295  data_time: 0.0026  memory: 2454  grad_norm: 45.8103  loss: 1.1325  loss_cls: 0.2481  loss_mask: 0.1025  loss_dice: 0.7820
2025/06/23 17:03:05 - mmengine - INFO - Epoch(train)  [1][2350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:51  time: 0.3253  data_time: 0.0063  memory: 2439  grad_norm: 13.5962  loss: 0.9686  loss_cls: 0.2056  loss_mask: 0.0951  loss_dice: 0.6679
2025/06/23 17:03:08 - mmengine - INFO - Epoch(train)  [1][2360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:47  time: 0.3301  data_time: 0.0013  memory: 2468  grad_norm: 143.7253  loss: 1.9233  loss_cls: 0.4669  loss_mask: 0.2456  loss_dice: 1.2109
2025/06/23 17:03:12 - mmengine - INFO - Epoch(train)  [1][2370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:43  time: 0.3275  data_time: 0.0017  memory: 2271  grad_norm: 81.1694  loss: 1.2462  loss_cls: 0.2370  loss_mask: 0.1408  loss_dice: 0.8684
2025/06/23 17:03:15 - mmengine - INFO - Epoch(train)  [1][2380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:38  time: 0.3244  data_time: 0.0031  memory: 2203  grad_norm: 16.7249  loss: 0.8064  loss_cls: 0.2383  loss_mask: 0.0414  loss_dice: 0.5267
2025/06/23 17:03:18 - mmengine - INFO - Epoch(train)  [1][2390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:34  time: 0.3253  data_time: 0.0017  memory: 2124  grad_norm: 93.9436  loss: 1.4361  loss_cls: 0.4369  loss_mask: 0.1147  loss_dice: 0.8845
2025/06/23 17:03:21 - mmengine - INFO - Epoch(train)  [1][2400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:31  time: 0.3356  data_time: 0.0022  memory: 2184  grad_norm: 34.0553  loss: 0.8644  loss_cls: 0.1807  loss_mask: 0.1015  loss_dice: 0.5821
2025/06/23 17:03:25 - mmengine - INFO - Epoch(train)  [1][2410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:28  time: 0.3339  data_time: 0.0024  memory: 2372  grad_norm: 33.5281  loss: 1.0666  loss_cls: 0.2844  loss_mask: 0.0812  loss_dice: 0.7010
2025/06/23 17:03:28 - mmengine - INFO - Epoch(train)  [1][2420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:24  time: 0.3292  data_time: 0.0023  memory: 2215  grad_norm: 81.6917  loss: 1.1241  loss_cls: 0.2928  loss_mask: 0.1071  loss_dice: 0.7241
2025/06/23 17:03:31 - mmengine - INFO - Epoch(train)  [1][2430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:20  time: 0.3241  data_time: 0.0017  memory: 2148  grad_norm: 17.6304  loss: 1.1819  loss_cls: 0.3199  loss_mask: 0.0561  loss_dice: 0.8059
2025/06/23 17:03:35 - mmengine - INFO - Epoch(train)  [1][2440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:17  time: 0.3336  data_time: 0.0021  memory: 2321  grad_norm: 34.6822  loss: 1.1287  loss_cls: 0.2498  loss_mask: 0.1288  loss_dice: 0.7501
2025/06/23 17:03:38 - mmengine - INFO - Epoch(train)  [1][2450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:14  time: 0.3400  data_time: 0.0024  memory: 2336  grad_norm: 31.0931  loss: 1.2620  loss_cls: 0.2688  loss_mask: 0.1307  loss_dice: 0.8626
2025/06/23 17:03:41 - mmengine - INFO - Epoch(train)  [1][2460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:12  time: 0.3379  data_time: 0.0029  memory: 2329  grad_norm: 17.0663  loss: 1.8208  loss_cls: 0.2075  loss_mask: 0.1758  loss_dice: 1.4375
2025/06/23 17:03:45 - mmengine - INFO - Epoch(train)  [1][2470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:08  time: 0.3306  data_time: 0.0025  memory: 2387  grad_norm: 39.8328  loss: 1.0973  loss_cls: 0.1954  loss_mask: 0.0904  loss_dice: 0.8116
2025/06/23 17:03:48 - mmengine - INFO - Epoch(train)  [1][2480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:04  time: 0.3257  data_time: 0.0022  memory: 2271  grad_norm: 92.1356  loss: 0.9183  loss_cls: 0.2381  loss_mask: 0.0628  loss_dice: 0.6174
2025/06/23 17:03:51 - mmengine - INFO - Epoch(train)  [1][2490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:00  time: 0.3287  data_time: 0.0013  memory: 2239  grad_norm: 38.8579  loss: 1.2577  loss_cls: 0.2703  loss_mask: 0.1605  loss_dice: 0.8269
2025/06/23 17:03:55 - mmengine - INFO - Epoch(train)  [1][2500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:56  time: 0.3268  data_time: 0.0016  memory: 2209  grad_norm: 116.7657  loss: 0.8509  loss_cls: 0.1810  loss_mask: 0.0741  loss_dice: 0.5958
2025/06/23 17:03:58 - mmengine - INFO - Epoch(train)  [1][2510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:53  time: 0.3317  data_time: 0.0023  memory: 2490  grad_norm: 509.9393  loss: 1.4945  loss_cls: 0.4070  loss_mask: 0.1091  loss_dice: 0.9784
2025/06/23 17:04:01 - mmengine - INFO - Epoch(train)  [1][2520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:48  time: 0.3271  data_time: 0.0021  memory: 2278  grad_norm: 38.5254  loss: 1.0858  loss_cls: 0.2750  loss_mask: 0.1216  loss_dice: 0.6893
2025/06/23 17:04:04 - mmengine - INFO - Epoch(train)  [1][2530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:44  time: 0.3264  data_time: 0.0030  memory: 2380  grad_norm: 50.4204  loss: 1.0177  loss_cls: 0.2086  loss_mask: 0.0831  loss_dice: 0.7260
2025/06/23 17:04:08 - mmengine - INFO - Epoch(train)  [1][2540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:40  time: 0.3274  data_time: 0.0020  memory: 2387  grad_norm: 124.8712  loss: 1.9326  loss_cls: 0.3274  loss_mask: 0.5247  loss_dice: 1.0805
2025/06/23 17:04:11 - mmengine - INFO - Epoch(train)  [1][2550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:36  time: 0.3271  data_time: 0.0020  memory: 2417  grad_norm: 115.3741  loss: 1.1150  loss_cls: 0.3841  loss_mask: 0.0754  loss_dice: 0.6555
2025/06/23 17:04:14 - mmengine - INFO - Epoch(train)  [1][2560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:32  time: 0.3281  data_time: 0.0037  memory: 2285  grad_norm: 33.6386  loss: 0.8921  loss_cls: 0.2524  loss_mask: 0.0659  loss_dice: 0.5737
2025/06/23 17:04:17 - mmengine - INFO - Epoch(train)  [1][2570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:29  time: 0.3287  data_time: 0.0024  memory: 2380  grad_norm: 180.3747  loss: 1.6113  loss_cls: 0.4682  loss_mask: 0.1915  loss_dice: 0.9517
2025/06/23 17:04:21 - mmengine - INFO - Epoch(train)  [1][2580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:25  time: 0.3300  data_time: 0.0025  memory: 2409  grad_norm: 31.9661  loss: 1.1249  loss_cls: 0.2898  loss_mask: 0.1074  loss_dice: 0.7277
2025/06/23 17:04:24 - mmengine - INFO - Epoch(train)  [1][2590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:21  time: 0.3304  data_time: 0.0022  memory: 2372  grad_norm: 149.7332  loss: 1.3500  loss_cls: 0.2901  loss_mask: 0.1509  loss_dice: 0.9090
2025/06/23 17:04:27 - mmengine - INFO - Epoch(train)  [1][2600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:17  time: 0.3263  data_time: 0.0022  memory: 2278  grad_norm: 94.5491  loss: 1.1402  loss_cls: 0.2999  loss_mask: 0.1004  loss_dice: 0.7398
2025/06/23 17:04:31 - mmengine - INFO - Epoch(train)  [1][2610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:14  time: 0.3351  data_time: 0.0029  memory: 2233  grad_norm: 82.5981  loss: 1.0561  loss_cls: 0.2686  loss_mask: 0.1114  loss_dice: 0.6761
2025/06/23 17:04:34 - mmengine - INFO - Epoch(train)  [1][2620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:11  time: 0.3288  data_time: 0.0023  memory: 2454  grad_norm: 116.5409  loss: 1.2122  loss_cls: 0.3721  loss_mask: 0.0883  loss_dice: 0.7518
2025/06/23 17:04:37 - mmengine - INFO - Epoch(train)  [1][2630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:06  time: 0.3265  data_time: 0.0023  memory: 2233  grad_norm: 37.0299  loss: 0.8023  loss_cls: 0.2254  loss_mask: 0.0395  loss_dice: 0.5374
2025/06/23 17:04:41 - mmengine - INFO - Epoch(train)  [1][2640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:03  time: 0.3282  data_time: 0.0022  memory: 2203  grad_norm: 97.2576  loss: 1.3724  loss_cls: 0.2569  loss_mask: 0.1065  loss_dice: 1.0090
2025/06/23 17:04:44 - mmengine - INFO - Epoch(train)  [1][2650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:59  time: 0.3277  data_time: 0.0023  memory: 2300  grad_norm: 74.8989  loss: 1.0294  loss_cls: 0.2520  loss_mask: 0.1063  loss_dice: 0.6711
2025/06/23 17:04:47 - mmengine - INFO - Epoch(train)  [1][2660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:55  time: 0.3300  data_time: 0.0025  memory: 2321  grad_norm: 48.4433  loss: 1.3581  loss_cls: 0.3458  loss_mask: 0.0970  loss_dice: 0.9153
2025/06/23 17:04:50 - mmengine - INFO - Epoch(train)  [1][2670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:51  time: 0.3274  data_time: 0.0023  memory: 2285  grad_norm: 29.1467  loss: 1.7650  loss_cls: 0.5033  loss_mask: 0.0984  loss_dice: 1.1633
2025/06/23 17:04:54 - mmengine - INFO - Epoch(train)  [1][2680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:47  time: 0.3281  data_time: 0.0022  memory: 2285  grad_norm: 50.2447  loss: 0.9604  loss_cls: 0.1827  loss_mask: 0.0947  loss_dice: 0.6831
2025/06/23 17:04:57 - mmengine - INFO - Epoch(train)  [1][2690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:43  time: 0.3278  data_time: 0.0022  memory: 2505  grad_norm: 198.3404  loss: 0.8415  loss_cls: 0.1813  loss_mask: 0.0994  loss_dice: 0.5608
2025/06/23 17:05:00 - mmengine - INFO - Epoch(train)  [1][2700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:40  time: 0.3288  data_time: 0.0022  memory: 2321  grad_norm: 19.4937  loss: 1.1640  loss_cls: 0.2575  loss_mask: 0.0764  loss_dice: 0.8302
2025/06/23 17:05:03 - mmengine - INFO - Epoch(train)  [1][2710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:35  time: 0.3239  data_time: 0.0024  memory: 2321  grad_norm: 57.2528  loss: 0.4962  loss_cls: 0.1064  loss_mask: 0.0665  loss_dice: 0.3232
2025/06/23 17:05:07 - mmengine - INFO - Epoch(train)  [1][2720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:31  time: 0.3281  data_time: 0.0019  memory: 2314  grad_norm: 24.6675  loss: 1.1732  loss_cls: 0.1465  loss_mask: 0.2038  loss_dice: 0.8229
2025/06/23 17:05:10 - mmengine - INFO - Epoch(train)  [1][2730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:28  time: 0.3287  data_time: 0.0022  memory: 2271  grad_norm: 127.9743  loss: 1.3961  loss_cls: 0.2805  loss_mask: 0.1065  loss_dice: 1.0091
2025/06/23 17:05:13 - mmengine - INFO - Epoch(train)  [1][2740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:24  time: 0.3270  data_time: 0.0018  memory: 2336  grad_norm: 24.0962  loss: 0.8380  loss_cls: 0.1915  loss_mask: 0.0603  loss_dice: 0.5862
2025/06/23 17:05:17 - mmengine - INFO - Epoch(train)  [1][2750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:20  time: 0.3316  data_time: 0.0022  memory: 2409  grad_norm: 87.7723  loss: 1.4498  loss_cls: 0.3604  loss_mask: 0.1737  loss_dice: 0.9156
2025/06/23 17:05:20 - mmengine - INFO - Epoch(train)  [1][2760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:16  time: 0.3250  data_time: 0.0021  memory: 2343  grad_norm: 47.4641  loss: 0.8508  loss_cls: 0.1907  loss_mask: 0.1506  loss_dice: 0.5095
2025/06/23 17:05:23 - mmengine - INFO - Epoch(train)  [1][2770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:12  time: 0.3290  data_time: 0.0018  memory: 2233  grad_norm: 212.8914  loss: 1.2955  loss_cls: 0.3167  loss_mask: 0.0945  loss_dice: 0.8843
2025/06/23 17:05:26 - mmengine - INFO - Epoch(train)  [1][2780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:09  time: 0.3290  data_time: 0.0021  memory: 2314  grad_norm: 144.7223  loss: 1.1128  loss_cls: 0.2190  loss_mask: 0.1106  loss_dice: 0.7832
2025/06/23 17:05:30 - mmengine - INFO - Epoch(train)  [1][2790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:05  time: 0.3304  data_time: 0.0040  memory: 2197  grad_norm: 29.9810  loss: 1.2706  loss_cls: 0.2525  loss_mask: 0.1573  loss_dice: 0.8607
2025/06/23 17:05:33 - mmengine - INFO - Epoch(train)  [1][2800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:01  time: 0.3251  data_time: 0.0029  memory: 2099  grad_norm: 41.4756  loss: 0.9659  loss_cls: 0.1305  loss_mask: 0.1314  loss_dice: 0.7040
2025/06/23 17:05:36 - mmengine - INFO - Epoch(train)  [1][2810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:57  time: 0.3237  data_time: 0.0022  memory: 2251  grad_norm: 61.1286  loss: 0.8328  loss_cls: 0.2116  loss_mask: 0.0612  loss_dice: 0.5600
2025/06/23 17:05:40 - mmengine - INFO - Epoch(train)  [1][2820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:53  time: 0.3318  data_time: 0.0021  memory: 2372  grad_norm: 76.0172  loss: 1.0805  loss_cls: 0.2172  loss_mask: 0.1311  loss_dice: 0.7322
2025/06/23 17:05:43 - mmengine - INFO - Epoch(train)  [1][2830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:51  time: 0.3373  data_time: 0.0035  memory: 2308  grad_norm: 91.3761  loss: 1.4831  loss_cls: 0.3793  loss_mask: 0.0901  loss_dice: 1.0138
2025/06/23 17:05:46 - mmengine - INFO - Epoch(train)  [1][2840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:47  time: 0.3285  data_time: 0.0026  memory: 2351  grad_norm: 34.1824  loss: 1.1685  loss_cls: 0.2480  loss_mask: 0.2455  loss_dice: 0.6751
2025/06/23 17:05:50 - mmengine - INFO - Epoch(train)  [1][2850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:44  time: 0.3310  data_time: 0.0014  memory: 2308  grad_norm: 68.0136  loss: 1.1359  loss_cls: 0.2932  loss_mask: 0.0946  loss_dice: 0.7480
2025/06/23 17:05:53 - mmengine - INFO - Epoch(train)  [1][2860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:40  time: 0.3284  data_time: 0.0028  memory: 2278  grad_norm: 83.5773  loss: 0.9266  loss_cls: 0.1866  loss_mask: 0.0602  loss_dice: 0.6798
2025/06/23 17:05:56 - mmengine - INFO - Epoch(train)  [1][2870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:36  time: 0.3274  data_time: 0.0013  memory: 2227  grad_norm: 39.3197  loss: 0.8264  loss_cls: 0.1879  loss_mask: 0.0852  loss_dice: 0.5534
2025/06/23 17:05:59 - mmengine - INFO - Epoch(train)  [1][2880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:33  time: 0.3341  data_time: 0.0011  memory: 2233  grad_norm: 47.9337  loss: 0.9742  loss_cls: 0.2759  loss_mask: 0.0796  loss_dice: 0.6187
2025/06/23 17:06:03 - mmengine - INFO - Epoch(train)  [1][2890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:29  time: 0.3256  data_time: 0.0033  memory: 2063  grad_norm: 42.2212  loss: 0.6788  loss_cls: 0.1032  loss_mask: 0.0815  loss_dice: 0.4940
2025/06/23 17:06:06 - mmengine - INFO - Epoch(train)  [1][2900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:26  time: 0.3374  data_time: 0.0056  memory: 2300  grad_norm: 25.3772  loss: 1.4009  loss_cls: 0.3437  loss_mask: 0.1142  loss_dice: 0.9430
2025/06/23 17:06:09 - mmengine - INFO - Epoch(train)  [1][2910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:23  time: 0.3313  data_time: 0.0013  memory: 2409  grad_norm: 117.4327  loss: 1.6356  loss_cls: 0.3955  loss_mask: 0.2373  loss_dice: 1.0028
2025/06/23 17:06:13 - mmengine - INFO - Epoch(train)  [1][2920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:19  time: 0.3290  data_time: 0.0034  memory: 2446  grad_norm: 48.6853  loss: 1.3463  loss_cls: 0.3652  loss_mask: 0.1264  loss_dice: 0.8547
2025/06/23 17:06:16 - mmengine - INFO - Epoch(train)  [1][2930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:15  time: 0.3229  data_time: 0.0005  memory: 2215  grad_norm: 15.2416  loss: 1.2333  loss_cls: 0.2786  loss_mask: 0.0946  loss_dice: 0.8601
2025/06/23 17:06:19 - mmengine - INFO - Epoch(train)  [1][2940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:11  time: 0.3287  data_time: 0.0050  memory: 2251  grad_norm: 62.2264  loss: 1.7620  loss_cls: 0.4520  loss_mask: 0.1438  loss_dice: 1.1662
2025/06/23 17:06:22 - mmengine - INFO - Epoch(train)  [1][2950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:07  time: 0.3294  data_time: 0.0016  memory: 2271  grad_norm: 42.7374  loss: 1.1456  loss_cls: 0.2610  loss_mask: 0.0794  loss_dice: 0.8052
2025/06/23 17:06:26 - mmengine - INFO - Epoch(train)  [1][2960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:03  time: 0.3259  data_time: 0.0011  memory: 2343  grad_norm: 57.7982  loss: 1.3281  loss_cls: 0.2896  loss_mask: 0.1860  loss_dice: 0.8525
2025/06/23 17:06:29 - mmengine - INFO - Epoch(train)  [1][2970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:00  time: 0.3271  data_time: 0.0032  memory: 2321  grad_norm: 44.4168  loss: 1.6481  loss_cls: 0.4399  loss_mask: 0.1318  loss_dice: 1.0765
2025/06/23 17:06:32 - mmengine - INFO - Epoch(train)  [1][2980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:56  time: 0.3268  data_time: 0.0028  memory: 2372  grad_norm: 22.6108  loss: 0.9084  loss_cls: 0.2053  loss_mask: 0.0762  loss_dice: 0.6269
2025/06/23 17:06:36 - mmengine - INFO - Epoch(train)  [1][2990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:52  time: 0.3269  data_time: 0.0025  memory: 2505  grad_norm: 41.1112  loss: 1.0515  loss_cls: 0.2841  loss_mask: 0.0931  loss_dice: 0.6743
2025/06/23 17:06:39 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:06:39 - mmengine - INFO - Epoch(train)  [1][3000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:48  time: 0.3267  data_time: 0.0016  memory: 2372  grad_norm: 79.6657  loss: 1.5196  loss_cls: 0.3106  loss_mask: 0.2348  loss_dice: 0.9742
2025/06/23 17:06:42 - mmengine - INFO - Epoch(train)  [1][3010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:45  time: 0.3317  data_time: 0.0027  memory: 2372  grad_norm: 72.3435  loss: 1.4781  loss_cls: 0.3635  loss_mask: 0.1340  loss_dice: 0.9806
2025/06/23 17:06:45 - mmengine - INFO - Epoch(train)  [1][3020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:41  time: 0.3321  data_time: 0.0013  memory: 2490  grad_norm: 35.2419  loss: 1.7942  loss_cls: 0.4205  loss_mask: 0.1280  loss_dice: 1.2457
2025/06/23 17:06:49 - mmengine - INFO - Epoch(train)  [1][3030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:38  time: 0.3318  data_time: 0.0023  memory: 2380  grad_norm: 61.9035  loss: 1.3250  loss_cls: 0.3494  loss_mask: 0.1169  loss_dice: 0.8586
2025/06/23 17:06:52 - mmengine - INFO - Epoch(train)  [1][3040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:34  time: 0.3300  data_time: 0.0007  memory: 2271  grad_norm: 226.6694  loss: 1.6417  loss_cls: 0.3071  loss_mask: 0.1883  loss_dice: 1.1463
2025/06/23 17:06:55 - mmengine - INFO - Epoch(train)  [1][3050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:31  time: 0.3325  data_time: 0.0020  memory: 2483  grad_norm: 74.3220  loss: 1.5732  loss_cls: 0.3226  loss_mask: 0.0996  loss_dice: 1.1510
2025/06/23 17:06:59 - mmengine - INFO - Epoch(train)  [1][3060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:28  time: 0.3299  data_time: 0.0006  memory: 2372  grad_norm: 163.8519  loss: 1.2993  loss_cls: 0.2448  loss_mask: 0.1101  loss_dice: 0.9443
2025/06/23 17:07:02 - mmengine - INFO - Epoch(train)  [1][3070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:24  time: 0.3260  data_time: 0.0031  memory: 2239  grad_norm: 116.0460  loss: 0.9407  loss_cls: 0.2713  loss_mask: 0.0867  loss_dice: 0.5827
2025/06/23 17:07:05 - mmengine - INFO - Epoch(train)  [1][3080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:20  time: 0.3325  data_time: 0.0046  memory: 2343  grad_norm: 80.3752  loss: 1.3772  loss_cls: 0.2609  loss_mask: 0.1388  loss_dice: 0.9775
2025/06/23 17:07:09 - mmengine - INFO - Epoch(train)  [1][3090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:16  time: 0.3246  data_time: 0.0056  memory: 2278  grad_norm: 60.5750  loss: 0.8169  loss_cls: 0.1811  loss_mask: 0.0717  loss_dice: 0.5641
2025/06/23 17:07:12 - mmengine - INFO - Epoch(train)  [1][3100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:13  time: 0.3274  data_time: 0.0019  memory: 2505  grad_norm: 29.8434  loss: 1.0440  loss_cls: 0.2561  loss_mask: 0.0885  loss_dice: 0.6994
2025/06/23 17:07:15 - mmengine - INFO - Epoch(train)  [1][3110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:09  time: 0.3282  data_time: 0.0047  memory: 2409  grad_norm: 59.5013  loss: 0.9856  loss_cls: 0.2077  loss_mask: 0.0952  loss_dice: 0.6827
2025/06/23 17:07:18 - mmengine - INFO - Epoch(train)  [1][3120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:05  time: 0.3309  data_time: 0.0049  memory: 2343  grad_norm: 105.1966  loss: 1.0419  loss_cls: 0.2664  loss_mask: 0.1075  loss_dice: 0.6681
2025/06/23 17:07:22 - mmengine - INFO - Epoch(train)  [1][3130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:03  time: 0.3363  data_time: 0.0025  memory: 2358  grad_norm: 180.7306  loss: 1.2037  loss_cls: 0.3241  loss_mask: 0.0985  loss_dice: 0.7811
2025/06/23 17:07:25 - mmengine - INFO - Epoch(train)  [1][3140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:00  time: 0.3388  data_time: 0.0034  memory: 2278  grad_norm: 64.6817  loss: 1.3446  loss_cls: 0.3788  loss_mask: 0.1009  loss_dice: 0.8649
2025/06/23 17:07:28 - mmengine - INFO - Epoch(train)  [1][3150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:57  time: 0.3348  data_time: 0.0024  memory: 2351  grad_norm: 85.7977  loss: 1.1570  loss_cls: 0.2764  loss_mask: 0.1094  loss_dice: 0.7712
2025/06/23 17:07:32 - mmengine - INFO - Epoch(train)  [1][3160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:53  time: 0.3277  data_time: 0.0016  memory: 2351  grad_norm: 16.9771  loss: 0.8207  loss_cls: 0.1902  loss_mask: 0.1097  loss_dice: 0.5208
2025/06/23 17:07:35 - mmengine - INFO - Epoch(train)  [1][3170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:50  time: 0.3308  data_time: 0.0028  memory: 2124  grad_norm: 52.8793  loss: 1.1708  loss_cls: 0.2168  loss_mask: 0.1058  loss_dice: 0.8483
2025/06/23 17:07:38 - mmengine - INFO - Epoch(train)  [1][3180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:46  time: 0.3242  data_time: 0.0025  memory: 2300  grad_norm: 37.2897  loss: 0.6626  loss_cls: 0.1300  loss_mask: 0.0861  loss_dice: 0.4464
2025/06/23 17:07:42 - mmengine - INFO - Epoch(train)  [1][3190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:42  time: 0.3307  data_time: 0.0026  memory: 2365  grad_norm: 55.5896  loss: 1.1674  loss_cls: 0.2319  loss_mask: 0.1428  loss_dice: 0.7928
2025/06/23 17:07:45 - mmengine - INFO - Epoch(train)  [1][3200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:39  time: 0.3273  data_time: 0.0009  memory: 2402  grad_norm: 41.7361  loss: 0.8987  loss_cls: 0.2284  loss_mask: 0.0764  loss_dice: 0.5939
2025/06/23 17:07:48 - mmengine - INFO - Epoch(train)  [1][3210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:35  time: 0.3279  data_time: 0.0039  memory: 2112  grad_norm: 60.0021  loss: 0.9106  loss_cls: 0.1276  loss_mask: 0.0863  loss_dice: 0.6967
2025/06/23 17:07:51 - mmengine - INFO - Epoch(train)  [1][3220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:31  time: 0.3302  data_time: 0.0028  memory: 2142  grad_norm: 120.0796  loss: 1.4447  loss_cls: 0.3311  loss_mask: 0.1066  loss_dice: 1.0070
2025/06/23 17:07:55 - mmengine - INFO - Epoch(train)  [1][3230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:28  time: 0.3272  data_time: 0.0033  memory: 2099  grad_norm: 75.7680  loss: 1.1184  loss_cls: 0.2391  loss_mask: 0.1103  loss_dice: 0.7691
2025/06/23 17:07:58 - mmengine - INFO - Epoch(train)  [1][3240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:24  time: 0.3272  data_time: 0.0029  memory: 2343  grad_norm: 43.1860  loss: 1.5997  loss_cls: 0.3392  loss_mask: 0.0815  loss_dice: 1.1790
2025/06/23 17:08:01 - mmengine - INFO - Epoch(train)  [1][3250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:20  time: 0.3308  data_time: 0.0065  memory: 2308  grad_norm: 91.0385  loss: 1.3430  loss_cls: 0.3010  loss_mask: 0.0977  loss_dice: 0.9443
2025/06/23 17:08:05 - mmengine - INFO - Epoch(train)  [1][3260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:17  time: 0.3325  data_time: 0.0054  memory: 2245  grad_norm: 64.2269  loss: 1.5694  loss_cls: 0.3105  loss_mask: 0.1917  loss_dice: 1.0673
2025/06/23 17:08:08 - mmengine - INFO - Epoch(train)  [1][3270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:14  time: 0.3360  data_time: 0.0031  memory: 2172  grad_norm: 117.5532  loss: 1.1158  loss_cls: 0.2833  loss_mask: 0.1053  loss_dice: 0.7272
2025/06/23 17:08:11 - mmengine - INFO - Epoch(train)  [1][3280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:11  time: 0.3271  data_time: 0.0030  memory: 2227  grad_norm: 27.7493  loss: 1.1017  loss_cls: 0.1598  loss_mask: 0.1234  loss_dice: 0.8185
2025/06/23 17:08:15 - mmengine - INFO - Epoch(train)  [1][3290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:07  time: 0.3255  data_time: 0.0012  memory: 2251  grad_norm: 86.0904  loss: 1.0972  loss_cls: 0.2484  loss_mask: 0.0674  loss_dice: 0.7814
2025/06/23 17:08:18 - mmengine - INFO - Epoch(train)  [1][3300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:03  time: 0.3301  data_time: 0.0047  memory: 2271  grad_norm: 32.1698  loss: 1.0864  loss_cls: 0.2597  loss_mask: 0.0672  loss_dice: 0.7594
2025/06/23 17:08:21 - mmengine - INFO - Epoch(train)  [1][3310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:00  time: 0.3289  data_time: 0.0043  memory: 2124  grad_norm: 64.5386  loss: 1.7933  loss_cls: 0.2616  loss_mask: 0.5878  loss_dice: 0.9439
2025/06/23 17:08:24 - mmengine - INFO - Epoch(train)  [1][3320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:55  time: 0.3241  data_time: 0.0023  memory: 2271  grad_norm: 54.5814  loss: 0.9395  loss_cls: 0.2078  loss_mask: 0.1252  loss_dice: 0.6066
2025/06/23 17:08:28 - mmengine - INFO - Epoch(train)  [1][3330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:52  time: 0.3275  data_time: 0.0011  memory: 2166  grad_norm: 272.3935  loss: 1.0606  loss_cls: 0.2426  loss_mask: 0.2169  loss_dice: 0.6011
2025/06/23 17:08:31 - mmengine - INFO - Epoch(train)  [1][3340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:49  time: 0.3355  data_time: 0.0019  memory: 2336  grad_norm: 49.3565  loss: 1.5358  loss_cls: 0.2939  loss_mask: 0.0845  loss_dice: 1.1574
2025/06/23 17:08:34 - mmengine - INFO - Epoch(train)  [1][3350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:45  time: 0.3288  data_time: 0.0037  memory: 2490  grad_norm: 158.6002  loss: 1.5122  loss_cls: 0.4246  loss_mask: 0.1110  loss_dice: 0.9767
2025/06/23 17:08:38 - mmengine - INFO - Epoch(train)  [1][3360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:42  time: 0.3281  data_time: 0.0028  memory: 2446  grad_norm: 65.6506  loss: 1.0972  loss_cls: 0.3181  loss_mask: 0.0813  loss_dice: 0.6979
2025/06/23 17:08:41 - mmengine - INFO - Epoch(train)  [1][3370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:38  time: 0.3298  data_time: 0.0030  memory: 2446  grad_norm: 97.3399  loss: 1.2868  loss_cls: 0.3219  loss_mask: 0.0884  loss_dice: 0.8765
2025/06/23 17:08:44 - mmengine - INFO - Epoch(train)  [1][3380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:34  time: 0.3222  data_time: 0.0029  memory: 2172  grad_norm: 25.5860  loss: 0.6558  loss_cls: 0.1503  loss_mask: 0.0905  loss_dice: 0.4149
2025/06/23 17:08:47 - mmengine - INFO - Epoch(train)  [1][3390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:30  time: 0.3256  data_time: 0.0049  memory: 2106  grad_norm: 22.2704  loss: 0.9203  loss_cls: 0.1378  loss_mask: 0.0900  loss_dice: 0.6925
2025/06/23 17:08:51 - mmengine - INFO - Epoch(train)  [1][3400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:26  time: 0.3289  data_time: 0.0010  memory: 2454  grad_norm: 88.1159  loss: 1.2752  loss_cls: 0.3183  loss_mask: 0.0900  loss_dice: 0.8668
2025/06/23 17:08:54 - mmengine - INFO - Epoch(train)  [1][3410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:22  time: 0.3264  data_time: 0.0036  memory: 2271  grad_norm: 179.6174  loss: 1.2890  loss_cls: 0.2334  loss_mask: 0.1791  loss_dice: 0.8765
2025/06/23 17:08:57 - mmengine - INFO - Epoch(train)  [1][3420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:19  time: 0.3279  data_time: 0.0007  memory: 2285  grad_norm: 80.3134  loss: 1.0880  loss_cls: 0.1651  loss_mask: 0.1034  loss_dice: 0.8195
2025/06/23 17:09:00 - mmengine - INFO - Epoch(train)  [1][3430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:15  time: 0.3255  data_time: 0.0024  memory: 2365  grad_norm: 28.3619  loss: 1.2317  loss_cls: 0.1952  loss_mask: 0.0919  loss_dice: 0.9446
2025/06/23 17:09:04 - mmengine - INFO - Epoch(train)  [1][3440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:11  time: 0.3255  data_time: 0.0032  memory: 2209  grad_norm: 90.5815  loss: 1.3160  loss_cls: 0.3220  loss_mask: 0.1148  loss_dice: 0.8792
2025/06/23 17:09:07 - mmengine - INFO - Epoch(train)  [1][3450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:07  time: 0.3273  data_time: 0.0007  memory: 2300  grad_norm: 42.8040  loss: 1.2366  loss_cls: 0.2888  loss_mask: 0.1983  loss_dice: 0.7496
2025/06/23 17:09:10 - mmengine - INFO - Epoch(train)  [1][3460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:03  time: 0.3263  data_time: 0.0017  memory: 2454  grad_norm: 110.0462  loss: 1.4437  loss_cls: 0.3270  loss_mask: 0.1250  loss_dice: 0.9917
2025/06/23 17:09:13 - mmengine - INFO - Epoch(train)  [1][3470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:59  time: 0.3225  data_time: 0.0023  memory: 2172  grad_norm: 26.0819  loss: 1.0455  loss_cls: 0.1649  loss_mask: 0.1762  loss_dice: 0.7044
2025/06/23 17:09:17 - mmengine - INFO - Epoch(train)  [1][3480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:56  time: 0.3326  data_time: 0.0028  memory: 2519  grad_norm: 73.9465  loss: 1.5421  loss_cls: 0.3824  loss_mask: 0.1085  loss_dice: 1.0513
2025/06/23 17:09:20 - mmengine - INFO - Epoch(train)  [1][3490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:52  time: 0.3275  data_time: 0.0003  memory: 2380  grad_norm: 29.1479  loss: 1.1945  loss_cls: 0.3277  loss_mask: 0.0852  loss_dice: 0.7817
2025/06/23 17:09:23 - mmengine - INFO - Epoch(train)  [1][3500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:49  time: 0.3276  data_time: 0.0036  memory: 2505  grad_norm: 43.8286  loss: 0.8853  loss_cls: 0.2040  loss_mask: 0.0625  loss_dice: 0.6189
2025/06/23 17:09:27 - mmengine - INFO - Epoch(train)  [1][3510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:45  time: 0.3259  data_time: 0.0038  memory: 2221  grad_norm: 66.6773  loss: 1.5058  loss_cls: 0.2975  loss_mask: 0.1158  loss_dice: 1.0925
2025/06/23 17:09:30 - mmengine - INFO - Epoch(train)  [1][3520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:41  time: 0.3268  data_time: 0.0007  memory: 2321  grad_norm: 16.6735  loss: 1.4040  loss_cls: 0.2701  loss_mask: 0.0652  loss_dice: 1.0687
2025/06/23 17:09:33 - mmengine - INFO - Epoch(train)  [1][3530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:38  time: 0.3299  data_time: 0.0022  memory: 2387  grad_norm: 85.6564  loss: 1.1769  loss_cls: 0.2950  loss_mask: 0.1081  loss_dice: 0.7738
2025/06/23 17:09:36 - mmengine - INFO - Epoch(train)  [1][3540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:34  time: 0.3297  data_time: 0.0040  memory: 2321  grad_norm: 107.9585  loss: 1.7138  loss_cls: 0.4943  loss_mask: 0.1180  loss_dice: 1.1015
2025/06/23 17:09:40 - mmengine - INFO - Epoch(train)  [1][3550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:30  time: 0.3255  data_time: 0.0038  memory: 2293  grad_norm: 52.0525  loss: 0.7213  loss_cls: 0.1589  loss_mask: 0.0864  loss_dice: 0.4760
2025/06/23 17:09:43 - mmengine - INFO - Epoch(train)  [1][3560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:27  time: 0.3279  data_time: 0.0030  memory: 2278  grad_norm: 220.9205  loss: 1.2396  loss_cls: 0.3740  loss_mask: 0.0944  loss_dice: 0.7713
2025/06/23 17:09:46 - mmengine - INFO - Epoch(train)  [1][3570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:23  time: 0.3316  data_time: 0.0024  memory: 2541  grad_norm: 100.7322  loss: 1.5694  loss_cls: 0.3124  loss_mask: 0.0921  loss_dice: 1.1649
2025/06/23 17:09:50 - mmengine - INFO - Epoch(train)  [1][3580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:20  time: 0.3330  data_time: 0.0036  memory: 2417  grad_norm: 58.9666  loss: 1.1613  loss_cls: 0.3273  loss_mask: 0.0935  loss_dice: 0.7404
2025/06/23 17:09:53 - mmengine - INFO - Epoch(train)  [1][3590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:17  time: 0.3282  data_time: 0.0028  memory: 2221  grad_norm: 65.6928  loss: 1.0799  loss_cls: 0.2937  loss_mask: 0.0709  loss_dice: 0.7153
2025/06/23 17:09:56 - mmengine - INFO - Epoch(train)  [1][3600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:13  time: 0.3308  data_time: 0.0016  memory: 2387  grad_norm: 53.0621  loss: 1.1954  loss_cls: 0.3082  loss_mask: 0.1552  loss_dice: 0.7321
2025/06/23 17:10:00 - mmengine - INFO - Epoch(train)  [1][3610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:10  time: 0.3331  data_time: 0.0022  memory: 2245  grad_norm: 130.9070  loss: 1.3697  loss_cls: 0.2140  loss_mask: 0.0915  loss_dice: 1.0641
2025/06/23 17:10:03 - mmengine - INFO - Epoch(train)  [1][3620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:06  time: 0.3241  data_time: 0.0020  memory: 2112  grad_norm: 50.8187  loss: 1.3638  loss_cls: 0.3632  loss_mask: 0.1028  loss_dice: 0.8978
2025/06/23 17:10:06 - mmengine - INFO - Epoch(train)  [1][3630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:03  time: 0.3288  data_time: 0.0030  memory: 2285  grad_norm: 365.4840  loss: 1.0304  loss_cls: 0.2553  loss_mask: 0.0786  loss_dice: 0.6965
2025/06/23 17:10:09 - mmengine - INFO - Epoch(train)  [1][3640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:59  time: 0.3326  data_time: 0.0026  memory: 2300  grad_norm: 57.6202  loss: 1.5994  loss_cls: 0.3778  loss_mask: 0.1051  loss_dice: 1.1165
2025/06/23 17:10:13 - mmengine - INFO - Epoch(train)  [1][3650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:58  time: 0.3502  data_time: 0.0047  memory: 2519  grad_norm: 131.6575  loss: 1.2221  loss_cls: 0.2520  loss_mask: 0.1422  loss_dice: 0.8279
2025/06/23 17:10:16 - mmengine - INFO - Epoch(train)  [1][3660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:54  time: 0.3274  data_time: 0.0026  memory: 2402  grad_norm: 100.2609  loss: 1.0012  loss_cls: 0.2688  loss_mask: 0.0880  loss_dice: 0.6444
2025/06/23 17:10:19 - mmengine - INFO - Epoch(train)  [1][3670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:51  time: 0.3276  data_time: 0.0011  memory: 2380  grad_norm: 106.1443  loss: 1.6904  loss_cls: 0.4968  loss_mask: 0.1093  loss_dice: 1.0844
2025/06/23 17:10:23 - mmengine - INFO - Epoch(train)  [1][3680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:47  time: 0.3250  data_time: 0.0061  memory: 2160  grad_norm: 85.0887  loss: 1.2842  loss_cls: 0.4439  loss_mask: 0.0835  loss_dice: 0.7568
2025/06/23 17:10:26 - mmengine - INFO - Epoch(train)  [1][3690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:43  time: 0.3294  data_time: 0.0033  memory: 2417  grad_norm: 225.2539  loss: 1.9743  loss_cls: 0.4773  loss_mask: 0.1487  loss_dice: 1.3483
2025/06/23 17:10:29 - mmengine - INFO - Epoch(train)  [1][3700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:39  time: 0.3271  data_time: 0.0007  memory: 2483  grad_norm: 48.7037  loss: 1.4180  loss_cls: 0.4565  loss_mask: 0.1917  loss_dice: 0.7698
2025/06/23 17:10:33 - mmengine - INFO - Epoch(train)  [1][3710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:35  time: 0.3241  data_time: 0.0044  memory: 2136  grad_norm: 275.7621  loss: 1.2225  loss_cls: 0.2947  loss_mask: 0.1452  loss_dice: 0.7826
2025/06/23 17:10:36 - mmengine - INFO - Epoch(train)  [1][3720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:32  time: 0.3262  data_time: 0.0028  memory: 2402  grad_norm: 64.6526  loss: 1.8009  loss_cls: 0.3592  loss_mask: 0.1376  loss_dice: 1.3041
2025/06/23 17:10:39 - mmengine - INFO - Epoch(train)  [1][3730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:28  time: 0.3231  data_time: 0.0034  memory: 2233  grad_norm: 147.4491  loss: 1.1444  loss_cls: 0.2770  loss_mask: 0.1018  loss_dice: 0.7656
2025/06/23 17:10:42 - mmengine - INFO - Epoch(train)  [1][3740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:25  time: 0.3351  data_time: 0.0073  memory: 2257  grad_norm: 143.7199  loss: 1.4770  loss_cls: 0.3680  loss_mask: 0.1511  loss_dice: 0.9579
2025/06/23 17:10:46 - mmengine - INFO - Epoch(train)  [1][3750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:21  time: 0.3313  data_time: 0.0030  memory: 2308  grad_norm: 62.6761  loss: 1.2061  loss_cls: 0.3349  loss_mask: 0.0998  loss_dice: 0.7713
2025/06/23 17:10:49 - mmengine - INFO - Epoch(train)  [1][3760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:18  time: 0.3332  data_time: 0.0033  memory: 2372  grad_norm: 146.4833  loss: 1.3946  loss_cls: 0.3328  loss_mask: 0.1250  loss_dice: 0.9368
2025/06/23 17:10:52 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:10:52 - mmengine - INFO - Saving checkpoint at 1 epochs
2025/06/23 17:11:10 - mmengine - INFO - Epoch(val)  [1][ 10/209]    eta: 0:03:07  time: 0.9443  data_time: 0.5378  memory: 2387  
2025/06/23 17:11:14 - mmengine - INFO - Epoch(val)  [1][ 20/209]    eta: 0:02:15  time: 0.4893  data_time: 0.1207  memory: 1964  
2025/06/23 17:11:19 - mmengine - INFO - Epoch(val)  [1][ 30/209]    eta: 0:01:53  time: 0.4717  data_time: 0.0918  memory: 1964  
2025/06/23 17:11:24 - mmengine - INFO - Epoch(val)  [1][ 40/209]    eta: 0:01:40  time: 0.4780  data_time: 0.0989  memory: 1964  
2025/06/23 17:11:28 - mmengine - INFO - Epoch(val)  [1][ 50/209]    eta: 0:01:30  time: 0.4486  data_time: 0.0739  memory: 1964  
2025/06/23 17:11:34 - mmengine - INFO - Epoch(val)  [1][ 60/209]    eta: 0:01:24  time: 0.5537  data_time: 0.1611  memory: 1964  
2025/06/23 17:11:39 - mmengine - INFO - Epoch(val)  [1][ 70/209]    eta: 0:01:16  time: 0.4780  data_time: 0.0975  memory: 1964  
2025/06/23 17:11:44 - mmengine - INFO - Epoch(val)  [1][ 80/209]    eta: 0:01:10  time: 0.4990  data_time: 0.1114  memory: 1964  
2025/06/23 17:11:49 - mmengine - INFO - Epoch(val)  [1][ 90/209]    eta: 0:01:04  time: 0.5301  data_time: 0.1361  memory: 1964  
2025/06/23 17:11:54 - mmengine - INFO - Epoch(val)  [1][100/209]    eta: 0:00:58  time: 0.5005  data_time: 0.1087  memory: 1964  
2025/06/23 17:11:59 - mmengine - INFO - Epoch(val)  [1][110/209]    eta: 0:00:53  time: 0.5159  data_time: 0.1128  memory: 1964  
2025/06/23 17:12:04 - mmengine - INFO - Epoch(val)  [1][120/209]    eta: 0:00:47  time: 0.5118  data_time: 0.0954  memory: 1964  
2025/06/23 17:12:09 - mmengine - INFO - Epoch(val)  [1][130/209]    eta: 0:00:42  time: 0.5000  data_time: 0.0902  memory: 1964  
2025/06/23 17:12:14 - mmengine - INFO - Epoch(val)  [1][140/209]    eta: 0:00:36  time: 0.4690  data_time: 0.0740  memory: 1964  
2025/06/23 17:12:19 - mmengine - INFO - Epoch(val)  [1][150/209]    eta: 0:00:30  time: 0.4760  data_time: 0.0761  memory: 1964  
2025/06/23 17:12:24 - mmengine - INFO - Epoch(val)  [1][160/209]    eta: 0:00:25  time: 0.5146  data_time: 0.1100  memory: 1964  
2025/06/23 17:12:29 - mmengine - INFO - Epoch(val)  [1][170/209]    eta: 0:00:20  time: 0.5204  data_time: 0.1038  memory: 1964  
2025/06/23 17:12:34 - mmengine - INFO - Epoch(val)  [1][180/209]    eta: 0:00:15  time: 0.5386  data_time: 0.1362  memory: 1964  
2025/06/23 17:12:39 - mmengine - INFO - Epoch(val)  [1][190/209]    eta: 0:00:09  time: 0.4860  data_time: 0.0825  memory: 1964  
2025/06/23 17:12:44 - mmengine - INFO - Epoch(val)  [1][200/209]    eta: 0:00:04  time: 0.4702  data_time: 0.0557  memory: 1964  
2025/06/23 17:12:50 - mmengine - INFO - Evaluating segm...
2025/06/23 17:13:00 - mmengine - INFO - segm_mAP_copypaste: 0.381 0.612 0.408 0.177 0.497 0.718
2025/06/23 17:13:00 - mmengine - INFO - Epoch(val) [1][209/209]    coco/segm_mAP: 0.3810  coco/segm_mAP_50: 0.6120  coco/segm_mAP_75: 0.4080  coco/segm_mAP_s: 0.1770  coco/segm_mAP_m: 0.4970  coco/segm_mAP_l: 0.7180  data_time: 0.1221  time: 0.5178
2025/06/23 17:13:08 - mmengine - INFO - Epoch(train)  [2][  10/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:49  time: 0.7452  data_time: 0.4052  memory: 2425  grad_norm: 96.5429  loss: 1.7050  loss_cls: 0.4198  loss_mask: 0.0956  loss_dice: 1.1895
2025/06/23 17:13:11 - mmengine - INFO - Epoch(train)  [2][  20/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:45  time: 0.3238  data_time: 0.0027  memory: 2124  grad_norm: 51.5392  loss: 0.9329  loss_cls: 0.1027  loss_mask: 0.1127  loss_dice: 0.7175
2025/06/23 17:13:14 - mmengine - INFO - Epoch(train)  [2][  30/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:41  time: 0.3319  data_time: 0.0039  memory: 2409  grad_norm: 60.7072  loss: 1.3526  loss_cls: 0.3992  loss_mask: 0.0871  loss_dice: 0.8663
2025/06/23 17:13:18 - mmengine - INFO - Epoch(train)  [2][  40/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:38  time: 0.3284  data_time: 0.0025  memory: 2118  grad_norm: 287.6641  loss: 1.0538  loss_cls: 0.2692  loss_mask: 0.1013  loss_dice: 0.6832
2025/06/23 17:13:21 - mmengine - INFO - Epoch(train)  [2][  50/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:34  time: 0.3283  data_time: 0.0016  memory: 2329  grad_norm: 80.3567  loss: 1.0107  loss_cls: 0.2783  loss_mask: 0.0765  loss_dice: 0.6559
2025/06/23 17:13:24 - mmengine - INFO - Epoch(train)  [2][  60/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:30  time: 0.3260  data_time: 0.0042  memory: 2285  grad_norm: 38.3363  loss: 1.1240  loss_cls: 0.2721  loss_mask: 0.1301  loss_dice: 0.7219
2025/06/23 17:13:28 - mmengine - INFO - Epoch(train)  [2][  70/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:27  time: 0.3336  data_time: 0.0033  memory: 2358  grad_norm: 57.1057  loss: 1.7044  loss_cls: 0.4034  loss_mask: 0.1526  loss_dice: 1.1484
2025/06/23 17:13:31 - mmengine - INFO - Epoch(train)  [2][  80/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:23  time: 0.3277  data_time: 0.0038  memory: 2308  grad_norm: 78.8916  loss: 1.2767  loss_cls: 0.3105  loss_mask: 0.1136  loss_dice: 0.8526
2025/06/23 17:13:34 - mmengine - INFO - Epoch(train)  [2][  90/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:19  time: 0.3260  data_time: 0.0031  memory: 2197  grad_norm: 102.7136  loss: 1.2077  loss_cls: 0.3327  loss_mask: 0.0902  loss_dice: 0.7848
2025/06/23 17:13:37 - mmengine - INFO - Epoch(train)  [2][ 100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:15  time: 0.3251  data_time: 0.0014  memory: 2321  grad_norm: 30.1781  loss: 1.1245  loss_cls: 0.1837  loss_mask: 0.0841  loss_dice: 0.8567
2025/06/23 17:13:41 - mmengine - INFO - Epoch(train)  [2][ 110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:12  time: 0.3343  data_time: 0.0020  memory: 2454  grad_norm: 84.3257  loss: 1.8455  loss_cls: 0.3972  loss_mask: 0.1318  loss_dice: 1.3165
2025/06/23 17:13:44 - mmengine - INFO - Epoch(train)  [2][ 120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:08  time: 0.3265  data_time: 0.0021  memory: 2372  grad_norm: 217.3663  loss: 1.3187  loss_cls: 0.3286  loss_mask: 0.1532  loss_dice: 0.8369
2025/06/23 17:13:47 - mmengine - INFO - Epoch(train)  [2][ 130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:05  time: 0.3276  data_time: 0.0026  memory: 2271  grad_norm: 162.8501  loss: 1.0242  loss_cls: 0.3029  loss_mask: 0.0486  loss_dice: 0.6727
2025/06/23 17:13:51 - mmengine - INFO - Epoch(train)  [2][ 140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:01  time: 0.3251  data_time: 0.0017  memory: 2372  grad_norm: 56.8578  loss: 0.8728  loss_cls: 0.2500  loss_mask: 0.0941  loss_dice: 0.5286
2025/06/23 17:13:54 - mmengine - INFO - Epoch(train)  [2][ 150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:57  time: 0.3278  data_time: 0.0020  memory: 2483  grad_norm: 39.6863  loss: 1.2130  loss_cls: 0.2725  loss_mask: 0.0855  loss_dice: 0.8550
2025/06/23 17:13:57 - mmengine - INFO - Epoch(train)  [2][ 160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:53  time: 0.3277  data_time: 0.0031  memory: 2245  grad_norm: 85.7606  loss: 1.2572  loss_cls: 0.3327  loss_mask: 0.0751  loss_dice: 0.8494
2025/06/23 17:14:00 - mmengine - INFO - Epoch(train)  [2][ 170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:50  time: 0.3313  data_time: 0.0013  memory: 2351  grad_norm: 164.1466  loss: 1.4201  loss_cls: 0.3400  loss_mask: 0.1268  loss_dice: 0.9533
2025/06/23 17:14:04 - mmengine - INFO - Epoch(train)  [2][ 180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:47  time: 0.3348  data_time: 0.0013  memory: 2300  grad_norm: 134.7208  loss: 1.5417  loss_cls: 0.4493  loss_mask: 0.1433  loss_dice: 0.9491
2025/06/23 17:14:07 - mmengine - INFO - Epoch(train)  [2][ 190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:43  time: 0.3311  data_time: 0.0037  memory: 2278  grad_norm: 28.9579  loss: 1.3277  loss_cls: 0.2272  loss_mask: 0.1413  loss_dice: 0.9592
2025/06/23 17:14:10 - mmengine - INFO - Epoch(train)  [2][ 200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:40  time: 0.3340  data_time: 0.0027  memory: 2402  grad_norm: 202.8168  loss: 1.4870  loss_cls: 0.3363  loss_mask: 0.1214  loss_dice: 1.0293
2025/06/23 17:14:14 - mmengine - INFO - Epoch(train)  [2][ 210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:38  time: 0.3428  data_time: 0.0021  memory: 2490  grad_norm: 84.8139  loss: 1.6522  loss_cls: 0.4594  loss_mask: 0.1343  loss_dice: 1.0585
2025/06/23 17:14:17 - mmengine - INFO - Epoch(train)  [2][ 220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:35  time: 0.3378  data_time: 0.0016  memory: 2394  grad_norm: 149.6266  loss: 1.4378  loss_cls: 0.4039  loss_mask: 0.0828  loss_dice: 0.9511
2025/06/23 17:14:21 - mmengine - INFO - Epoch(train)  [2][ 230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:32  time: 0.3326  data_time: 0.0021  memory: 2314  grad_norm: 87.4460  loss: 0.9517  loss_cls: 0.1832  loss_mask: 0.0635  loss_dice: 0.7051
2025/06/23 17:14:21 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:14:24 - mmengine - INFO - Epoch(train)  [2][ 240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:28  time: 0.3300  data_time: 0.0056  memory: 2239  grad_norm: 55.9401  loss: 1.2590  loss_cls: 0.2728  loss_mask: 0.1396  loss_dice: 0.8467
2025/06/23 17:14:27 - mmengine - INFO - Epoch(train)  [2][ 250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:25  time: 0.3356  data_time: 0.0043  memory: 2417  grad_norm: 57.1692  loss: 1.3849  loss_cls: 0.2659  loss_mask: 0.1289  loss_dice: 0.9901
2025/06/23 17:14:31 - mmengine - INFO - Epoch(train)  [2][ 260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:22  time: 0.3340  data_time: 0.0024  memory: 2541  grad_norm: 74.0892  loss: 1.4600  loss_cls: 0.3416  loss_mask: 0.1311  loss_dice: 0.9873
2025/06/23 17:14:34 - mmengine - INFO - Epoch(train)  [2][ 270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:19  time: 0.3393  data_time: 0.0035  memory: 2351  grad_norm: 73.2977  loss: 0.8120  loss_cls: 0.1287  loss_mask: 0.0561  loss_dice: 0.6272
2025/06/23 17:14:37 - mmengine - INFO - Epoch(train)  [2][ 280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:16  time: 0.3310  data_time: 0.0040  memory: 2432  grad_norm: 118.7078  loss: 1.3826  loss_cls: 0.3765  loss_mask: 0.0878  loss_dice: 0.9183
2025/06/23 17:14:41 - mmengine - INFO - Epoch(train)  [2][ 290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:14  time: 0.3449  data_time: 0.0038  memory: 2483  grad_norm: 83.5525  loss: 1.1765  loss_cls: 0.2566  loss_mask: 0.1127  loss_dice: 0.8072
2025/06/23 17:14:44 - mmengine - INFO - Epoch(train)  [2][ 300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:10  time: 0.3277  data_time: 0.0024  memory: 2387  grad_norm: 162.2702  loss: 0.8575  loss_cls: 0.2491  loss_mask: 0.1050  loss_dice: 0.5035
2025/06/23 17:14:47 - mmengine - INFO - Epoch(train)  [2][ 310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:07  time: 0.3333  data_time: 0.0026  memory: 2505  grad_norm: 98.8128  loss: 1.4569  loss_cls: 0.2741  loss_mask: 0.1632  loss_dice: 1.0196
2025/06/23 17:14:51 - mmengine - INFO - Epoch(train)  [2][ 320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:04  time: 0.3399  data_time: 0.0021  memory: 2519  grad_norm: 81.6194  loss: 1.5680  loss_cls: 0.2754  loss_mask: 0.1277  loss_dice: 1.1648
2025/06/23 17:14:54 - mmengine - INFO - Epoch(train)  [2][ 330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:00  time: 0.3292  data_time: 0.0012  memory: 2197  grad_norm: 104.9847  loss: 1.0843  loss_cls: 0.1835  loss_mask: 0.0853  loss_dice: 0.8155
2025/06/23 17:14:57 - mmengine - INFO - Epoch(train)  [2][ 340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:57  time: 0.3300  data_time: 0.0024  memory: 2257  grad_norm: 43.5773  loss: 1.4131  loss_cls: 0.2724  loss_mask: 0.1052  loss_dice: 1.0356
2025/06/23 17:15:01 - mmengine - INFO - Epoch(train)  [2][ 350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:53  time: 0.3235  data_time: 0.0028  memory: 2203  grad_norm: 35.1649  loss: 0.6555  loss_cls: 0.0925  loss_mask: 0.0523  loss_dice: 0.5108
2025/06/23 17:15:04 - mmengine - INFO - Epoch(train)  [2][ 360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:49  time: 0.3292  data_time: 0.0034  memory: 2439  grad_norm: 88.4269  loss: 0.9868  loss_cls: 0.1930  loss_mask: 0.1862  loss_dice: 0.6076
2025/06/23 17:15:07 - mmengine - INFO - Epoch(train)  [2][ 370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:46  time: 0.3328  data_time: 0.0009  memory: 2321  grad_norm: 89.2639  loss: 1.3332  loss_cls: 0.3222  loss_mask: 0.1176  loss_dice: 0.8935
2025/06/23 17:15:10 - mmengine - INFO - Epoch(train)  [2][ 380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:43  time: 0.3325  data_time: 0.0026  memory: 2490  grad_norm: 41.1175  loss: 1.0749  loss_cls: 0.2449  loss_mask: 0.0961  loss_dice: 0.7338
2025/06/23 17:15:14 - mmengine - INFO - Epoch(train)  [2][ 390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:39  time: 0.3296  data_time: 0.0045  memory: 2130  grad_norm: 10.8084  loss: 0.9978  loss_cls: 0.0557  loss_mask: 0.0620  loss_dice: 0.8801
2025/06/23 17:15:17 - mmengine - INFO - Epoch(train)  [2][ 400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:36  time: 0.3325  data_time: 0.0029  memory: 2454  grad_norm: 181.4412  loss: 1.6846  loss_cls: 0.3781  loss_mask: 0.1005  loss_dice: 1.2060
2025/06/23 17:15:20 - mmengine - INFO - Epoch(train)  [2][ 410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:32  time: 0.3250  data_time: 0.0010  memory: 2321  grad_norm: 29.4648  loss: 0.9469  loss_cls: 0.1594  loss_mask: 0.1111  loss_dice: 0.6764
2025/06/23 17:15:24 - mmengine - INFO - Epoch(train)  [2][ 420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:29  time: 0.3362  data_time: 0.0027  memory: 2285  grad_norm: 70.4171  loss: 1.0506  loss_cls: 0.2404  loss_mask: 0.1097  loss_dice: 0.7005
2025/06/23 17:15:27 - mmengine - INFO - Epoch(train)  [2][ 430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:25  time: 0.3232  data_time: 0.0019  memory: 2052  grad_norm: 24.4963  loss: 0.5009  loss_cls: 0.1168  loss_mask: 0.0853  loss_dice: 0.2988
2025/06/23 17:15:30 - mmengine - INFO - Epoch(train)  [2][ 440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:21  time: 0.3281  data_time: 0.0032  memory: 2321  grad_norm: 66.3840  loss: 1.0169  loss_cls: 0.1963  loss_mask: 0.0898  loss_dice: 0.7307
2025/06/23 17:15:34 - mmengine - INFO - Epoch(train)  [2][ 450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:18  time: 0.3300  data_time: 0.0008  memory: 2446  grad_norm: 112.0304  loss: 1.1807  loss_cls: 0.3299  loss_mask: 0.1015  loss_dice: 0.7493
2025/06/23 17:15:37 - mmengine - INFO - Epoch(train)  [2][ 460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:14  time: 0.3300  data_time: 0.0034  memory: 2300  grad_norm: 52.6925  loss: 1.3307  loss_cls: 0.3088  loss_mask: 0.0882  loss_dice: 0.9337
2025/06/23 17:15:40 - mmengine - INFO - Epoch(train)  [2][ 470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:11  time: 0.3283  data_time: 0.0024  memory: 2106  grad_norm: 36.4116  loss: 0.9204  loss_cls: 0.2349  loss_mask: 0.0948  loss_dice: 0.5907
2025/06/23 17:15:43 - mmengine - INFO - Epoch(train)  [2][ 480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:07  time: 0.3321  data_time: 0.0030  memory: 2387  grad_norm: 52.6005  loss: 0.8659  loss_cls: 0.1814  loss_mask: 0.0992  loss_dice: 0.5854
2025/06/23 17:15:47 - mmengine - INFO - Epoch(train)  [2][ 490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:04  time: 0.3299  data_time: 0.0011  memory: 2343  grad_norm: 82.6923  loss: 1.3059  loss_cls: 0.3037  loss_mask: 0.0839  loss_dice: 0.9183
2025/06/23 17:15:50 - mmengine - INFO - Epoch(train)  [2][ 500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:05:00  time: 0.3253  data_time: 0.0023  memory: 2293  grad_norm: 64.4090  loss: 1.0216  loss_cls: 0.2422  loss_mask: 0.1676  loss_dice: 0.6118
2025/06/23 17:15:53 - mmengine - INFO - Epoch(train)  [2][ 510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:56  time: 0.3308  data_time: 0.0040  memory: 2365  grad_norm: 23.1295  loss: 1.5164  loss_cls: 0.2182  loss_mask: 0.1012  loss_dice: 1.1970
2025/06/23 17:15:57 - mmengine - INFO - Epoch(train)  [2][ 520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:53  time: 0.3339  data_time: 0.0020  memory: 2142  grad_norm: 37.9063  loss: 1.1647  loss_cls: 0.2571  loss_mask: 0.0882  loss_dice: 0.8193
2025/06/23 17:16:00 - mmengine - INFO - Epoch(train)  [2][ 530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:51  time: 0.3409  data_time: 0.0028  memory: 2534  grad_norm: 43.0072  loss: 0.9571  loss_cls: 0.1827  loss_mask: 0.1660  loss_dice: 0.6084
2025/06/23 17:16:03 - mmengine - INFO - Epoch(train)  [2][ 540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:47  time: 0.3261  data_time: 0.0016  memory: 2285  grad_norm: 23.7654  loss: 0.9150  loss_cls: 0.2422  loss_mask: 0.0672  loss_dice: 0.6056
2025/06/23 17:16:07 - mmengine - INFO - Epoch(train)  [2][ 550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:44  time: 0.3329  data_time: 0.0026  memory: 2314  grad_norm: 105.6797  loss: 1.3898  loss_cls: 0.3924  loss_mask: 0.1058  loss_dice: 0.8916
2025/06/23 17:16:10 - mmengine - INFO - Epoch(train)  [2][ 560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:40  time: 0.3317  data_time: 0.0037  memory: 2099  grad_norm: 35.1142  loss: 1.0848  loss_cls: 0.2737  loss_mask: 0.1131  loss_dice: 0.6980
2025/06/23 17:16:13 - mmengine - INFO - Epoch(train)  [2][ 570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:37  time: 0.3340  data_time: 0.0022  memory: 2271  grad_norm: 213.9204  loss: 1.4421  loss_cls: 0.3437  loss_mask: 0.1349  loss_dice: 0.9636
2025/06/23 17:16:17 - mmengine - INFO - Epoch(train)  [2][ 580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:34  time: 0.3321  data_time: 0.0021  memory: 2321  grad_norm: 503.5267  loss: 1.4651  loss_cls: 0.3397  loss_mask: 0.1065  loss_dice: 1.0189
2025/06/23 17:16:20 - mmengine - INFO - Epoch(train)  [2][ 590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:30  time: 0.3327  data_time: 0.0021  memory: 2454  grad_norm: 48.4313  loss: 1.5072  loss_cls: 0.3748  loss_mask: 0.1412  loss_dice: 0.9912
2025/06/23 17:16:23 - mmengine - INFO - Epoch(train)  [2][ 600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:27  time: 0.3354  data_time: 0.0036  memory: 2278  grad_norm: 159.9894  loss: 0.8636  loss_cls: 0.1961  loss_mask: 0.0807  loss_dice: 0.5867
2025/06/23 17:16:27 - mmengine - INFO - Epoch(train)  [2][ 610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:24  time: 0.3287  data_time: 0.0053  memory: 2387  grad_norm: 191.8756  loss: 1.2813  loss_cls: 0.3317  loss_mask: 0.0914  loss_dice: 0.8582
2025/06/23 17:16:30 - mmengine - INFO - Epoch(train)  [2][ 620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:20  time: 0.3308  data_time: 0.0010  memory: 2387  grad_norm: 38.9014  loss: 1.2798  loss_cls: 0.3689  loss_mask: 0.0929  loss_dice: 0.8179
2025/06/23 17:16:33 - mmengine - INFO - Epoch(train)  [2][ 630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:17  time: 0.3275  data_time: 0.0004  memory: 2409  grad_norm: 95.5412  loss: 1.4951  loss_cls: 0.3748  loss_mask: 0.0805  loss_dice: 1.0398
2025/06/23 17:16:36 - mmengine - INFO - Epoch(train)  [2][ 640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:13  time: 0.3289  data_time: 0.0030  memory: 2343  grad_norm: 267.6861  loss: 1.1676  loss_cls: 0.3059  loss_mask: 0.1128  loss_dice: 0.7488
2025/06/23 17:16:40 - mmengine - INFO - Epoch(train)  [2][ 650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:09  time: 0.3261  data_time: 0.0004  memory: 2372  grad_norm: 39.2778  loss: 1.0561  loss_cls: 0.2318  loss_mask: 0.0908  loss_dice: 0.7334
2025/06/23 17:16:43 - mmengine - INFO - Epoch(train)  [2][ 660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:06  time: 0.3283  data_time: 0.0040  memory: 2527  grad_norm: 40.2444  loss: 0.9813  loss_cls: 0.2340  loss_mask: 0.0891  loss_dice: 0.6583
2025/06/23 17:16:46 - mmengine - INFO - Epoch(train)  [2][ 670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:04:02  time: 0.3249  data_time: 0.0020  memory: 2209  grad_norm: 81.5864  loss: 0.8573  loss_cls: 0.1625  loss_mask: 0.0596  loss_dice: 0.6352
2025/06/23 17:16:50 - mmengine - INFO - Epoch(train)  [2][ 680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:58  time: 0.3291  data_time: 0.0018  memory: 2461  grad_norm: 87.7953  loss: 1.0987  loss_cls: 0.2673  loss_mask: 0.0959  loss_dice: 0.7355
2025/06/23 17:16:53 - mmengine - INFO - Epoch(train)  [2][ 690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:55  time: 0.3281  data_time: 0.0025  memory: 2278  grad_norm: 86.7175  loss: 1.2493  loss_cls: 0.2915  loss_mask: 0.1298  loss_dice: 0.8280
2025/06/23 17:16:56 - mmengine - INFO - Epoch(train)  [2][ 700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:51  time: 0.3306  data_time: 0.0056  memory: 2439  grad_norm: 209.1463  loss: 1.2048  loss_cls: 0.2784  loss_mask: 0.1265  loss_dice: 0.7999
2025/06/23 17:16:59 - mmengine - INFO - Epoch(train)  [2][ 710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:48  time: 0.3263  data_time: 0.0033  memory: 2209  grad_norm: 102.1011  loss: 1.1739  loss_cls: 0.1934  loss_mask: 0.0989  loss_dice: 0.8817
2025/06/23 17:17:03 - mmengine - INFO - Epoch(train)  [2][ 720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:44  time: 0.3271  data_time: 0.0015  memory: 2245  grad_norm: 165.0108  loss: 1.1683  loss_cls: 0.2414  loss_mask: 0.1108  loss_dice: 0.8161
2025/06/23 17:17:06 - mmengine - INFO - Epoch(train)  [2][ 730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:40  time: 0.3266  data_time: 0.0007  memory: 2257  grad_norm: 114.1886  loss: 1.5909  loss_cls: 0.2821  loss_mask: 0.4477  loss_dice: 0.8611
2025/06/23 17:17:09 - mmengine - INFO - Epoch(train)  [2][ 740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:37  time: 0.3311  data_time: 0.0022  memory: 2461  grad_norm: 53.5162  loss: 1.3406  loss_cls: 0.3766  loss_mask: 0.1074  loss_dice: 0.8566
2025/06/23 17:17:12 - mmengine - INFO - Epoch(train)  [2][ 750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:33  time: 0.3269  data_time: 0.0026  memory: 2483  grad_norm: 35.6852  loss: 1.0915  loss_cls: 0.2802  loss_mask: 0.1020  loss_dice: 0.7093
2025/06/23 17:17:16 - mmengine - INFO - Epoch(train)  [2][ 760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:29  time: 0.3260  data_time: 0.0023  memory: 2358  grad_norm: 47.7572  loss: 1.0325  loss_cls: 0.2076  loss_mask: 0.0921  loss_dice: 0.7328
2025/06/23 17:17:19 - mmengine - INFO - Epoch(train)  [2][ 770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:26  time: 0.3274  data_time: 0.0034  memory: 2387  grad_norm: 98.3340  loss: 1.4898  loss_cls: 0.3540  loss_mask: 0.1037  loss_dice: 1.0320
2025/06/23 17:17:22 - mmengine - INFO - Epoch(train)  [2][ 780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:22  time: 0.3258  data_time: 0.0054  memory: 2505  grad_norm: 239.4785  loss: 1.5678  loss_cls: 0.5589  loss_mask: 0.1173  loss_dice: 0.8916
2025/06/23 17:17:26 - mmengine - INFO - Epoch(train)  [2][ 790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:18  time: 0.3241  data_time: 0.0004  memory: 2257  grad_norm: 260.4888  loss: 0.9406  loss_cls: 0.2022  loss_mask: 0.0661  loss_dice: 0.6723
2025/06/23 17:17:29 - mmengine - INFO - Epoch(train)  [2][ 800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:14  time: 0.3242  data_time: 0.0019  memory: 2093  grad_norm: 166.5330  loss: 0.9896  loss_cls: 0.1636  loss_mask: 0.2021  loss_dice: 0.6240
2025/06/23 17:17:32 - mmengine - INFO - Epoch(train)  [2][ 810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:10  time: 0.3265  data_time: 0.0046  memory: 2300  grad_norm: 204.7395  loss: 1.1698  loss_cls: 0.2479  loss_mask: 0.1133  loss_dice: 0.8086
2025/06/23 17:17:35 - mmengine - INFO - Epoch(train)  [2][ 820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:07  time: 0.3260  data_time: 0.0033  memory: 2402  grad_norm: 107.4493  loss: 0.9775  loss_cls: 0.2551  loss_mask: 0.1023  loss_dice: 0.6200
2025/06/23 17:17:39 - mmengine - INFO - Epoch(train)  [2][ 830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:03:03  time: 0.3273  data_time: 0.0038  memory: 2215  grad_norm: 48.5302  loss: 1.1700  loss_cls: 0.2280  loss_mask: 0.1059  loss_dice: 0.8361
2025/06/23 17:17:42 - mmengine - INFO - Epoch(train)  [2][ 840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:59  time: 0.3286  data_time: 0.0027  memory: 2372  grad_norm: 61.2743  loss: 1.2868  loss_cls: 0.3225  loss_mask: 0.1444  loss_dice: 0.8199
2025/06/23 17:17:45 - mmengine - INFO - Epoch(train)  [2][ 850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:56  time: 0.3292  data_time: 0.0044  memory: 2271  grad_norm: 210.4425  loss: 1.2771  loss_cls: 0.3329  loss_mask: 0.1369  loss_dice: 0.8074
2025/06/23 17:17:48 - mmengine - INFO - Epoch(train)  [2][ 860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:52  time: 0.3283  data_time: 0.0006  memory: 2321  grad_norm: 86.4977  loss: 1.3080  loss_cls: 0.3055  loss_mask: 0.0991  loss_dice: 0.9034
2025/06/23 17:17:52 - mmengine - INFO - Epoch(train)  [2][ 870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:49  time: 0.3345  data_time: 0.0025  memory: 2402  grad_norm: 48.8331  loss: 1.1629  loss_cls: 0.3027  loss_mask: 0.0859  loss_dice: 0.7743
2025/06/23 17:17:55 - mmengine - INFO - Epoch(train)  [2][ 880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:46  time: 0.3336  data_time: 0.0053  memory: 2505  grad_norm: 88.4968  loss: 1.2877  loss_cls: 0.2794  loss_mask: 0.1080  loss_dice: 0.9003
2025/06/23 17:17:58 - mmengine - INFO - Epoch(train)  [2][ 890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:43  time: 0.3320  data_time: 0.0025  memory: 2483  grad_norm: 280.5944  loss: 1.3093  loss_cls: 0.3093  loss_mask: 0.1158  loss_dice: 0.8843
2025/06/23 17:18:02 - mmengine - INFO - Epoch(train)  [2][ 900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:39  time: 0.3269  data_time: 0.0022  memory: 2285  grad_norm: 65.3733  loss: 1.1947  loss_cls: 0.3073  loss_mask: 0.0802  loss_dice: 0.8072
2025/06/23 17:18:05 - mmengine - INFO - Epoch(train)  [2][ 910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:36  time: 0.3308  data_time: 0.0055  memory: 2380  grad_norm: 63.6928  loss: 1.2033  loss_cls: 0.2477  loss_mask: 0.1135  loss_dice: 0.8421
2025/06/23 17:18:08 - mmengine - INFO - Epoch(train)  [2][ 920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:32  time: 0.3255  data_time: 0.0052  memory: 2215  grad_norm: 153.4882  loss: 1.3874  loss_cls: 0.3859  loss_mask: 0.1680  loss_dice: 0.8335
2025/06/23 17:18:12 - mmengine - INFO - Epoch(train)  [2][ 930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:28  time: 0.3261  data_time: 0.0028  memory: 2329  grad_norm: 149.7076  loss: 1.2831  loss_cls: 0.2386  loss_mask: 0.2009  loss_dice: 0.8437
2025/06/23 17:18:15 - mmengine - INFO - Epoch(train)  [2][ 940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:24  time: 0.3262  data_time: 0.0021  memory: 2227  grad_norm: 122.1635  loss: 1.0618  loss_cls: 0.2054  loss_mask: 0.1255  loss_dice: 0.7309
2025/06/23 17:18:18 - mmengine - INFO - Epoch(train)  [2][ 950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:21  time: 0.3264  data_time: 0.0004  memory: 2154  grad_norm: 251.0878  loss: 1.1075  loss_cls: 0.3266  loss_mask: 0.1113  loss_dice: 0.6696
2025/06/23 17:18:21 - mmengine - INFO - Epoch(train)  [2][ 960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:17  time: 0.3239  data_time: 0.0055  memory: 2271  grad_norm: 160.3078  loss: 0.9175  loss_cls: 0.2198  loss_mask: 0.1009  loss_dice: 0.5968
2025/06/23 17:18:25 - mmengine - INFO - Epoch(train)  [2][ 970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:14  time: 0.3337  data_time: 0.0022  memory: 2527  grad_norm: 84.0812  loss: 1.6620  loss_cls: 0.3648  loss_mask: 0.1249  loss_dice: 1.1723
2025/06/23 17:18:28 - mmengine - INFO - Epoch(train)  [2][ 980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:10  time: 0.3277  data_time: 0.0005  memory: 2527  grad_norm: 153.7578  loss: 0.9956  loss_cls: 0.2722  loss_mask: 0.0718  loss_dice: 0.6516
2025/06/23 17:18:31 - mmengine - INFO - Epoch(train)  [2][ 990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:06  time: 0.3281  data_time: 0.0019  memory: 2343  grad_norm: 412.2206  loss: 1.3736  loss_cls: 0.3800  loss_mask: 0.0486  loss_dice: 0.9450
2025/06/23 17:18:34 - mmengine - INFO - Epoch(train)  [2][1000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:02:03  time: 0.3274  data_time: 0.0002  memory: 2148  grad_norm: 65.5391  loss: 1.0754  loss_cls: 0.1749  loss_mask: 0.1007  loss_dice: 0.7999
2025/06/23 17:18:38 - mmengine - INFO - Epoch(train)  [2][1010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:59  time: 0.3285  data_time: 0.0021  memory: 2432  grad_norm: 118.7808  loss: 1.2918  loss_cls: 0.3542  loss_mask: 0.1082  loss_dice: 0.8294
2025/06/23 17:18:41 - mmengine - INFO - Epoch(train)  [2][1020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:55  time: 0.3251  data_time: 0.0003  memory: 2271  grad_norm: 155.2823  loss: 1.2780  loss_cls: 0.1923  loss_mask: 0.1248  loss_dice: 0.9609
2025/06/23 17:18:44 - mmengine - INFO - Epoch(train)  [2][1030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:52  time: 0.3266  data_time: 0.0010  memory: 2285  grad_norm: 176.5887  loss: 1.1978  loss_cls: 0.2815  loss_mask: 0.1585  loss_dice: 0.7579
2025/06/23 17:18:48 - mmengine - INFO - Epoch(train)  [2][1040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:48  time: 0.3299  data_time: 0.0020  memory: 2285  grad_norm: 75.6115  loss: 1.1995  loss_cls: 0.2524  loss_mask: 0.0719  loss_dice: 0.8752
2025/06/23 17:18:51 - mmengine - INFO - Epoch(train)  [2][1050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:45  time: 0.3277  data_time: 0.0026  memory: 2409  grad_norm: 134.7776  loss: 1.5540  loss_cls: 0.2846  loss_mask: 0.4091  loss_dice: 0.8603
2025/06/23 17:18:54 - mmengine - INFO - Epoch(train)  [2][1060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:41  time: 0.3245  data_time: 0.0022  memory: 2251  grad_norm: 63.0767  loss: 0.9045  loss_cls: 0.2175  loss_mask: 0.0613  loss_dice: 0.6257
2025/06/23 17:18:57 - mmengine - INFO - Epoch(train)  [2][1070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:37  time: 0.3232  data_time: 0.0027  memory: 2278  grad_norm: 55.9291  loss: 0.7154  loss_cls: 0.1237  loss_mask: 0.0701  loss_dice: 0.5215
2025/06/23 17:19:01 - mmengine - INFO - Epoch(train)  [2][1080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:33  time: 0.3261  data_time: 0.0043  memory: 2160  grad_norm: 154.1355  loss: 1.6043  loss_cls: 0.2861  loss_mask: 0.1481  loss_dice: 1.1701
2025/06/23 17:19:04 - mmengine - INFO - Epoch(train)  [2][1090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:30  time: 0.3286  data_time: 0.0029  memory: 2271  grad_norm: 126.4222  loss: 1.1500  loss_cls: 0.2929  loss_mask: 0.1356  loss_dice: 0.7215
2025/06/23 17:19:07 - mmengine - INFO - Epoch(train)  [2][1100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:26  time: 0.3266  data_time: 0.0014  memory: 2372  grad_norm: 142.0769  loss: 1.5452  loss_cls: 0.2530  loss_mask: 0.3771  loss_dice: 0.9151
2025/06/23 17:19:10 - mmengine - INFO - Epoch(train)  [2][1110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:23  time: 0.3317  data_time: 0.0023  memory: 2380  grad_norm: 164.4217  loss: 1.4222  loss_cls: 0.3851  loss_mask: 0.1244  loss_dice: 0.9128
2025/06/23 17:19:14 - mmengine - INFO - Epoch(train)  [2][1120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:19  time: 0.3287  data_time: 0.0006  memory: 2372  grad_norm: 66.2458  loss: 1.1500  loss_cls: 0.2827  loss_mask: 0.0737  loss_dice: 0.7936
2025/06/23 17:19:17 - mmengine - INFO - Epoch(train)  [2][1130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:16  time: 0.3319  data_time: 0.0012  memory: 2468  grad_norm: 42.7312  loss: 1.3274  loss_cls: 0.3872  loss_mask: 0.0937  loss_dice: 0.8466
2025/06/23 17:19:20 - mmengine - INFO - Epoch(train)  [2][1140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:12  time: 0.3264  data_time: 0.0015  memory: 2293  grad_norm: 604.5637  loss: 1.1769  loss_cls: 0.3332  loss_mask: 0.0895  loss_dice: 0.7542
2025/06/23 17:19:24 - mmengine - INFO - Epoch(train)  [2][1150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:09  time: 0.3271  data_time: 0.0030  memory: 2227  grad_norm: 61.3286  loss: 1.2803  loss_cls: 0.2710  loss_mask: 0.1006  loss_dice: 0.9087
2025/06/23 17:19:27 - mmengine - INFO - Epoch(train)  [2][1160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:05  time: 0.3261  data_time: 0.0016  memory: 2343  grad_norm: 48.5622  loss: 1.1012  loss_cls: 0.3313  loss_mask: 0.0588  loss_dice: 0.7111
2025/06/23 17:19:30 - mmengine - INFO - Epoch(train)  [2][1170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:01:01  time: 0.3236  data_time: 0.0031  memory: 2239  grad_norm: 230.5182  loss: 0.8374  loss_cls: 0.2062  loss_mask: 0.0774  loss_dice: 0.5538
2025/06/23 17:19:33 - mmengine - INFO - Epoch(train)  [2][1180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:58  time: 0.3299  data_time: 0.0042  memory: 2519  grad_norm: 75.8222  loss: 1.6167  loss_cls: 0.4264  loss_mask: 0.1186  loss_dice: 1.0718
2025/06/23 17:19:37 - mmengine - INFO - Epoch(train)  [2][1190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:54  time: 0.3253  data_time: 0.0033  memory: 2069  grad_norm: 122.1916  loss: 0.9578  loss_cls: 0.2325  loss_mask: 0.0957  loss_dice: 0.6295
2025/06/23 17:19:40 - mmengine - INFO - Epoch(train)  [2][1200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:50  time: 0.3263  data_time: 0.0003  memory: 2197  grad_norm: 729.5017  loss: 0.9225  loss_cls: 0.2240  loss_mask: 0.1083  loss_dice: 0.5902
2025/06/23 17:19:43 - mmengine - INFO - Epoch(train)  [2][1210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:47  time: 0.3293  data_time: 0.0028  memory: 2300  grad_norm: 96.2477  loss: 1.8524  loss_cls: 0.3192  loss_mask: 0.4709  loss_dice: 1.0622
2025/06/23 17:19:46 - mmengine - INFO - Epoch(train)  [2][1220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:43  time: 0.3291  data_time: 0.0036  memory: 2505  grad_norm: 52.8521  loss: 1.2867  loss_cls: 0.2967  loss_mask: 0.1020  loss_dice: 0.8880
2025/06/23 17:19:50 - mmengine - INFO - Epoch(train)  [2][1230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:40  time: 0.3265  data_time: 0.0013  memory: 2106  grad_norm: 128.9731  loss: 1.5402  loss_cls: 0.3412  loss_mask: 0.1471  loss_dice: 1.0520
2025/06/23 17:19:50 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:19:53 - mmengine - INFO - Epoch(train)  [2][1240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:36  time: 0.3250  data_time: 0.0005  memory: 2432  grad_norm: 275.9663  loss: 0.9224  loss_cls: 0.2475  loss_mask: 0.0800  loss_dice: 0.5949
2025/06/23 17:19:56 - mmengine - INFO - Epoch(train)  [2][1250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:32  time: 0.3270  data_time: 0.0012  memory: 2314  grad_norm: 140.8115  loss: 1.3335  loss_cls: 0.3247  loss_mask: 0.0770  loss_dice: 0.9319
2025/06/23 17:19:59 - mmengine - INFO - Epoch(train)  [2][1260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:29  time: 0.3247  data_time: 0.0028  memory: 2075  grad_norm: 85.9424  loss: 1.1109  loss_cls: 0.2240  loss_mask: 0.1672  loss_dice: 0.7197
2025/06/23 17:20:03 - mmengine - INFO - Epoch(train)  [2][1270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:25  time: 0.3282  data_time: 0.0022  memory: 2432  grad_norm: 57.5410  loss: 1.1281  loss_cls: 0.2323  loss_mask: 0.0782  loss_dice: 0.8175
2025/06/23 17:20:06 - mmengine - INFO - Epoch(train)  [2][1280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:22  time: 0.3284  data_time: 0.0046  memory: 2351  grad_norm: 156.9064  loss: 1.0859  loss_cls: 0.2161  loss_mask: 0.0938  loss_dice: 0.7761
2025/06/23 17:20:09 - mmengine - INFO - Epoch(train)  [2][1290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:19  time: 0.3403  data_time: 0.0042  memory: 2372  grad_norm: 71.4365  loss: 1.5167  loss_cls: 0.3153  loss_mask: 0.1497  loss_dice: 1.0517
2025/06/23 17:20:13 - mmengine - INFO - Epoch(train)  [2][1300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:15  time: 0.3264  data_time: 0.0013  memory: 2191  grad_norm: 229.3272  loss: 1.1159  loss_cls: 0.2741  loss_mask: 0.1441  loss_dice: 0.6977
2025/06/23 17:20:16 - mmengine - INFO - Epoch(train)  [2][1310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:12  time: 0.3268  data_time: 0.0010  memory: 2257  grad_norm: 189.5146  loss: 1.5536  loss_cls: 0.4069  loss_mask: 0.1833  loss_dice: 0.9635
2025/06/23 17:20:19 - mmengine - INFO - Epoch(train)  [2][1320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:08  time: 0.3307  data_time: 0.0018  memory: 2239  grad_norm: 86.4059  loss: 1.4101  loss_cls: 0.3284  loss_mask: 0.1329  loss_dice: 0.9488
2025/06/23 17:20:23 - mmengine - INFO - Epoch(train)  [2][1330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:04  time: 0.3253  data_time: 0.0051  memory: 2191  grad_norm: 84.8362  loss: 1.0110  loss_cls: 0.2436  loss_mask: 0.1078  loss_dice: 0.6595
2025/06/23 17:20:26 - mmengine - INFO - Epoch(train)  [2][1340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:00:01  time: 0.3290  data_time: 0.0045  memory: 2300  grad_norm: 72.4993  loss: 1.2712  loss_cls: 0.2535  loss_mask: 0.1509  loss_dice: 0.8667
2025/06/23 17:20:29 - mmengine - INFO - Epoch(train)  [2][1350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:58  time: 0.3362  data_time: 0.0036  memory: 2439  grad_norm: 242.5053  loss: 1.5424  loss_cls: 0.4702  loss_mask: 0.1255  loss_dice: 0.9466
2025/06/23 17:20:32 - mmengine - INFO - Epoch(train)  [2][1360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:55  time: 0.3288  data_time: 0.0034  memory: 2372  grad_norm: 60.1566  loss: 1.2503  loss_cls: 0.3114  loss_mask: 0.1030  loss_dice: 0.8360
2025/06/23 17:20:36 - mmengine - INFO - Epoch(train)  [2][1370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:51  time: 0.3275  data_time: 0.0015  memory: 2197  grad_norm: 307.9053  loss: 1.7170  loss_cls: 0.5234  loss_mask: 0.1858  loss_dice: 1.0078
2025/06/23 17:20:39 - mmengine - INFO - Epoch(train)  [2][1380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:47  time: 0.3265  data_time: 0.0021  memory: 2278  grad_norm: 63.9676  loss: 1.0026  loss_cls: 0.2581  loss_mask: 0.1191  loss_dice: 0.6254
2025/06/23 17:20:42 - mmengine - INFO - Epoch(train)  [2][1390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:44  time: 0.3271  data_time: 0.0037  memory: 2372  grad_norm: 145.9606  loss: 1.4869  loss_cls: 0.3463  loss_mask: 0.1071  loss_dice: 1.0335
2025/06/23 17:20:46 - mmengine - INFO - Epoch(train)  [2][1400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:40  time: 0.3282  data_time: 0.0060  memory: 2154  grad_norm: 135.1643  loss: 1.1855  loss_cls: 0.3087  loss_mask: 0.1390  loss_dice: 0.7379
2025/06/23 17:20:49 - mmengine - INFO - Epoch(train)  [2][1410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:37  time: 0.3286  data_time: 0.0038  memory: 2483  grad_norm: 59.1302  loss: 1.0901  loss_cls: 0.2327  loss_mask: 0.1150  loss_dice: 0.7424
2025/06/23 17:20:52 - mmengine - INFO - Epoch(train)  [2][1420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:33  time: 0.3333  data_time: 0.0015  memory: 2387  grad_norm: 79.3607  loss: 1.2110  loss_cls: 0.3701  loss_mask: 0.0844  loss_dice: 0.7565
2025/06/23 17:20:56 - mmengine - INFO - Epoch(train)  [2][1430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:30  time: 0.3332  data_time: 0.0031  memory: 2372  grad_norm: 86.2529  loss: 1.0622  loss_cls: 0.3148  loss_mask: 0.0878  loss_dice: 0.6596
2025/06/23 17:20:59 - mmengine - INFO - Epoch(train)  [2][1440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:27  time: 0.3284  data_time: 0.0023  memory: 2203  grad_norm: 158.4777  loss: 1.3335  loss_cls: 0.3362  loss_mask: 0.1517  loss_dice: 0.8456
2025/06/23 17:21:02 - mmengine - INFO - Epoch(train)  [2][1450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:23  time: 0.3264  data_time: 0.0034  memory: 2454  grad_norm: 96.2239  loss: 1.3175  loss_cls: 0.2610  loss_mask: 0.1676  loss_dice: 0.8889
2025/06/23 17:21:05 - mmengine - INFO - Epoch(train)  [2][1460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:20  time: 0.3322  data_time: 0.0044  memory: 2278  grad_norm: 116.3990  loss: 1.4689  loss_cls: 0.4234  loss_mask: 0.0923  loss_dice: 0.9532
2025/06/23 17:21:09 - mmengine - INFO - Epoch(train)  [2][1470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:17  time: 0.3312  data_time: 0.0037  memory: 2490  grad_norm: 153.9782  loss: 1.7630  loss_cls: 0.4992  loss_mask: 0.2186  loss_dice: 1.0451
2025/06/23 17:21:12 - mmengine - INFO - Epoch(train)  [2][1480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:13  time: 0.3254  data_time: 0.0032  memory: 2166  grad_norm: 124.5163  loss: 0.9839  loss_cls: 0.2442  loss_mask: 0.0755  loss_dice: 0.6642
2025/06/23 17:21:15 - mmengine - INFO - Epoch(train)  [2][1490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:09  time: 0.3240  data_time: 0.0034  memory: 2124  grad_norm: 133.1199  loss: 1.0892  loss_cls: 0.2773  loss_mask: 0.1817  loss_dice: 0.6302
2025/06/23 17:21:18 - mmengine - INFO - Epoch(train)  [2][1500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:06  time: 0.3278  data_time: 0.0025  memory: 2278  grad_norm: 103.0552  loss: 0.9604  loss_cls: 0.2702  loss_mask: 0.0650  loss_dice: 0.6252
2025/06/23 17:21:22 - mmengine - INFO - Epoch(train)  [2][1510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:59:02  time: 0.3267  data_time: 0.0040  memory: 2257  grad_norm: 81.6758  loss: 0.9561  loss_cls: 0.2413  loss_mask: 0.0829  loss_dice: 0.6319
2025/06/23 17:21:25 - mmengine - INFO - Epoch(train)  [2][1520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:58  time: 0.3290  data_time: 0.0029  memory: 2468  grad_norm: 154.2857  loss: 1.2140  loss_cls: 0.2682  loss_mask: 0.1326  loss_dice: 0.8132
2025/06/23 17:21:28 - mmengine - INFO - Epoch(train)  [2][1530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:55  time: 0.3230  data_time: 0.0021  memory: 2112  grad_norm: 55.5180  loss: 0.6325  loss_cls: 0.1675  loss_mask: 0.0506  loss_dice: 0.4143
2025/06/23 17:21:32 - mmengine - INFO - Epoch(train)  [2][1540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:51  time: 0.3299  data_time: 0.0027  memory: 2468  grad_norm: 79.3399  loss: 1.3218  loss_cls: 0.3875  loss_mask: 0.1177  loss_dice: 0.8166
2025/06/23 17:21:35 - mmengine - INFO - Epoch(train)  [2][1550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:48  time: 0.3295  data_time: 0.0034  memory: 2571  grad_norm: 58.6214  loss: 1.1808  loss_cls: 0.2568  loss_mask: 0.0997  loss_dice: 0.8243
2025/06/23 17:21:38 - mmengine - INFO - Epoch(train)  [2][1560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:44  time: 0.3301  data_time: 0.0024  memory: 2351  grad_norm: 65.6152  loss: 1.2935  loss_cls: 0.2815  loss_mask: 0.1170  loss_dice: 0.8951
2025/06/23 17:21:41 - mmengine - INFO - Epoch(train)  [2][1570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:41  time: 0.3247  data_time: 0.0003  memory: 2221  grad_norm: 172.2903  loss: 1.0560  loss_cls: 0.2977  loss_mask: 0.0926  loss_dice: 0.6657
2025/06/23 17:21:45 - mmengine - INFO - Epoch(train)  [2][1580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:37  time: 0.3267  data_time: 0.0021  memory: 2293  grad_norm: 126.3281  loss: 1.3238  loss_cls: 0.4399  loss_mask: 0.1185  loss_dice: 0.7654
2025/06/23 17:21:48 - mmengine - INFO - Epoch(train)  [2][1590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:34  time: 0.3280  data_time: 0.0030  memory: 2365  grad_norm: 106.5715  loss: 1.2340  loss_cls: 0.3142  loss_mask: 0.0787  loss_dice: 0.8411
2025/06/23 17:21:51 - mmengine - INFO - Epoch(train)  [2][1600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:30  time: 0.3274  data_time: 0.0005  memory: 2351  grad_norm: 163.1190  loss: 1.8883  loss_cls: 0.2915  loss_mask: 0.3050  loss_dice: 1.2918
2025/06/23 17:21:55 - mmengine - INFO - Epoch(train)  [2][1610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:27  time: 0.3313  data_time: 0.0018  memory: 2534  grad_norm: 67.2851  loss: 1.5678  loss_cls: 0.3927  loss_mask: 0.1530  loss_dice: 1.0220
2025/06/23 17:21:58 - mmengine - INFO - Epoch(train)  [2][1620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:23  time: 0.3225  data_time: 0.0017  memory: 2099  grad_norm: 79.2953  loss: 0.8789  loss_cls: 0.2032  loss_mask: 0.0578  loss_dice: 0.6179
2025/06/23 17:22:01 - mmengine - INFO - Epoch(train)  [2][1630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:19  time: 0.3249  data_time: 0.0035  memory: 2075  grad_norm: 193.8778  loss: 1.6612  loss_cls: 0.2414  loss_mask: 0.2183  loss_dice: 1.2016
2025/06/23 17:22:04 - mmengine - INFO - Epoch(train)  [2][1640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:16  time: 0.3263  data_time: 0.0029  memory: 2343  grad_norm: 105.4208  loss: 0.8912  loss_cls: 0.2237  loss_mask: 0.0736  loss_dice: 0.5939
2025/06/23 17:22:08 - mmengine - INFO - Epoch(train)  [2][1650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:12  time: 0.3298  data_time: 0.0011  memory: 2409  grad_norm: 72.4826  loss: 1.2892  loss_cls: 0.3268  loss_mask: 0.1162  loss_dice: 0.8461
2025/06/23 17:22:11 - mmengine - INFO - Epoch(train)  [2][1660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:09  time: 0.3285  data_time: 0.0053  memory: 2387  grad_norm: 425.6659  loss: 1.2626  loss_cls: 0.3399  loss_mask: 0.0994  loss_dice: 0.8233
2025/06/23 17:22:14 - mmengine - INFO - Epoch(train)  [2][1670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:05  time: 0.3279  data_time: 0.0035  memory: 2285  grad_norm: 162.4678  loss: 1.3516  loss_cls: 0.3817  loss_mask: 0.1463  loss_dice: 0.8236
2025/06/23 17:22:17 - mmengine - INFO - Epoch(train)  [2][1680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:58:02  time: 0.3303  data_time: 0.0010  memory: 2314  grad_norm: 245.9172  loss: 1.7304  loss_cls: 0.4932  loss_mask: 0.1390  loss_dice: 1.0982
2025/06/23 17:22:21 - mmengine - INFO - Epoch(train)  [2][1690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:58  time: 0.3284  data_time: 0.0032  memory: 2351  grad_norm: 101.3305  loss: 1.7088  loss_cls: 0.4256  loss_mask: 0.1642  loss_dice: 1.1191
2025/06/23 17:22:24 - mmengine - INFO - Epoch(train)  [2][1700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:55  time: 0.3285  data_time: 0.0029  memory: 2300  grad_norm: 298.5946  loss: 1.5094  loss_cls: 0.4478  loss_mask: 0.1197  loss_dice: 0.9419
2025/06/23 17:22:27 - mmengine - INFO - Epoch(train)  [2][1710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:51  time: 0.3257  data_time: 0.0049  memory: 2527  grad_norm: 72.2130  loss: 1.5047  loss_cls: 0.2882  loss_mask: 0.1700  loss_dice: 1.0464
2025/06/23 17:22:31 - mmengine - INFO - Epoch(train)  [2][1720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:48  time: 0.3293  data_time: 0.0029  memory: 2351  grad_norm: 125.7944  loss: 1.5357  loss_cls: 0.3464  loss_mask: 0.1080  loss_dice: 1.0814
2025/06/23 17:22:34 - mmengine - INFO - Epoch(train)  [2][1730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:44  time: 0.3281  data_time: 0.0045  memory: 2239  grad_norm: 86.8576  loss: 1.2055  loss_cls: 0.2992  loss_mask: 0.1446  loss_dice: 0.7617
2025/06/23 17:22:37 - mmengine - INFO - Epoch(train)  [2][1740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:41  time: 0.3304  data_time: 0.0017  memory: 2314  grad_norm: 137.5654  loss: 1.0726  loss_cls: 0.2310  loss_mask: 0.1044  loss_dice: 0.7372
2025/06/23 17:22:40 - mmengine - INFO - Epoch(train)  [2][1750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:37  time: 0.3262  data_time: 0.0023  memory: 2251  grad_norm: 231.6180  loss: 1.3214  loss_cls: 0.4092  loss_mask: 0.1255  loss_dice: 0.7867
2025/06/23 17:22:44 - mmengine - INFO - Epoch(train)  [2][1760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:34  time: 0.3271  data_time: 0.0023  memory: 2336  grad_norm: 136.1383  loss: 1.4382  loss_cls: 0.3428  loss_mask: 0.1351  loss_dice: 0.9602
2025/06/23 17:22:47 - mmengine - INFO - Epoch(train)  [2][1770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:30  time: 0.3279  data_time: 0.0010  memory: 2409  grad_norm: 118.2644  loss: 1.3854  loss_cls: 0.3194  loss_mask: 0.1628  loss_dice: 0.9032
2025/06/23 17:22:50 - mmengine - INFO - Epoch(train)  [2][1780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:27  time: 0.3285  data_time: 0.0016  memory: 2358  grad_norm: 59.8602  loss: 1.2763  loss_cls: 0.3371  loss_mask: 0.1221  loss_dice: 0.8171
2025/06/23 17:22:54 - mmengine - INFO - Epoch(train)  [2][1790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:23  time: 0.3253  data_time: 0.0015  memory: 2215  grad_norm: 91.0531  loss: 1.3240  loss_cls: 0.3028  loss_mask: 0.1299  loss_dice: 0.8913
2025/06/23 17:22:57 - mmengine - INFO - Epoch(train)  [2][1800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:19  time: 0.3234  data_time: 0.0045  memory: 2106  grad_norm: 137.3798  loss: 1.0558  loss_cls: 0.1641  loss_mask: 0.1905  loss_dice: 0.7012
2025/06/23 17:23:00 - mmengine - INFO - Epoch(train)  [2][1810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:16  time: 0.3248  data_time: 0.0025  memory: 2075  grad_norm: 407.7344  loss: 1.2803  loss_cls: 0.3859  loss_mask: 0.1146  loss_dice: 0.7798
2025/06/23 17:23:03 - mmengine - INFO - Epoch(train)  [2][1820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:12  time: 0.3276  data_time: 0.0032  memory: 2409  grad_norm: 81.3263  loss: 1.1956  loss_cls: 0.2841  loss_mask: 0.1137  loss_dice: 0.7978
2025/06/23 17:23:07 - mmengine - INFO - Epoch(train)  [2][1830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:09  time: 0.3279  data_time: 0.0025  memory: 2358  grad_norm: 36.7564  loss: 1.3236  loss_cls: 0.2857  loss_mask: 0.1304  loss_dice: 0.9074
2025/06/23 17:23:10 - mmengine - INFO - Epoch(train)  [2][1840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:05  time: 0.3287  data_time: 0.0031  memory: 2245  grad_norm: 73.2826  loss: 1.0418  loss_cls: 0.3171  loss_mask: 0.0882  loss_dice: 0.6365
2025/06/23 17:23:13 - mmengine - INFO - Epoch(train)  [2][1850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:57:02  time: 0.3288  data_time: 0.0014  memory: 2233  grad_norm: 93.5284  loss: 1.3865  loss_cls: 0.3884  loss_mask: 0.0917  loss_dice: 0.9063
2025/06/23 17:23:16 - mmengine - INFO - Epoch(train)  [2][1860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:58  time: 0.3319  data_time: 0.0036  memory: 2239  grad_norm: 114.3816  loss: 1.1875  loss_cls: 0.2610  loss_mask: 0.1111  loss_dice: 0.8154
2025/06/23 17:23:20 - mmengine - INFO - Epoch(train)  [2][1870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:55  time: 0.3279  data_time: 0.0041  memory: 2093  grad_norm: 57.2270  loss: 1.2112  loss_cls: 0.2912  loss_mask: 0.1013  loss_dice: 0.8186
2025/06/23 17:23:23 - mmengine - INFO - Epoch(train)  [2][1880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:52  time: 0.3301  data_time: 0.0017  memory: 2209  grad_norm: 37.8631  loss: 1.2718  loss_cls: 0.2757  loss_mask: 0.1394  loss_dice: 0.8566
2025/06/23 17:23:26 - mmengine - INFO - Epoch(train)  [2][1890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:48  time: 0.3295  data_time: 0.0014  memory: 2432  grad_norm: 59.7608  loss: 1.2147  loss_cls: 0.3272  loss_mask: 0.1263  loss_dice: 0.7613
2025/06/23 17:23:30 - mmengine - INFO - Epoch(train)  [2][1900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:45  time: 0.3341  data_time: 0.0033  memory: 2519  grad_norm: 95.9482  loss: 1.7365  loss_cls: 0.3753  loss_mask: 0.1381  loss_dice: 1.2231
2025/06/23 17:23:33 - mmengine - INFO - Epoch(train)  [2][1910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:42  time: 0.3315  data_time: 0.0026  memory: 2227  grad_norm: 268.7293  loss: 1.4537  loss_cls: 0.2668  loss_mask: 0.3935  loss_dice: 0.7934
2025/06/23 17:23:36 - mmengine - INFO - Epoch(train)  [2][1920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:38  time: 0.3322  data_time: 0.0024  memory: 2351  grad_norm: 104.7047  loss: 2.0160  loss_cls: 0.6650  loss_mask: 0.1417  loss_dice: 1.2093
2025/06/23 17:23:40 - mmengine - INFO - Epoch(train)  [2][1930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:35  time: 0.3308  data_time: 0.0029  memory: 2191  grad_norm: 73.6847  loss: 1.2712  loss_cls: 0.3122  loss_mask: 0.0785  loss_dice: 0.8806
2025/06/23 17:23:43 - mmengine - INFO - Epoch(train)  [2][1940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:32  time: 0.3334  data_time: 0.0065  memory: 2358  grad_norm: 176.2739  loss: 2.2334  loss_cls: 0.5198  loss_mask: 0.5526  loss_dice: 1.1610
2025/06/23 17:23:46 - mmengine - INFO - Epoch(train)  [2][1950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:29  time: 0.3326  data_time: 0.0046  memory: 2285  grad_norm: 94.9868  loss: 1.6739  loss_cls: 0.3882  loss_mask: 0.1064  loss_dice: 1.1793
2025/06/23 17:23:50 - mmengine - INFO - Epoch(train)  [2][1960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:25  time: 0.3294  data_time: 0.0025  memory: 2293  grad_norm: 170.4193  loss: 2.2490  loss_cls: 0.3525  loss_mask: 0.4164  loss_dice: 1.4800
2025/06/23 17:23:53 - mmengine - INFO - Epoch(train)  [2][1970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:21  time: 0.3234  data_time: 0.0026  memory: 2087  grad_norm: 129.6376  loss: 1.9030  loss_cls: 0.3936  loss_mask: 0.1837  loss_dice: 1.3257
2025/06/23 17:23:56 - mmengine - INFO - Epoch(train)  [2][1980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:18  time: 0.3256  data_time: 0.0036  memory: 2365  grad_norm: 68.2156  loss: 1.5272  loss_cls: 0.2922  loss_mask: 0.1741  loss_dice: 1.0608
2025/06/23 17:23:59 - mmengine - INFO - Epoch(train)  [2][1990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:14  time: 0.3265  data_time: 0.0060  memory: 2191  grad_norm: 77.3918  loss: 1.7860  loss_cls: 0.3727  loss_mask: 0.2060  loss_dice: 1.2073
2025/06/23 17:24:03 - mmengine - INFO - Epoch(train)  [2][2000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:11  time: 0.3232  data_time: 0.0011  memory: 2184  grad_norm: 57.3803  loss: 1.2210  loss_cls: 0.2642  loss_mask: 0.1004  loss_dice: 0.8564
2025/06/23 17:24:06 - mmengine - INFO - Epoch(train)  [2][2010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:07  time: 0.3292  data_time: 0.0030  memory: 2314  grad_norm: 202.9131  loss: 2.3768  loss_cls: 0.5676  loss_mask: 0.2324  loss_dice: 1.5768
2025/06/23 17:24:09 - mmengine - INFO - Epoch(train)  [2][2020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:03  time: 0.3251  data_time: 0.0046  memory: 2081  grad_norm: 128.5107  loss: 1.3579  loss_cls: 0.2920  loss_mask: 0.1835  loss_dice: 0.8825
2025/06/23 17:24:12 - mmengine - INFO - Epoch(train)  [2][2030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:56:00  time: 0.3287  data_time: 0.0006  memory: 2278  grad_norm: 93.5162  loss: 1.5584  loss_cls: 0.3606  loss_mask: 0.1178  loss_dice: 1.0799
2025/06/23 17:24:16 - mmengine - INFO - Epoch(train)  [2][2040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:56  time: 0.3242  data_time: 0.0028  memory: 2136  grad_norm: 104.1442  loss: 1.4521  loss_cls: 0.2393  loss_mask: 0.3297  loss_dice: 0.8831
2025/06/23 17:24:19 - mmengine - INFO - Epoch(train)  [2][2050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:53  time: 0.3259  data_time: 0.0027  memory: 2308  grad_norm: 155.2700  loss: 1.5646  loss_cls: 0.3935  loss_mask: 0.0806  loss_dice: 1.0906
2025/06/23 17:24:22 - mmengine - INFO - Epoch(train)  [2][2060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:49  time: 0.3290  data_time: 0.0003  memory: 2498  grad_norm: 105.6049  loss: 1.6359  loss_cls: 0.4111  loss_mask: 0.1413  loss_dice: 1.0834
2025/06/23 17:24:25 - mmengine - INFO - Epoch(train)  [2][2070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:46  time: 0.3226  data_time: 0.0015  memory: 2308  grad_norm: 118.9584  loss: 1.0060  loss_cls: 0.3139  loss_mask: 0.0978  loss_dice: 0.5943
2025/06/23 17:24:29 - mmengine - INFO - Epoch(train)  [2][2080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:42  time: 0.3270  data_time: 0.0035  memory: 2439  grad_norm: 56.4581  loss: 1.3506  loss_cls: 0.2676  loss_mask: 0.2034  loss_dice: 0.8795
2025/06/23 17:24:32 - mmengine - INFO - Epoch(train)  [2][2090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:39  time: 0.3320  data_time: 0.0010  memory: 2365  grad_norm: 147.8408  loss: 1.4645  loss_cls: 0.4132  loss_mask: 0.1166  loss_dice: 0.9347
2025/06/23 17:24:35 - mmengine - INFO - Epoch(train)  [2][2100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:35  time: 0.3253  data_time: 0.0026  memory: 2184  grad_norm: 82.9523  loss: 1.3631  loss_cls: 0.2809  loss_mask: 0.0937  loss_dice: 0.9885
2025/06/23 17:24:39 - mmengine - INFO - Epoch(train)  [2][2110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:31  time: 0.3251  data_time: 0.0037  memory: 2106  grad_norm: 67.8247  loss: 0.9537  loss_cls: 0.1873  loss_mask: 0.0987  loss_dice: 0.6677
2025/06/23 17:24:42 - mmengine - INFO - Epoch(train)  [2][2120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:28  time: 0.3286  data_time: 0.0031  memory: 2178  grad_norm: 121.9797  loss: 1.0871  loss_cls: 0.2612  loss_mask: 0.1155  loss_dice: 0.7103
2025/06/23 17:24:45 - mmengine - INFO - Epoch(train)  [2][2130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:25  time: 0.3288  data_time: 0.0048  memory: 2417  grad_norm: 62.2700  loss: 1.2435  loss_cls: 0.3703  loss_mask: 0.1350  loss_dice: 0.7382
2025/06/23 17:24:48 - mmengine - INFO - Epoch(train)  [2][2140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:21  time: 0.3269  data_time: 0.0009  memory: 2380  grad_norm: 59.0635  loss: 1.4213  loss_cls: 0.3916  loss_mask: 0.1249  loss_dice: 0.9048
2025/06/23 17:24:52 - mmengine - INFO - Epoch(train)  [2][2150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:17  time: 0.3243  data_time: 0.0067  memory: 2142  grad_norm: 54.8317  loss: 1.0102  loss_cls: 0.2786  loss_mask: 0.0516  loss_dice: 0.6800
2025/06/23 17:24:55 - mmengine - INFO - Epoch(train)  [2][2160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:14  time: 0.3286  data_time: 0.0017  memory: 2142  grad_norm: 81.5233  loss: 1.5025  loss_cls: 0.4138  loss_mask: 0.1053  loss_dice: 0.9834
2025/06/23 17:24:58 - mmengine - INFO - Epoch(train)  [2][2170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:10  time: 0.3243  data_time: 0.0012  memory: 2239  grad_norm: 172.4734  loss: 1.2086  loss_cls: 0.2991  loss_mask: 0.1812  loss_dice: 0.7282
2025/06/23 17:25:01 - mmengine - INFO - Epoch(train)  [2][2180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:07  time: 0.3254  data_time: 0.0010  memory: 2343  grad_norm: 349.4605  loss: 2.1949  loss_cls: 0.3046  loss_mask: 0.8501  loss_dice: 1.0402
2025/06/23 17:25:05 - mmengine - INFO - Epoch(train)  [2][2190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:03  time: 0.3297  data_time: 0.0034  memory: 2308  grad_norm: 82.6779  loss: 1.5506  loss_cls: 0.4191  loss_mask: 0.1203  loss_dice: 1.0112
2025/06/23 17:25:08 - mmengine - INFO - Epoch(train)  [2][2200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:55:00  time: 0.3316  data_time: 0.0020  memory: 2490  grad_norm: 160.9330  loss: 1.7876  loss_cls: 0.4552  loss_mask: 0.1351  loss_dice: 1.1972
2025/06/23 17:25:11 - mmengine - INFO - Epoch(train)  [2][2210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:56  time: 0.3256  data_time: 0.0018  memory: 2245  grad_norm: 69.9686  loss: 1.3942  loss_cls: 0.4015  loss_mask: 0.0948  loss_dice: 0.8979
2025/06/23 17:25:15 - mmengine - INFO - Epoch(train)  [2][2220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:53  time: 0.3291  data_time: 0.0032  memory: 2278  grad_norm: 77.9085  loss: 1.2497  loss_cls: 0.3438  loss_mask: 0.1131  loss_dice: 0.7928
2025/06/23 17:25:18 - mmengine - INFO - Epoch(train)  [2][2230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:49  time: 0.3250  data_time: 0.0020  memory: 2314  grad_norm: 52.0992  loss: 1.5096  loss_cls: 0.3240  loss_mask: 0.1435  loss_dice: 1.0421
2025/06/23 17:25:18 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:25:21 - mmengine - INFO - Epoch(train)  [2][2240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:46  time: 0.3314  data_time: 0.0058  memory: 2351  grad_norm: 96.1524  loss: 1.7830  loss_cls: 0.4341  loss_mask: 0.2247  loss_dice: 1.1242
2025/06/23 17:25:24 - mmengine - INFO - Epoch(train)  [2][2250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:42  time: 0.3250  data_time: 0.0007  memory: 2166  grad_norm: 246.6455  loss: 1.6317  loss_cls: 0.4115  loss_mask: 0.1936  loss_dice: 1.0266
2025/06/23 17:25:28 - mmengine - INFO - Epoch(train)  [2][2260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:39  time: 0.3275  data_time: 0.0053  memory: 2372  grad_norm: 55.8492  loss: 1.6022  loss_cls: 0.4501  loss_mask: 0.1178  loss_dice: 1.0343
2025/06/23 17:25:31 - mmengine - INFO - Epoch(train)  [2][2270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:35  time: 0.3257  data_time: 0.0037  memory: 2351  grad_norm: 36.6881  loss: 0.9801  loss_cls: 0.2321  loss_mask: 0.1239  loss_dice: 0.6241
2025/06/23 17:25:34 - mmengine - INFO - Epoch(train)  [2][2280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:32  time: 0.3297  data_time: 0.0031  memory: 2505  grad_norm: 52.1960  loss: 1.5107  loss_cls: 0.4372  loss_mask: 0.0980  loss_dice: 0.9755
2025/06/23 17:25:37 - mmengine - INFO - Epoch(train)  [2][2290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:29  time: 0.3291  data_time: 0.0017  memory: 2417  grad_norm: 34.5364  loss: 1.4683  loss_cls: 0.3606  loss_mask: 0.1323  loss_dice: 0.9754
2025/06/23 17:25:41 - mmengine - INFO - Epoch(train)  [2][2300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:25  time: 0.3250  data_time: 0.0010  memory: 2191  grad_norm: 89.9143  loss: 1.1819  loss_cls: 0.3460  loss_mask: 0.1396  loss_dice: 0.6963
2025/06/23 17:25:44 - mmengine - INFO - Epoch(train)  [2][2310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:21  time: 0.3262  data_time: 0.0014  memory: 2372  grad_norm: 71.0424  loss: 1.5079  loss_cls: 0.4252  loss_mask: 0.0875  loss_dice: 0.9952
2025/06/23 17:25:47 - mmengine - INFO - Epoch(train)  [2][2320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:18  time: 0.3281  data_time: 0.0030  memory: 2387  grad_norm: 246.7472  loss: 1.3886  loss_cls: 0.3735  loss_mask: 0.1283  loss_dice: 0.8868
2025/06/23 17:25:51 - mmengine - INFO - Epoch(train)  [2][2330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:14  time: 0.3260  data_time: 0.0019  memory: 2257  grad_norm: 99.9171  loss: 1.6219  loss_cls: 0.4614  loss_mask: 0.1975  loss_dice: 0.9630
2025/06/23 17:25:54 - mmengine - INFO - Epoch(train)  [2][2340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:11  time: 0.3281  data_time: 0.0019  memory: 2505  grad_norm: 34.4254  loss: 1.1694  loss_cls: 0.3313  loss_mask: 0.0698  loss_dice: 0.7683
2025/06/23 17:25:57 - mmengine - INFO - Epoch(train)  [2][2350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:07  time: 0.3272  data_time: 0.0029  memory: 2380  grad_norm: 59.4885  loss: 1.1639  loss_cls: 0.2680  loss_mask: 0.0999  loss_dice: 0.7960
2025/06/23 17:26:00 - mmengine - INFO - Epoch(train)  [2][2360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:04  time: 0.3253  data_time: 0.0015  memory: 2278  grad_norm: 67.7701  loss: 1.2355  loss_cls: 0.2730  loss_mask: 0.1550  loss_dice: 0.8076
2025/06/23 17:26:04 - mmengine - INFO - Epoch(train)  [2][2370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:54:01  time: 0.3323  data_time: 0.0051  memory: 2446  grad_norm: 64.8331  loss: 1.7524  loss_cls: 0.3906  loss_mask: 0.1494  loss_dice: 1.2124
2025/06/23 17:26:07 - mmengine - INFO - Epoch(train)  [2][2380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:57  time: 0.3298  data_time: 0.0026  memory: 2372  grad_norm: 74.7830  loss: 1.5910  loss_cls: 0.4463  loss_mask: 0.1336  loss_dice: 1.0111
2025/06/23 17:26:10 - mmengine - INFO - Epoch(train)  [2][2390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:54  time: 0.3267  data_time: 0.0041  memory: 2184  grad_norm: 36.9050  loss: 1.1960  loss_cls: 0.2819  loss_mask: 0.0664  loss_dice: 0.8477
2025/06/23 17:26:14 - mmengine - INFO - Epoch(train)  [2][2400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:50  time: 0.3316  data_time: 0.0018  memory: 2365  grad_norm: 168.0353  loss: 1.9100  loss_cls: 0.4498  loss_mask: 0.1465  loss_dice: 1.3136
2025/06/23 17:26:17 - mmengine - INFO - Epoch(train)  [2][2410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:47  time: 0.3280  data_time: 0.0048  memory: 2271  grad_norm: 64.0066  loss: 1.1576  loss_cls: 0.3446  loss_mask: 0.1206  loss_dice: 0.6924
2025/06/23 17:26:20 - mmengine - INFO - Epoch(train)  [2][2420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:43  time: 0.3247  data_time: 0.0005  memory: 2118  grad_norm: 111.3592  loss: 1.6893  loss_cls: 0.3869  loss_mask: 0.1634  loss_dice: 1.1390
2025/06/23 17:26:23 - mmengine - INFO - Epoch(train)  [2][2430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:40  time: 0.3362  data_time: 0.0020  memory: 2417  grad_norm: 50.7091  loss: 1.9736  loss_cls: 0.4513  loss_mask: 0.1557  loss_dice: 1.3666
2025/06/23 17:26:27 - mmengine - INFO - Epoch(train)  [2][2440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:37  time: 0.3255  data_time: 0.0020  memory: 2160  grad_norm: 45.7165  loss: 0.9911  loss_cls: 0.2323  loss_mask: 0.1130  loss_dice: 0.6459
2025/06/23 17:26:30 - mmengine - INFO - Epoch(train)  [2][2450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:33  time: 0.3299  data_time: 0.0046  memory: 2343  grad_norm: 62.2307  loss: 1.5170  loss_cls: 0.3839  loss_mask: 0.1006  loss_dice: 1.0325
2025/06/23 17:26:33 - mmengine - INFO - Epoch(train)  [2][2460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:30  time: 0.3244  data_time: 0.0019  memory: 2343  grad_norm: 41.6486  loss: 1.9441  loss_cls: 0.3558  loss_mask: 0.3719  loss_dice: 1.2163
2025/06/23 17:26:37 - mmengine - INFO - Epoch(train)  [2][2470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:26  time: 0.3264  data_time: 0.0047  memory: 2519  grad_norm: 81.9596  loss: 1.5779  loss_cls: 0.3751  loss_mask: 0.1910  loss_dice: 1.0118
2025/06/23 17:26:40 - mmengine - INFO - Epoch(train)  [2][2480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:23  time: 0.3300  data_time: 0.0016  memory: 2425  grad_norm: 75.3280  loss: 1.6374  loss_cls: 0.4279  loss_mask: 0.1134  loss_dice: 1.0961
2025/06/23 17:26:43 - mmengine - INFO - Epoch(train)  [2][2490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:19  time: 0.3270  data_time: 0.0042  memory: 2343  grad_norm: 74.6754  loss: 1.2350  loss_cls: 0.3409  loss_mask: 0.0787  loss_dice: 0.8154
2025/06/23 17:26:46 - mmengine - INFO - Epoch(train)  [2][2500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:16  time: 0.3257  data_time: 0.0031  memory: 2446  grad_norm: 34.4334  loss: 0.8045  loss_cls: 0.2390  loss_mask: 0.0607  loss_dice: 0.5048
2025/06/23 17:26:50 - mmengine - INFO - Epoch(train)  [2][2510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:12  time: 0.3260  data_time: 0.0009  memory: 2483  grad_norm: 138.9745  loss: 1.7983  loss_cls: 0.4537  loss_mask: 0.1758  loss_dice: 1.1688
2025/06/23 17:26:53 - mmengine - INFO - Epoch(train)  [2][2520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:09  time: 0.3299  data_time: 0.0035  memory: 2505  grad_norm: 155.3562  loss: 1.4224  loss_cls: 0.4449  loss_mask: 0.1256  loss_dice: 0.8519
2025/06/23 17:26:56 - mmengine - INFO - Epoch(train)  [2][2530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:05  time: 0.3267  data_time: 0.0034  memory: 2409  grad_norm: 121.7805  loss: 1.4319  loss_cls: 0.3509  loss_mask: 0.2513  loss_dice: 0.8298
2025/06/23 17:26:59 - mmengine - INFO - Epoch(train)  [2][2540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:53:02  time: 0.3266  data_time: 0.0004  memory: 2409  grad_norm: 193.4714  loss: 1.3920  loss_cls: 0.3271  loss_mask: 0.1843  loss_dice: 0.8806
2025/06/23 17:27:03 - mmengine - INFO - Epoch(train)  [2][2550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:58  time: 0.3238  data_time: 0.0016  memory: 2490  grad_norm: 94.7135  loss: 1.1146  loss_cls: 0.3291  loss_mask: 0.0819  loss_dice: 0.7036
2025/06/23 17:27:06 - mmengine - INFO - Epoch(train)  [2][2560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:55  time: 0.3266  data_time: 0.0032  memory: 2106  grad_norm: 114.6816  loss: 1.3122  loss_cls: 0.2743  loss_mask: 0.1361  loss_dice: 0.9017
2025/06/23 17:27:09 - mmengine - INFO - Epoch(train)  [2][2570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:51  time: 0.3267  data_time: 0.0023  memory: 2321  grad_norm: 42.9111  loss: 1.0713  loss_cls: 0.3229  loss_mask: 0.0633  loss_dice: 0.6852
2025/06/23 17:27:13 - mmengine - INFO - Epoch(train)  [2][2580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:48  time: 0.3300  data_time: 0.0028  memory: 2563  grad_norm: 106.8775  loss: 1.2945  loss_cls: 0.3132  loss_mask: 0.1154  loss_dice: 0.8659
2025/06/23 17:27:16 - mmengine - INFO - Epoch(train)  [2][2590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:44  time: 0.3229  data_time: 0.0027  memory: 2075  grad_norm: 73.6026  loss: 0.9720  loss_cls: 0.2066  loss_mask: 0.0998  loss_dice: 0.6656
2025/06/23 17:27:19 - mmengine - INFO - Epoch(train)  [2][2600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:41  time: 0.3304  data_time: 0.0031  memory: 2351  grad_norm: 254.2541  loss: 1.5669  loss_cls: 0.4917  loss_mask: 0.1160  loss_dice: 0.9592
2025/06/23 17:27:22 - mmengine - INFO - Epoch(train)  [2][2610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:37  time: 0.3281  data_time: 0.0040  memory: 2365  grad_norm: 83.0342  loss: 1.1626  loss_cls: 0.3400  loss_mask: 0.1280  loss_dice: 0.6946
2025/06/23 17:27:26 - mmengine - INFO - Epoch(train)  [2][2620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:34  time: 0.3266  data_time: 0.0027  memory: 2387  grad_norm: 69.9398  loss: 1.1041  loss_cls: 0.2557  loss_mask: 0.0857  loss_dice: 0.7627
2025/06/23 17:27:29 - mmengine - INFO - Epoch(train)  [2][2630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:31  time: 0.3316  data_time: 0.0044  memory: 2271  grad_norm: 66.9224  loss: 1.0922  loss_cls: 0.2387  loss_mask: 0.0811  loss_dice: 0.7724
2025/06/23 17:27:32 - mmengine - INFO - Epoch(train)  [2][2640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:27  time: 0.3230  data_time: 0.0009  memory: 2178  grad_norm: 199.7013  loss: 1.4925  loss_cls: 0.2234  loss_mask: 0.2995  loss_dice: 0.9696
2025/06/23 17:27:35 - mmengine - INFO - Epoch(train)  [2][2650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:23  time: 0.3288  data_time: 0.0026  memory: 2271  grad_norm: 114.3324  loss: 1.8913  loss_cls: 0.4603  loss_mask: 0.1475  loss_dice: 1.2835
2025/06/23 17:27:39 - mmengine - INFO - Epoch(train)  [2][2660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:20  time: 0.3315  data_time: 0.0025  memory: 2519  grad_norm: 46.8349  loss: 1.1020  loss_cls: 0.2655  loss_mask: 0.0787  loss_dice: 0.7577
2025/06/23 17:27:42 - mmengine - INFO - Epoch(train)  [2][2670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:17  time: 0.3263  data_time: 0.0026  memory: 2233  grad_norm: 52.0667  loss: 1.2098  loss_cls: 0.3425  loss_mask: 0.0774  loss_dice: 0.7900
2025/06/23 17:27:45 - mmengine - INFO - Epoch(train)  [2][2680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:13  time: 0.3290  data_time: 0.0022  memory: 2409  grad_norm: 55.1535  loss: 1.2179  loss_cls: 0.3023  loss_mask: 0.0914  loss_dice: 0.8242
2025/06/23 17:27:49 - mmengine - INFO - Epoch(train)  [2][2690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:10  time: 0.3386  data_time: 0.0033  memory: 2358  grad_norm: 84.2103  loss: 1.3526  loss_cls: 0.3824  loss_mask: 0.0912  loss_dice: 0.8791
2025/06/23 17:27:52 - mmengine - INFO - Epoch(train)  [2][2700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:07  time: 0.3341  data_time: 0.0024  memory: 2285  grad_norm: 286.5298  loss: 1.1148  loss_cls: 0.3313  loss_mask: 0.1062  loss_dice: 0.6773
2025/06/23 17:27:55 - mmengine - INFO - Epoch(train)  [2][2710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:04  time: 0.3311  data_time: 0.0019  memory: 2505  grad_norm: 90.0292  loss: 1.2990  loss_cls: 0.3277  loss_mask: 0.0731  loss_dice: 0.8983
2025/06/23 17:27:59 - mmengine - INFO - Epoch(train)  [2][2720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:52:00  time: 0.3255  data_time: 0.0041  memory: 2148  grad_norm: 53.2580  loss: 0.9289  loss_cls: 0.1791  loss_mask: 0.1112  loss_dice: 0.6386
2025/06/23 17:28:02 - mmengine - INFO - Epoch(train)  [2][2730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:57  time: 0.3329  data_time: 0.0023  memory: 2358  grad_norm: 93.7953  loss: 1.5501  loss_cls: 0.3907  loss_mask: 0.1682  loss_dice: 0.9912
2025/06/23 17:28:05 - mmengine - INFO - Epoch(train)  [2][2740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:54  time: 0.3372  data_time: 0.0022  memory: 2321  grad_norm: 81.5973  loss: 1.4249  loss_cls: 0.4309  loss_mask: 0.1466  loss_dice: 0.8475
2025/06/23 17:28:09 - mmengine - INFO - Epoch(train)  [2][2750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:51  time: 0.3341  data_time: 0.0036  memory: 2402  grad_norm: 89.4002  loss: 1.4986  loss_cls: 0.4360  loss_mask: 0.1715  loss_dice: 0.8912
2025/06/23 17:28:12 - mmengine - INFO - Epoch(train)  [2][2760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:48  time: 0.3292  data_time: 0.0022  memory: 2154  grad_norm: 47.7461  loss: 1.1958  loss_cls: 0.3836  loss_mask: 0.1011  loss_dice: 0.7111
2025/06/23 17:28:15 - mmengine - INFO - Epoch(train)  [2][2770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:45  time: 0.3377  data_time: 0.0037  memory: 2478  grad_norm: 112.8918  loss: 1.3001  loss_cls: 0.3102  loss_mask: 0.1026  loss_dice: 0.8873
2025/06/23 17:28:19 - mmengine - INFO - Epoch(train)  [2][2780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:42  time: 0.3361  data_time: 0.0033  memory: 2245  grad_norm: 49.1666  loss: 1.4003  loss_cls: 0.4056  loss_mask: 0.1274  loss_dice: 0.8673
2025/06/23 17:28:22 - mmengine - INFO - Epoch(train)  [2][2790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:38  time: 0.3343  data_time: 0.0023  memory: 2380  grad_norm: 78.1818  loss: 1.2049  loss_cls: 0.3378  loss_mask: 0.0979  loss_dice: 0.7691
2025/06/23 17:28:25 - mmengine - INFO - Epoch(train)  [2][2800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:35  time: 0.3324  data_time: 0.0016  memory: 2160  grad_norm: 58.0697  loss: 2.2452  loss_cls: 0.4791  loss_mask: 0.3070  loss_dice: 1.4591
2025/06/23 17:28:29 - mmengine - INFO - Epoch(train)  [2][2810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:32  time: 0.3268  data_time: 0.0016  memory: 2300  grad_norm: 54.0422  loss: 1.8344  loss_cls: 0.3934  loss_mask: 0.2000  loss_dice: 1.2410
2025/06/23 17:28:32 - mmengine - INFO - Epoch(train)  [2][2820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:28  time: 0.3311  data_time: 0.0021  memory: 2468  grad_norm: 42.0523  loss: 1.4597  loss_cls: 0.4036  loss_mask: 0.0515  loss_dice: 1.0047
2025/06/23 17:28:35 - mmengine - INFO - Epoch(train)  [2][2830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:25  time: 0.3316  data_time: 0.0028  memory: 2221  grad_norm: 80.5605  loss: 1.5367  loss_cls: 0.2840  loss_mask: 0.1108  loss_dice: 1.1419
2025/06/23 17:28:39 - mmengine - INFO - Epoch(train)  [2][2840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:22  time: 0.3341  data_time: 0.0029  memory: 2358  grad_norm: 70.4200  loss: 1.3524  loss_cls: 0.4026  loss_mask: 0.0740  loss_dice: 0.8758
2025/06/23 17:28:42 - mmengine - INFO - Epoch(train)  [2][2850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:19  time: 0.3303  data_time: 0.0016  memory: 2166  grad_norm: 47.2650  loss: 1.0259  loss_cls: 0.2177  loss_mask: 0.1948  loss_dice: 0.6134
2025/06/23 17:28:45 - mmengine - INFO - Epoch(train)  [2][2860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:15  time: 0.3338  data_time: 0.0043  memory: 2245  grad_norm: 48.6605  loss: 1.7852  loss_cls: 0.4549  loss_mask: 0.1482  loss_dice: 1.1821
2025/06/23 17:28:49 - mmengine - INFO - Epoch(train)  [2][2870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:13  time: 0.3391  data_time: 0.0035  memory: 2446  grad_norm: 85.2271  loss: 2.0927  loss_cls: 0.4785  loss_mask: 0.1290  loss_dice: 1.4852
2025/06/23 17:28:52 - mmengine - INFO - Epoch(train)  [2][2880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:09  time: 0.3329  data_time: 0.0012  memory: 2343  grad_norm: 32.2152  loss: 1.3419  loss_cls: 0.4245  loss_mask: 0.0826  loss_dice: 0.8348
2025/06/23 17:28:55 - mmengine - INFO - Epoch(train)  [2][2890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:06  time: 0.3405  data_time: 0.0029  memory: 2343  grad_norm: 58.8771  loss: 1.9018  loss_cls: 0.5262  loss_mask: 0.2636  loss_dice: 1.1119
2025/06/23 17:28:59 - mmengine - INFO - Epoch(train)  [2][2900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:03  time: 0.3278  data_time: 0.0023  memory: 2227  grad_norm: 37.0959  loss: 1.3520  loss_cls: 0.4081  loss_mask: 0.1101  loss_dice: 0.8337
2025/06/23 17:29:02 - mmengine - INFO - Epoch(train)  [2][2910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:51:00  time: 0.3308  data_time: 0.0029  memory: 2380  grad_norm: 48.6325  loss: 1.1890  loss_cls: 0.2949  loss_mask: 0.0648  loss_dice: 0.8293
2025/06/23 17:29:05 - mmengine - INFO - Epoch(train)  [2][2920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:57  time: 0.3355  data_time: 0.0041  memory: 2534  grad_norm: 110.5273  loss: 1.8185  loss_cls: 0.4686  loss_mask: 0.1346  loss_dice: 1.2153
2025/06/23 17:29:09 - mmengine - INFO - Epoch(train)  [2][2930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:53  time: 0.3251  data_time: 0.0019  memory: 2336  grad_norm: 77.6599  loss: 1.1864  loss_cls: 0.3180  loss_mask: 0.1512  loss_dice: 0.7173
2025/06/23 17:29:12 - mmengine - INFO - Epoch(train)  [2][2940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:49  time: 0.3250  data_time: 0.0016  memory: 2172  grad_norm: 40.8448  loss: 1.3696  loss_cls: 0.2720  loss_mask: 0.1577  loss_dice: 0.9400
2025/06/23 17:29:15 - mmengine - INFO - Epoch(train)  [2][2950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:46  time: 0.3309  data_time: 0.0012  memory: 2432  grad_norm: 50.0146  loss: 1.2308  loss_cls: 0.3729  loss_mask: 0.0643  loss_dice: 0.7936
2025/06/23 17:29:18 - mmengine - INFO - Epoch(train)  [2][2960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:43  time: 0.3293  data_time: 0.0014  memory: 2505  grad_norm: 63.0891  loss: 1.2812  loss_cls: 0.3800  loss_mask: 0.1110  loss_dice: 0.7902
2025/06/23 17:29:22 - mmengine - INFO - Epoch(train)  [2][2970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:39  time: 0.3257  data_time: 0.0040  memory: 2380  grad_norm: 68.7195  loss: 2.0435  loss_cls: 0.6002  loss_mask: 0.1749  loss_dice: 1.2684
2025/06/23 17:29:25 - mmengine - INFO - Epoch(train)  [2][2980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:36  time: 0.3299  data_time: 0.0043  memory: 2439  grad_norm: 66.7737  loss: 1.5841  loss_cls: 0.3890  loss_mask: 0.1198  loss_dice: 1.0753
2025/06/23 17:29:28 - mmengine - INFO - Epoch(train)  [2][2990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:33  time: 0.3317  data_time: 0.0042  memory: 2372  grad_norm: 35.0956  loss: 1.7239  loss_cls: 0.4129  loss_mask: 0.1279  loss_dice: 1.1831
2025/06/23 17:29:32 - mmengine - INFO - Epoch(train)  [2][3000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:30  time: 0.3361  data_time: 0.0021  memory: 2372  grad_norm: 66.9576  loss: 2.3697  loss_cls: 0.4861  loss_mask: 0.3709  loss_dice: 1.5127
2025/06/23 17:29:35 - mmengine - INFO - Epoch(train)  [2][3010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:26  time: 0.3258  data_time: 0.0014  memory: 2278  grad_norm: 41.1614  loss: 1.3083  loss_cls: 0.3366  loss_mask: 0.0722  loss_dice: 0.8995
2025/06/23 17:29:38 - mmengine - INFO - Epoch(train)  [2][3020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:22  time: 0.3236  data_time: 0.0033  memory: 2239  grad_norm: 39.4696  loss: 1.3138  loss_cls: 0.3912  loss_mask: 0.0987  loss_dice: 0.8240
2025/06/23 17:29:41 - mmengine - INFO - Epoch(train)  [2][3030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:19  time: 0.3306  data_time: 0.0043  memory: 2285  grad_norm: 29.7777  loss: 1.9675  loss_cls: 0.4618  loss_mask: 0.1577  loss_dice: 1.3480
2025/06/23 17:29:45 - mmengine - INFO - Epoch(train)  [2][3040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:16  time: 0.3324  data_time: 0.0007  memory: 2505  grad_norm: 75.5752  loss: 2.0450  loss_cls: 0.4995  loss_mask: 0.1648  loss_dice: 1.3806
2025/06/23 17:29:48 - mmengine - INFO - Epoch(train)  [2][3050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:12  time: 0.3296  data_time: 0.0016  memory: 2251  grad_norm: 68.5492  loss: 1.8762  loss_cls: 0.3885  loss_mask: 0.2820  loss_dice: 1.2057
2025/06/23 17:29:51 - mmengine - INFO - Epoch(train)  [2][3060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:09  time: 0.3312  data_time: 0.0018  memory: 2300  grad_norm: 122.9583  loss: 1.5909  loss_cls: 0.4917  loss_mask: 0.0677  loss_dice: 1.0315
2025/06/23 17:29:55 - mmengine - INFO - Epoch(train)  [2][3070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:06  time: 0.3298  data_time: 0.0038  memory: 2483  grad_norm: 81.0936  loss: 1.7871  loss_cls: 0.5223  loss_mask: 0.1424  loss_dice: 1.1225
2025/06/23 17:29:58 - mmengine - INFO - Epoch(train)  [2][3080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:50:02  time: 0.3289  data_time: 0.0044  memory: 2314  grad_norm: 49.7985  loss: 1.3646  loss_cls: 0.3364  loss_mask: 0.1780  loss_dice: 0.8502
2025/06/23 17:30:01 - mmengine - INFO - Epoch(train)  [2][3090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:59  time: 0.3320  data_time: 0.0029  memory: 2490  grad_norm: 55.0263  loss: 1.6302  loss_cls: 0.4153  loss_mask: 0.1554  loss_dice: 1.0595
2025/06/23 17:30:05 - mmengine - INFO - Epoch(train)  [2][3100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:56  time: 0.3326  data_time: 0.0017  memory: 2308  grad_norm: 31.9536  loss: 1.1653  loss_cls: 0.3278  loss_mask: 0.0711  loss_dice: 0.7664
2025/06/23 17:30:08 - mmengine - INFO - Epoch(train)  [2][3110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:53  time: 0.3331  data_time: 0.0063  memory: 2184  grad_norm: 73.6142  loss: 1.7595  loss_cls: 0.4246  loss_mask: 0.1157  loss_dice: 1.2192
2025/06/23 17:30:11 - mmengine - INFO - Epoch(train)  [2][3120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:50  time: 0.3341  data_time: 0.0007  memory: 2425  grad_norm: 49.1331  loss: 2.1976  loss_cls: 0.6416  loss_mask: 0.1975  loss_dice: 1.3585
2025/06/23 17:30:15 - mmengine - INFO - Epoch(train)  [2][3130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:46  time: 0.3277  data_time: 0.0053  memory: 2468  grad_norm: 35.1870  loss: 1.3334  loss_cls: 0.2508  loss_mask: 0.1165  loss_dice: 0.9661
2025/06/23 17:30:18 - mmengine - INFO - Epoch(train)  [2][3140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:43  time: 0.3264  data_time: 0.0006  memory: 2293  grad_norm: 101.3570  loss: 1.9925  loss_cls: 0.3335  loss_mask: 0.4411  loss_dice: 1.2179
2025/06/23 17:30:21 - mmengine - INFO - Epoch(train)  [2][3150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:39  time: 0.3296  data_time: 0.0053  memory: 2197  grad_norm: 31.1624  loss: 1.7082  loss_cls: 0.4894  loss_mask: 0.1147  loss_dice: 1.1041
2025/06/23 17:30:24 - mmengine - INFO - Epoch(train)  [2][3160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:36  time: 0.3252  data_time: 0.0012  memory: 2336  grad_norm: 47.3981  loss: 1.5578  loss_cls: 0.4282  loss_mask: 0.1226  loss_dice: 1.0070
2025/06/23 17:30:28 - mmengine - INFO - Epoch(train)  [2][3170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:32  time: 0.3313  data_time: 0.0020  memory: 2417  grad_norm: 35.7241  loss: 2.2204  loss_cls: 0.5667  loss_mask: 0.0991  loss_dice: 1.5546
2025/06/23 17:30:31 - mmengine - INFO - Epoch(train)  [2][3180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:29  time: 0.3261  data_time: 0.0004  memory: 2233  grad_norm: 33.6297  loss: 1.0945  loss_cls: 0.2904  loss_mask: 0.1262  loss_dice: 0.6779
2025/06/23 17:30:34 - mmengine - INFO - Epoch(train)  [2][3190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:25  time: 0.3261  data_time: 0.0043  memory: 2308  grad_norm: 52.1325  loss: 1.4988  loss_cls: 0.3428  loss_mask: 0.2253  loss_dice: 0.9308
2025/06/23 17:30:37 - mmengine - INFO - Epoch(train)  [2][3200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:22  time: 0.3257  data_time: 0.0022  memory: 2209  grad_norm: 59.1878  loss: 1.6813  loss_cls: 0.4386  loss_mask: 0.1144  loss_dice: 1.1283
2025/06/23 17:30:41 - mmengine - INFO - Epoch(train)  [2][3210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:18  time: 0.3248  data_time: 0.0013  memory: 2308  grad_norm: 38.7020  loss: 2.2867  loss_cls: 0.5697  loss_mask: 0.4397  loss_dice: 1.2773
2025/06/23 17:30:44 - mmengine - INFO - Epoch(train)  [2][3220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:15  time: 0.3270  data_time: 0.0023  memory: 2387  grad_norm: 45.3574  loss: 1.7373  loss_cls: 0.5575  loss_mask: 0.1528  loss_dice: 1.0270
2025/06/23 17:30:47 - mmengine - INFO - Epoch(train)  [2][3230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:11  time: 0.3287  data_time: 0.0032  memory: 2293  grad_norm: 286.3928  loss: 1.6250  loss_cls: 0.4902  loss_mask: 0.1259  loss_dice: 1.0088
2025/06/23 17:30:48 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:30:51 - mmengine - INFO - Epoch(train)  [2][3240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:08  time: 0.3298  data_time: 0.0031  memory: 2483  grad_norm: 102.4534  loss: 1.3402  loss_cls: 0.4183  loss_mask: 0.0699  loss_dice: 0.8520
2025/06/23 17:30:54 - mmengine - INFO - Epoch(train)  [2][3250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:04  time: 0.3256  data_time: 0.0068  memory: 2239  grad_norm: 83.6194  loss: 1.5345  loss_cls: 0.4172  loss_mask: 0.1191  loss_dice: 0.9982
2025/06/23 17:30:57 - mmengine - INFO - Epoch(train)  [2][3260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:49:01  time: 0.3275  data_time: 0.0030  memory: 2314  grad_norm: 120.3114  loss: 1.7909  loss_cls: 0.4109  loss_mask: 0.1105  loss_dice: 1.2694
2025/06/23 17:31:00 - mmengine - INFO - Epoch(train)  [2][3270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:58  time: 0.3270  data_time: 0.0032  memory: 2446  grad_norm: 24.1706  loss: 1.5206  loss_cls: 0.4118  loss_mask: 0.0963  loss_dice: 1.0125
2025/06/23 17:31:04 - mmengine - INFO - Epoch(train)  [2][3280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:54  time: 0.3311  data_time: 0.0020  memory: 2498  grad_norm: 118.3617  loss: 1.6766  loss_cls: 0.4755  loss_mask: 0.1289  loss_dice: 1.0722
2025/06/23 17:31:07 - mmengine - INFO - Epoch(train)  [2][3290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:51  time: 0.3285  data_time: 0.0033  memory: 2478  grad_norm: 54.8628  loss: 2.0630  loss_cls: 0.4260  loss_mask: 0.7497  loss_dice: 0.8873
2025/06/23 17:31:10 - mmengine - INFO - Epoch(train)  [2][3300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:47  time: 0.3294  data_time: 0.0029  memory: 2534  grad_norm: 47.6151  loss: 1.5810  loss_cls: 0.5763  loss_mask: 0.2014  loss_dice: 0.8032
2025/06/23 17:31:13 - mmengine - INFO - Epoch(train)  [2][3310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:44  time: 0.3265  data_time: 0.0010  memory: 2197  grad_norm: 45.7683  loss: 1.4741  loss_cls: 0.4703  loss_mask: 0.1345  loss_dice: 0.8694
2025/06/23 17:31:17 - mmengine - INFO - Epoch(train)  [2][3320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:41  time: 0.3340  data_time: 0.0021  memory: 2454  grad_norm: 56.6526  loss: 1.2523  loss_cls: 0.3779  loss_mask: 0.0867  loss_dice: 0.7878
2025/06/23 17:31:20 - mmengine - INFO - Epoch(train)  [2][3330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:38  time: 0.3361  data_time: 0.0030  memory: 2184  grad_norm: 40.6211  loss: 1.4874  loss_cls: 0.4568  loss_mask: 0.1514  loss_dice: 0.8792
2025/06/23 17:31:24 - mmengine - INFO - Epoch(train)  [2][3340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:35  time: 0.3368  data_time: 0.0026  memory: 2300  grad_norm: 34.8316  loss: 1.2314  loss_cls: 0.3274  loss_mask: 0.1011  loss_dice: 0.8030
2025/06/23 17:31:27 - mmengine - INFO - Epoch(train)  [2][3350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:32  time: 0.3523  data_time: 0.0032  memory: 2563  grad_norm: 31.7319  loss: 1.5690  loss_cls: 0.5072  loss_mask: 0.1160  loss_dice: 0.9458
2025/06/23 17:31:31 - mmengine - INFO - Epoch(train)  [2][3360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:29  time: 0.3423  data_time: 0.0037  memory: 2351  grad_norm: 40.3242  loss: 1.2164  loss_cls: 0.4720  loss_mask: 0.1112  loss_dice: 0.6332
2025/06/23 17:31:34 - mmengine - INFO - Epoch(train)  [2][3370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:26  time: 0.3348  data_time: 0.0028  memory: 2372  grad_norm: 65.6579  loss: 1.3156  loss_cls: 0.4339  loss_mask: 0.0924  loss_dice: 0.7893
2025/06/23 17:31:37 - mmengine - INFO - Epoch(train)  [2][3380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:23  time: 0.3332  data_time: 0.0015  memory: 2336  grad_norm: 124.2204  loss: 0.9530  loss_cls: 0.3330  loss_mask: 0.0781  loss_dice: 0.5419
2025/06/23 17:31:40 - mmengine - INFO - Epoch(train)  [2][3390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:20  time: 0.3258  data_time: 0.0003  memory: 2251  grad_norm: 27.8481  loss: 1.1603  loss_cls: 0.2371  loss_mask: 0.1003  loss_dice: 0.8229
2025/06/23 17:31:44 - mmengine - INFO - Epoch(train)  [2][3400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:17  time: 0.3369  data_time: 0.0031  memory: 2221  grad_norm: 59.8835  loss: 1.4646  loss_cls: 0.4956  loss_mask: 0.0792  loss_dice: 0.8899
2025/06/23 17:31:47 - mmengine - INFO - Epoch(train)  [2][3410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:13  time: 0.3332  data_time: 0.0037  memory: 2239  grad_norm: 36.3815  loss: 1.5609  loss_cls: 0.3687  loss_mask: 0.1530  loss_dice: 1.0392
2025/06/23 17:31:50 - mmengine - INFO - Epoch(train)  [2][3420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:10  time: 0.3321  data_time: 0.0023  memory: 2417  grad_norm: 60.3586  loss: 1.6968  loss_cls: 0.4505  loss_mask: 0.1497  loss_dice: 1.0966
2025/06/23 17:31:54 - mmengine - INFO - Epoch(train)  [2][3430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:07  time: 0.3323  data_time: 0.0029  memory: 2454  grad_norm: 206.8814  loss: 1.0416  loss_cls: 0.3268  loss_mask: 0.0725  loss_dice: 0.6423
2025/06/23 17:31:57 - mmengine - INFO - Epoch(train)  [2][3440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:04  time: 0.3326  data_time: 0.0017  memory: 2314  grad_norm: 73.1541  loss: 1.2834  loss_cls: 0.3158  loss_mask: 0.1558  loss_dice: 0.8118
2025/06/23 17:32:00 - mmengine - INFO - Epoch(train)  [2][3450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:48:00  time: 0.3307  data_time: 0.0024  memory: 2184  grad_norm: 91.2688  loss: 1.6455  loss_cls: 0.4017  loss_mask: 0.2194  loss_dice: 1.0244
2025/06/23 17:32:04 - mmengine - INFO - Epoch(train)  [2][3460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:57  time: 0.3327  data_time: 0.0048  memory: 2380  grad_norm: 67.9950  loss: 1.5327  loss_cls: 0.3334  loss_mask: 0.3942  loss_dice: 0.8051
2025/06/23 17:32:07 - mmengine - INFO - Epoch(train)  [2][3470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:54  time: 0.3273  data_time: 0.0023  memory: 2387  grad_norm: 93.7073  loss: 1.0055  loss_cls: 0.3003  loss_mask: 0.0642  loss_dice: 0.6410
2025/06/23 17:32:10 - mmengine - INFO - Epoch(train)  [2][3480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:50  time: 0.3335  data_time: 0.0016  memory: 2454  grad_norm: 32.4712  loss: 1.4305  loss_cls: 0.4151  loss_mask: 0.0856  loss_dice: 0.9298
2025/06/23 17:32:14 - mmengine - INFO - Epoch(train)  [2][3490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:47  time: 0.3224  data_time: 0.0003  memory: 2056  grad_norm: 40.0646  loss: 0.7071  loss_cls: 0.1557  loss_mask: 0.0746  loss_dice: 0.4768
2025/06/23 17:32:17 - mmengine - INFO - Epoch(train)  [2][3500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:43  time: 0.3265  data_time: 0.0064  memory: 2321  grad_norm: 28.0601  loss: 1.2827  loss_cls: 0.2554  loss_mask: 0.0591  loss_dice: 0.9682
2025/06/23 17:32:20 - mmengine - INFO - Epoch(train)  [2][3510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:40  time: 0.3303  data_time: 0.0016  memory: 2300  grad_norm: 26.3126  loss: 0.9244  loss_cls: 0.2468  loss_mask: 0.0830  loss_dice: 0.5946
2025/06/23 17:32:24 - mmengine - INFO - Epoch(train)  [2][3520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:37  time: 0.3431  data_time: 0.0033  memory: 2197  grad_norm: 45.9332  loss: 1.1480  loss_cls: 0.3000  loss_mask: 0.1128  loss_dice: 0.7352
2025/06/23 17:32:27 - mmengine - INFO - Epoch(train)  [2][3530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:35  time: 0.3531  data_time: 0.0024  memory: 2257  grad_norm: 33.5947  loss: 1.0758  loss_cls: 0.3239  loss_mask: 0.0762  loss_dice: 0.6757
2025/06/23 17:32:30 - mmengine - INFO - Epoch(train)  [2][3540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:31  time: 0.3307  data_time: 0.0011  memory: 2468  grad_norm: 49.3183  loss: 1.3195  loss_cls: 0.2633  loss_mask: 0.1625  loss_dice: 0.8938
2025/06/23 17:32:34 - mmengine - INFO - Epoch(train)  [2][3550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:28  time: 0.3302  data_time: 0.0049  memory: 2314  grad_norm: 49.8752  loss: 1.2430  loss_cls: 0.3371  loss_mask: 0.0987  loss_dice: 0.8071
2025/06/23 17:32:37 - mmengine - INFO - Epoch(train)  [2][3560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:25  time: 0.3317  data_time: 0.0025  memory: 2358  grad_norm: 43.8489  loss: 1.1687  loss_cls: 0.3066  loss_mask: 0.1027  loss_dice: 0.7594
2025/06/23 17:32:40 - mmengine - INFO - Epoch(train)  [2][3570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:22  time: 0.3296  data_time: 0.0016  memory: 2483  grad_norm: 28.1922  loss: 1.4814  loss_cls: 0.2695  loss_mask: 0.1176  loss_dice: 1.0942
2025/06/23 17:32:44 - mmengine - INFO - Epoch(train)  [2][3580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:18  time: 0.3320  data_time: 0.0010  memory: 2215  grad_norm: 45.9077  loss: 0.8575  loss_cls: 0.2438  loss_mask: 0.0810  loss_dice: 0.5327
2025/06/23 17:32:47 - mmengine - INFO - Epoch(train)  [2][3590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:15  time: 0.3367  data_time: 0.0034  memory: 2454  grad_norm: 52.6761  loss: 1.3042  loss_cls: 0.3403  loss_mask: 0.1294  loss_dice: 0.8345
2025/06/23 17:32:50 - mmengine - INFO - Epoch(train)  [2][3600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:12  time: 0.3306  data_time: 0.0019  memory: 2336  grad_norm: 38.6652  loss: 1.1075  loss_cls: 0.3265  loss_mask: 0.0795  loss_dice: 0.7015
2025/06/23 17:32:54 - mmengine - INFO - Epoch(train)  [2][3610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:09  time: 0.3344  data_time: 0.0012  memory: 2285  grad_norm: 49.2717  loss: 1.2473  loss_cls: 0.3664  loss_mask: 0.1473  loss_dice: 0.7336
2025/06/23 17:32:57 - mmengine - INFO - Epoch(train)  [2][3620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:06  time: 0.3338  data_time: 0.0034  memory: 2321  grad_norm: 132.8907  loss: 1.4050  loss_cls: 0.3622  loss_mask: 0.1516  loss_dice: 0.8913
2025/06/23 17:33:00 - mmengine - INFO - Epoch(train)  [2][3630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:47:02  time: 0.3364  data_time: 0.0041  memory: 2278  grad_norm: 120.9621  loss: 1.4503  loss_cls: 0.4197  loss_mask: 0.0990  loss_dice: 0.9316
2025/06/23 17:33:04 - mmengine - INFO - Epoch(train)  [2][3640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:59  time: 0.3402  data_time: 0.0025  memory: 2483  grad_norm: 36.0765  loss: 1.3371  loss_cls: 0.2899  loss_mask: 0.0816  loss_dice: 0.9656
2025/06/23 17:33:07 - mmengine - INFO - Epoch(train)  [2][3650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:56  time: 0.3370  data_time: 0.0040  memory: 2365  grad_norm: 101.7487  loss: 1.4478  loss_cls: 0.4631  loss_mask: 0.2123  loss_dice: 0.7724
2025/06/23 17:33:10 - mmengine - INFO - Epoch(train)  [2][3660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:53  time: 0.3324  data_time: 0.0021  memory: 2351  grad_norm: 69.0062  loss: 1.2816  loss_cls: 0.3331  loss_mask: 0.1059  loss_dice: 0.8425
2025/06/23 17:33:14 - mmengine - INFO - Epoch(train)  [2][3670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:50  time: 0.3342  data_time: 0.0031  memory: 2215  grad_norm: 44.6114  loss: 1.0438  loss_cls: 0.2527  loss_mask: 0.0897  loss_dice: 0.7015
2025/06/23 17:33:17 - mmengine - INFO - Epoch(train)  [2][3680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:47  time: 0.3331  data_time: 0.0032  memory: 2343  grad_norm: 134.8986  loss: 1.5573  loss_cls: 0.4747  loss_mask: 0.1261  loss_dice: 0.9565
2025/06/23 17:33:20 - mmengine - INFO - Epoch(train)  [2][3690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:43  time: 0.3288  data_time: 0.0010  memory: 2336  grad_norm: 108.5662  loss: 1.4662  loss_cls: 0.4677  loss_mask: 0.0996  loss_dice: 0.8989
2025/06/23 17:33:24 - mmengine - INFO - Epoch(train)  [2][3700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:40  time: 0.3322  data_time: 0.0019  memory: 2351  grad_norm: 101.4234  loss: 1.0975  loss_cls: 0.2678  loss_mask: 0.1167  loss_dice: 0.7131
2025/06/23 17:33:27 - mmengine - INFO - Epoch(train)  [2][3710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:37  time: 0.3268  data_time: 0.0041  memory: 2227  grad_norm: 50.6849  loss: 0.8822  loss_cls: 0.2314  loss_mask: 0.1332  loss_dice: 0.5177
2025/06/23 17:33:30 - mmengine - INFO - Epoch(train)  [2][3720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:33  time: 0.3291  data_time: 0.0016  memory: 2402  grad_norm: 41.2874  loss: 1.5845  loss_cls: 0.4479  loss_mask: 0.1397  loss_dice: 0.9969
2025/06/23 17:33:34 - mmengine - INFO - Epoch(train)  [2][3730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:30  time: 0.3331  data_time: 0.0055  memory: 2372  grad_norm: 56.4331  loss: 1.3786  loss_cls: 0.3717  loss_mask: 0.1102  loss_dice: 0.8967
2025/06/23 17:33:37 - mmengine - INFO - Epoch(train)  [2][3740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:27  time: 0.3309  data_time: 0.0054  memory: 2257  grad_norm: 43.1013  loss: 1.2666  loss_cls: 0.3835  loss_mask: 0.0941  loss_dice: 0.7890
2025/06/23 17:33:40 - mmengine - INFO - Epoch(train)  [2][3750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:23  time: 0.3282  data_time: 0.0017  memory: 2541  grad_norm: 259.9707  loss: 1.7233  loss_cls: 0.4657  loss_mask: 0.1269  loss_dice: 1.1306
2025/06/23 17:33:44 - mmengine - INFO - Epoch(train)  [2][3760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:20  time: 0.3269  data_time: 0.0025  memory: 2197  grad_norm: 49.9387  loss: 1.4562  loss_cls: 0.4795  loss_mask: 0.1075  loss_dice: 0.8693
2025/06/23 17:33:46 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:33:47 - mmengine - INFO - Saving checkpoint at 2 epochs
2025/06/23 17:34:03 - mmengine - INFO - Epoch(val)  [2][ 10/209]    eta: 0:02:53  time: 0.8701  data_time: 0.4797  memory: 2172  
2025/06/23 17:34:07 - mmengine - INFO - Epoch(val)  [2][ 20/209]    eta: 0:02:03  time: 0.4383  data_time: 0.0718  memory: 1964  
2025/06/23 17:34:12 - mmengine - INFO - Epoch(val)  [2][ 30/209]    eta: 0:01:45  time: 0.4653  data_time: 0.0833  memory: 1964  
2025/06/23 17:34:18 - mmengine - INFO - Epoch(val)  [2][ 40/209]    eta: 0:01:40  time: 0.5982  data_time: 0.1880  memory: 1964  
2025/06/23 17:34:23 - mmengine - INFO - Epoch(val)  [2][ 50/209]    eta: 0:01:32  time: 0.5470  data_time: 0.1674  memory: 1964  
2025/06/23 17:34:28 - mmengine - INFO - Epoch(val)  [2][ 60/209]    eta: 0:01:23  time: 0.4622  data_time: 0.0754  memory: 1964  
2025/06/23 17:34:34 - mmengine - INFO - Epoch(val)  [2][ 70/209]    eta: 0:01:18  time: 0.5534  data_time: 0.0831  memory: 1964  
2025/06/23 17:34:39 - mmengine - INFO - Epoch(val)  [2][ 80/209]    eta: 0:01:12  time: 0.5444  data_time: 0.1181  memory: 1964  
2025/06/23 17:34:45 - mmengine - INFO - Epoch(val)  [2][ 90/209]    eta: 0:01:06  time: 0.5820  data_time: 0.1742  memory: 1964  
2025/06/23 17:34:50 - mmengine - INFO - Epoch(val)  [2][100/209]    eta: 0:01:00  time: 0.5192  data_time: 0.1169  memory: 1964  
2025/06/23 17:34:55 - mmengine - INFO - Epoch(val)  [2][110/209]    eta: 0:00:54  time: 0.5227  data_time: 0.1269  memory: 1964  
2025/06/23 17:35:00 - mmengine - INFO - Epoch(val)  [2][120/209]    eta: 0:00:48  time: 0.4808  data_time: 0.0783  memory: 1964  
2025/06/23 17:35:05 - mmengine - INFO - Epoch(val)  [2][130/209]    eta: 0:00:43  time: 0.4972  data_time: 0.0904  memory: 1964  
2025/06/23 17:35:10 - mmengine - INFO - Epoch(val)  [2][140/209]    eta: 0:00:37  time: 0.4670  data_time: 0.0937  memory: 1964  
2025/06/23 17:35:14 - mmengine - INFO - Epoch(val)  [2][150/209]    eta: 0:00:31  time: 0.4497  data_time: 0.0756  memory: 1964  
2025/06/23 17:35:19 - mmengine - INFO - Epoch(val)  [2][160/209]    eta: 0:00:26  time: 0.5040  data_time: 0.1075  memory: 1964  
2025/06/23 17:35:24 - mmengine - INFO - Epoch(val)  [2][170/209]    eta: 0:00:20  time: 0.4643  data_time: 0.0669  memory: 1964  
2025/06/23 17:35:29 - mmengine - INFO - Epoch(val)  [2][180/209]    eta: 0:00:15  time: 0.4874  data_time: 0.0826  memory: 1964  
2025/06/23 17:35:34 - mmengine - INFO - Epoch(val)  [2][190/209]    eta: 0:00:09  time: 0.5044  data_time: 0.1019  memory: 1964  
2025/06/23 17:35:38 - mmengine - INFO - Epoch(val)  [2][200/209]    eta: 0:00:04  time: 0.4375  data_time: 0.0547  memory: 1964  
2025/06/23 17:35:45 - mmengine - INFO - Evaluating segm...
2025/06/23 17:35:55 - mmengine - INFO - segm_mAP_copypaste: 0.322 0.519 0.345 0.168 0.412 0.625
2025/06/23 17:35:55 - mmengine - INFO - Epoch(val) [2][209/209]    coco/segm_mAP: 0.3220  coco/segm_mAP_50: 0.5190  coco/segm_mAP_75: 0.3450  coco/segm_mAP_s: 0.1680  coco/segm_mAP_m: 0.4120  coco/segm_mAP_l: 0.6250  data_time: 0.1199  time: 0.5185
2025/06/23 17:36:02 - mmengine - INFO - Epoch(train)  [3][  10/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:30  time: 0.7469  data_time: 0.4089  memory: 2336  grad_norm: 49.8050  loss: 1.4612  loss_cls: 0.5690  loss_mask: 0.1170  loss_dice: 0.7753
2025/06/23 17:36:06 - mmengine - INFO - Epoch(train)  [3][  20/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:27  time: 0.3302  data_time: 0.0028  memory: 2409  grad_norm: 59.4646  loss: 1.3166  loss_cls: 0.3617  loss_mask: 0.1417  loss_dice: 0.8133
2025/06/23 17:36:09 - mmengine - INFO - Epoch(train)  [3][  30/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:24  time: 0.3347  data_time: 0.0017  memory: 2432  grad_norm: 53.1781  loss: 1.5442  loss_cls: 0.4467  loss_mask: 0.1358  loss_dice: 0.9617
2025/06/23 17:36:12 - mmengine - INFO - Epoch(train)  [3][  40/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:20  time: 0.3249  data_time: 0.0030  memory: 2351  grad_norm: 66.9377  loss: 1.1966  loss_cls: 0.2264  loss_mask: 0.1436  loss_dice: 0.8266
2025/06/23 17:36:16 - mmengine - INFO - Epoch(train)  [3][  50/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:17  time: 0.3294  data_time: 0.0018  memory: 2380  grad_norm: 50.0594  loss: 1.3402  loss_cls: 0.4084  loss_mask: 0.1384  loss_dice: 0.7935
2025/06/23 17:36:19 - mmengine - INFO - Epoch(train)  [3][  60/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:13  time: 0.3316  data_time: 0.0016  memory: 2358  grad_norm: 68.0928  loss: 1.4300  loss_cls: 0.4233  loss_mask: 0.1278  loss_dice: 0.8789
2025/06/23 17:36:22 - mmengine - INFO - Epoch(train)  [3][  70/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:10  time: 0.3255  data_time: 0.0021  memory: 2439  grad_norm: 57.1281  loss: 1.4665  loss_cls: 0.3724  loss_mask: 0.1498  loss_dice: 0.9443
2025/06/23 17:36:25 - mmengine - INFO - Epoch(train)  [3][  80/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:06  time: 0.3243  data_time: 0.0017  memory: 2130  grad_norm: 29.8681  loss: 1.4694  loss_cls: 0.3888  loss_mask: 0.1057  loss_dice: 0.9749
2025/06/23 17:36:29 - mmengine - INFO - Epoch(train)  [3][  90/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:46:03  time: 0.3235  data_time: 0.0031  memory: 2166  grad_norm: 30.3295  loss: 1.5033  loss_cls: 0.2973  loss_mask: 0.2466  loss_dice: 0.9594
2025/06/23 17:36:32 - mmengine - INFO - Epoch(train)  [3][ 100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:59  time: 0.3292  data_time: 0.0059  memory: 2308  grad_norm: 34.5397  loss: 1.6980  loss_cls: 0.4092  loss_mask: 0.2409  loss_dice: 1.0478
2025/06/23 17:36:35 - mmengine - INFO - Epoch(train)  [3][ 110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:56  time: 0.3376  data_time: 0.0037  memory: 2519  grad_norm: 76.7003  loss: 2.1713  loss_cls: 0.5648  loss_mask: 0.1701  loss_dice: 1.4364
2025/06/23 17:36:39 - mmengine - INFO - Epoch(train)  [3][ 120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:53  time: 0.3332  data_time: 0.0031  memory: 2417  grad_norm: 67.5359  loss: 1.4906  loss_cls: 0.4674  loss_mask: 0.0842  loss_dice: 0.9390
2025/06/23 17:36:42 - mmengine - INFO - Epoch(train)  [3][ 130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:50  time: 0.3328  data_time: 0.0033  memory: 2372  grad_norm: 38.5220  loss: 1.3201  loss_cls: 0.3732  loss_mask: 0.1371  loss_dice: 0.8097
2025/06/23 17:36:45 - mmengine - INFO - Epoch(train)  [3][ 140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:46  time: 0.3311  data_time: 0.0028  memory: 2203  grad_norm: 64.5408  loss: 1.3227  loss_cls: 0.3739  loss_mask: 0.0539  loss_dice: 0.8949
2025/06/23 17:36:49 - mmengine - INFO - Epoch(train)  [3][ 150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:43  time: 0.3336  data_time: 0.0025  memory: 2365  grad_norm: 59.9506  loss: 1.9485  loss_cls: 0.4761  loss_mask: 0.2265  loss_dice: 1.2459
2025/06/23 17:36:52 - mmengine - INFO - Epoch(train)  [3][ 160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:40  time: 0.3345  data_time: 0.0033  memory: 2372  grad_norm: 37.0419  loss: 1.2025  loss_cls: 0.3915  loss_mask: 0.0985  loss_dice: 0.7125
2025/06/23 17:36:55 - mmengine - INFO - Epoch(train)  [3][ 170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:37  time: 0.3354  data_time: 0.0044  memory: 2380  grad_norm: 84.1347  loss: 1.3960  loss_cls: 0.3745  loss_mask: 0.1336  loss_dice: 0.8878
2025/06/23 17:36:59 - mmengine - INFO - Epoch(train)  [3][ 180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:33  time: 0.3288  data_time: 0.0034  memory: 2166  grad_norm: 34.7686  loss: 1.1186  loss_cls: 0.3472  loss_mask: 0.1094  loss_dice: 0.6619
2025/06/23 17:37:02 - mmengine - INFO - Epoch(train)  [3][ 190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:30  time: 0.3284  data_time: 0.0019  memory: 2321  grad_norm: 68.2344  loss: 1.1971  loss_cls: 0.3486  loss_mask: 0.1664  loss_dice: 0.6821
2025/06/23 17:37:05 - mmengine - INFO - Epoch(train)  [3][ 200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:27  time: 0.3329  data_time: 0.0030  memory: 2321  grad_norm: 43.5015  loss: 1.3396  loss_cls: 0.4727  loss_mask: 0.0969  loss_dice: 0.7700
2025/06/23 17:37:09 - mmengine - INFO - Epoch(train)  [3][ 210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:23  time: 0.3348  data_time: 0.0035  memory: 2351  grad_norm: 152.6406  loss: 1.7545  loss_cls: 0.4030  loss_mask: 0.1589  loss_dice: 1.1927
2025/06/23 17:37:12 - mmengine - INFO - Epoch(train)  [3][ 220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:20  time: 0.3348  data_time: 0.0044  memory: 2402  grad_norm: 337.5699  loss: 1.9216  loss_cls: 0.4442  loss_mask: 0.3430  loss_dice: 1.1344
2025/06/23 17:37:15 - mmengine - INFO - Epoch(train)  [3][ 230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:17  time: 0.3340  data_time: 0.0031  memory: 2308  grad_norm: 58.6440  loss: 1.7721  loss_cls: 0.4936  loss_mask: 0.1090  loss_dice: 1.1695
2025/06/23 17:37:19 - mmengine - INFO - Epoch(train)  [3][ 240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:14  time: 0.3280  data_time: 0.0010  memory: 2446  grad_norm: 70.2854  loss: 1.0202  loss_cls: 0.2765  loss_mask: 0.0536  loss_dice: 0.6900
2025/06/23 17:37:22 - mmengine - INFO - Epoch(train)  [3][ 250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:10  time: 0.3301  data_time: 0.0013  memory: 2321  grad_norm: 76.6084  loss: 1.8909  loss_cls: 0.5211  loss_mask: 0.1288  loss_dice: 1.2409
2025/06/23 17:37:25 - mmengine - INFO - Epoch(train)  [3][ 260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:07  time: 0.3323  data_time: 0.0028  memory: 2468  grad_norm: 79.3720  loss: 1.8519  loss_cls: 0.3528  loss_mask: 0.1941  loss_dice: 1.3050
2025/06/23 17:37:29 - mmengine - INFO - Epoch(train)  [3][ 270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:04  time: 0.3305  data_time: 0.0040  memory: 2394  grad_norm: 66.8370  loss: 1.6443  loss_cls: 0.3595  loss_mask: 0.1512  loss_dice: 1.1336
2025/06/23 17:37:32 - mmengine - INFO - Epoch(train)  [3][ 280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:45:00  time: 0.3310  data_time: 0.0043  memory: 2372  grad_norm: 41.8809  loss: 1.3470  loss_cls: 0.3535  loss_mask: 0.0835  loss_dice: 0.9099
2025/06/23 17:37:35 - mmengine - INFO - Epoch(train)  [3][ 290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:57  time: 0.3291  data_time: 0.0026  memory: 2446  grad_norm: 57.0459  loss: 1.6616  loss_cls: 0.4291  loss_mask: 0.1809  loss_dice: 1.0516
2025/06/23 17:37:38 - mmengine - INFO - Epoch(train)  [3][ 300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:53  time: 0.3256  data_time: 0.0030  memory: 2380  grad_norm: 26.5586  loss: 0.9624  loss_cls: 0.2294  loss_mask: 0.0724  loss_dice: 0.6606
2025/06/23 17:37:42 - mmengine - INFO - Epoch(train)  [3][ 310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:50  time: 0.3247  data_time: 0.0023  memory: 2358  grad_norm: 55.5121  loss: 1.3381  loss_cls: 0.3688  loss_mask: 0.1231  loss_dice: 0.8461
2025/06/23 17:37:45 - mmengine - INFO - Epoch(train)  [3][ 320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:46  time: 0.3294  data_time: 0.0058  memory: 2300  grad_norm: 40.8224  loss: 1.4754  loss_cls: 0.3672  loss_mask: 0.0989  loss_dice: 1.0093
2025/06/23 17:37:48 - mmengine - INFO - Epoch(train)  [3][ 330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:43  time: 0.3331  data_time: 0.0032  memory: 2239  grad_norm: 57.5198  loss: 1.2546  loss_cls: 0.3014  loss_mask: 0.1641  loss_dice: 0.7891
2025/06/23 17:37:52 - mmengine - INFO - Epoch(train)  [3][ 340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:40  time: 0.3286  data_time: 0.0048  memory: 2239  grad_norm: 75.6562  loss: 1.2295  loss_cls: 0.3638  loss_mask: 0.1161  loss_dice: 0.7495
2025/06/23 17:37:55 - mmengine - INFO - Epoch(train)  [3][ 350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:36  time: 0.3295  data_time: 0.0020  memory: 2336  grad_norm: 41.3930  loss: 0.8922  loss_cls: 0.1751  loss_mask: 0.1024  loss_dice: 0.6147
2025/06/23 17:37:58 - mmengine - INFO - Epoch(train)  [3][ 360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:33  time: 0.3311  data_time: 0.0033  memory: 2293  grad_norm: 62.1545  loss: 1.4815  loss_cls: 0.2368  loss_mask: 0.1575  loss_dice: 1.0872
2025/06/23 17:38:02 - mmengine - INFO - Epoch(train)  [3][ 370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:30  time: 0.3358  data_time: 0.0054  memory: 2387  grad_norm: 42.2834  loss: 1.3277  loss_cls: 0.3438  loss_mask: 0.1281  loss_dice: 0.8558
2025/06/23 17:38:05 - mmengine - INFO - Epoch(train)  [3][ 380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:27  time: 0.3339  data_time: 0.0019  memory: 2314  grad_norm: 37.5501  loss: 1.7328  loss_cls: 0.3795  loss_mask: 0.1442  loss_dice: 1.2092
2025/06/23 17:38:08 - mmengine - INFO - Epoch(train)  [3][ 390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:23  time: 0.3328  data_time: 0.0032  memory: 2160  grad_norm: 28.1176  loss: 0.7327  loss_cls: 0.1617  loss_mask: 0.0780  loss_dice: 0.4929
2025/06/23 17:38:12 - mmengine - INFO - Epoch(train)  [3][ 400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:20  time: 0.3375  data_time: 0.0040  memory: 2372  grad_norm: 38.6389  loss: 1.2601  loss_cls: 0.2915  loss_mask: 0.0905  loss_dice: 0.8781
2025/06/23 17:38:15 - mmengine - INFO - Epoch(train)  [3][ 410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:17  time: 0.3379  data_time: 0.0035  memory: 2336  grad_norm: 22.3484  loss: 0.9307  loss_cls: 0.2293  loss_mask: 0.0933  loss_dice: 0.6081
2025/06/23 17:38:18 - mmengine - INFO - Epoch(train)  [3][ 420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:14  time: 0.3442  data_time: 0.0026  memory: 2490  grad_norm: 42.8180  loss: 1.7179  loss_cls: 0.5196  loss_mask: 0.1146  loss_dice: 1.0837
2025/06/23 17:38:22 - mmengine - INFO - Epoch(train)  [3][ 430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:11  time: 0.3377  data_time: 0.0037  memory: 2166  grad_norm: 172.8062  loss: 1.4216  loss_cls: 0.3625  loss_mask: 0.1244  loss_dice: 0.9347
2025/06/23 17:38:25 - mmengine - INFO - Epoch(train)  [3][ 440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:08  time: 0.3413  data_time: 0.0028  memory: 2380  grad_norm: 57.8935  loss: 2.0354  loss_cls: 0.6273  loss_mask: 0.1224  loss_dice: 1.2857
2025/06/23 17:38:28 - mmengine - INFO - Epoch(train)  [3][ 450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:05  time: 0.3332  data_time: 0.0026  memory: 2351  grad_norm: 50.6153  loss: 1.3111  loss_cls: 0.4014  loss_mask: 0.1132  loss_dice: 0.7966
2025/06/23 17:38:32 - mmengine - INFO - Epoch(train)  [3][ 460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:44:02  time: 0.3333  data_time: 0.0039  memory: 2417  grad_norm: 63.3814  loss: 2.5508  loss_cls: 0.5814  loss_mask: 0.2732  loss_dice: 1.6962
2025/06/23 17:38:33 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:38:35 - mmengine - INFO - Epoch(train)  [3][ 470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:59  time: 0.3352  data_time: 0.0010  memory: 2203  grad_norm: 192.6163  loss: 1.7616  loss_cls: 0.3799  loss_mask: 0.3396  loss_dice: 1.0422
2025/06/23 17:38:39 - mmengine - INFO - Epoch(train)  [3][ 480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:55  time: 0.3374  data_time: 0.0006  memory: 2184  grad_norm: 43.7979  loss: 1.5216  loss_cls: 0.3132  loss_mask: 0.1221  loss_dice: 1.0863
2025/06/23 17:38:42 - mmengine - INFO - Epoch(train)  [3][ 490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:52  time: 0.3329  data_time: 0.0058  memory: 2321  grad_norm: 81.3781  loss: 1.6358  loss_cls: 0.2592  loss_mask: 0.2432  loss_dice: 1.1334
2025/06/23 17:38:45 - mmengine - INFO - Epoch(train)  [3][ 500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:49  time: 0.3333  data_time: 0.0035  memory: 2534  grad_norm: 34.2179  loss: 1.4597  loss_cls: 0.2580  loss_mask: 0.1248  loss_dice: 1.0768
2025/06/23 17:38:49 - mmengine - INFO - Epoch(train)  [3][ 510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:46  time: 0.3305  data_time: 0.0019  memory: 2278  grad_norm: 33.6976  loss: 1.4056  loss_cls: 0.2851  loss_mask: 0.1407  loss_dice: 0.9798
2025/06/23 17:38:52 - mmengine - INFO - Epoch(train)  [3][ 520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:42  time: 0.3342  data_time: 0.0034  memory: 2365  grad_norm: 70.4481  loss: 1.3612  loss_cls: 0.3085  loss_mask: 0.1035  loss_dice: 0.9492
2025/06/23 17:38:55 - mmengine - INFO - Epoch(train)  [3][ 530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:39  time: 0.3345  data_time: 0.0043  memory: 2490  grad_norm: 143.1670  loss: 1.3711  loss_cls: 0.2996  loss_mask: 0.0996  loss_dice: 0.9719
2025/06/23 17:38:59 - mmengine - INFO - Epoch(train)  [3][ 540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:36  time: 0.3305  data_time: 0.0036  memory: 2257  grad_norm: 71.7040  loss: 1.3188  loss_cls: 0.2616  loss_mask: 0.1777  loss_dice: 0.8794
2025/06/23 17:39:02 - mmengine - INFO - Epoch(train)  [3][ 550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:33  time: 0.3332  data_time: 0.0026  memory: 2351  grad_norm: 107.7055  loss: 1.2566  loss_cls: 0.3266  loss_mask: 0.1153  loss_dice: 0.8147
2025/06/23 17:39:05 - mmengine - INFO - Epoch(train)  [3][ 560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:29  time: 0.3332  data_time: 0.0018  memory: 2300  grad_norm: 74.4699  loss: 1.4089  loss_cls: 0.3223  loss_mask: 0.1044  loss_dice: 0.9822
2025/06/23 17:39:09 - mmengine - INFO - Epoch(train)  [3][ 570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:26  time: 0.3319  data_time: 0.0016  memory: 2351  grad_norm: 92.3308  loss: 1.2323  loss_cls: 0.3446  loss_mask: 0.0774  loss_dice: 0.8103
2025/06/23 17:39:12 - mmengine - INFO - Epoch(train)  [3][ 580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:23  time: 0.3284  data_time: 0.0045  memory: 2454  grad_norm: 351.1165  loss: 2.0812  loss_cls: 0.6139  loss_mask: 0.3313  loss_dice: 1.1360
2025/06/23 17:39:15 - mmengine - INFO - Epoch(train)  [3][ 590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:19  time: 0.3311  data_time: 0.0009  memory: 2321  grad_norm: 88.5107  loss: 2.4230  loss_cls: 0.5368  loss_mask: 0.3548  loss_dice: 1.5314
2025/06/23 17:39:18 - mmengine - INFO - Epoch(train)  [3][ 600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:16  time: 0.3288  data_time: 0.0030  memory: 2233  grad_norm: 76.4178  loss: 1.0829  loss_cls: 0.2384  loss_mask: 0.1159  loss_dice: 0.7286
2025/06/23 17:39:22 - mmengine - INFO - Epoch(train)  [3][ 610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:13  time: 0.3302  data_time: 0.0048  memory: 2099  grad_norm: 127.8689  loss: 1.3312  loss_cls: 0.4103  loss_mask: 0.0889  loss_dice: 0.8320
2025/06/23 17:39:25 - mmengine - INFO - Epoch(train)  [3][ 620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:09  time: 0.3311  data_time: 0.0024  memory: 2148  grad_norm: 44.9426  loss: 1.2692  loss_cls: 0.3533  loss_mask: 0.1047  loss_dice: 0.8112
2025/06/23 17:39:28 - mmengine - INFO - Epoch(train)  [3][ 630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:06  time: 0.3365  data_time: 0.0024  memory: 2483  grad_norm: 82.5363  loss: 1.6513  loss_cls: 0.4207  loss_mask: 0.1349  loss_dice: 1.0957
2025/06/23 17:39:32 - mmengine - INFO - Epoch(train)  [3][ 640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:43:03  time: 0.3290  data_time: 0.0034  memory: 2300  grad_norm: 44.6967  loss: 1.3535  loss_cls: 0.4197  loss_mask: 0.1377  loss_dice: 0.7960
2025/06/23 17:39:35 - mmengine - INFO - Epoch(train)  [3][ 650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:59  time: 0.3278  data_time: 0.0081  memory: 2329  grad_norm: 54.1514  loss: 1.5220  loss_cls: 0.4184  loss_mask: 0.0863  loss_dice: 1.0173
2025/06/23 17:39:38 - mmengine - INFO - Epoch(train)  [3][ 660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:56  time: 0.3260  data_time: 0.0010  memory: 2285  grad_norm: 28.0926  loss: 0.9843  loss_cls: 0.3273  loss_mask: 0.0565  loss_dice: 0.6006
2025/06/23 17:39:42 - mmengine - INFO - Epoch(train)  [3][ 670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:52  time: 0.3315  data_time: 0.0033  memory: 2387  grad_norm: 73.8383  loss: 1.7489  loss_cls: 0.5060  loss_mask: 0.1191  loss_dice: 1.1238
2025/06/23 17:39:45 - mmengine - INFO - Epoch(train)  [3][ 680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:49  time: 0.3278  data_time: 0.0020  memory: 2409  grad_norm: 173.8903  loss: 1.5724  loss_cls: 0.3706  loss_mask: 0.0933  loss_dice: 1.1085
2025/06/23 17:39:48 - mmengine - INFO - Epoch(train)  [3][ 690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:46  time: 0.3318  data_time: 0.0034  memory: 2387  grad_norm: 60.0575  loss: 1.5134  loss_cls: 0.4335  loss_mask: 0.1092  loss_dice: 0.9706
2025/06/23 17:39:51 - mmengine - INFO - Epoch(train)  [3][ 700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:42  time: 0.3226  data_time: 0.0011  memory: 2343  grad_norm: 160.3374  loss: 1.0943  loss_cls: 0.3072  loss_mask: 0.0880  loss_dice: 0.6991
2025/06/23 17:39:55 - mmengine - INFO - Epoch(train)  [3][ 710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:39  time: 0.3310  data_time: 0.0034  memory: 2490  grad_norm: 46.9996  loss: 1.5950  loss_cls: 0.4605  loss_mask: 0.2167  loss_dice: 0.9178
2025/06/23 17:39:58 - mmengine - INFO - Epoch(train)  [3][ 720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:35  time: 0.3329  data_time: 0.0039  memory: 2468  grad_norm: 85.8599  loss: 2.1129  loss_cls: 0.6176  loss_mask: 0.1537  loss_dice: 1.3416
2025/06/23 17:40:01 - mmengine - INFO - Epoch(train)  [3][ 730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:32  time: 0.3308  data_time: 0.0020  memory: 2358  grad_norm: 89.8122  loss: 1.2487  loss_cls: 0.4738  loss_mask: 0.1116  loss_dice: 0.6633
2025/06/23 17:40:05 - mmengine - INFO - Epoch(train)  [3][ 740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:29  time: 0.3348  data_time: 0.0019  memory: 2203  grad_norm: 43.9136  loss: 0.9471  loss_cls: 0.1999  loss_mask: 0.0550  loss_dice: 0.6922
2025/06/23 17:40:08 - mmengine - INFO - Epoch(train)  [3][ 750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:26  time: 0.3386  data_time: 0.0048  memory: 2343  grad_norm: 59.6081  loss: 1.0418  loss_cls: 0.2478  loss_mask: 0.1043  loss_dice: 0.6898
2025/06/23 17:40:11 - mmengine - INFO - Epoch(train)  [3][ 760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:22  time: 0.3288  data_time: 0.0021  memory: 2351  grad_norm: 64.6074  loss: 1.3389  loss_cls: 0.3331  loss_mask: 0.1342  loss_dice: 0.8716
2025/06/23 17:40:15 - mmengine - INFO - Epoch(train)  [3][ 770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:19  time: 0.3330  data_time: 0.0019  memory: 2439  grad_norm: 98.9723  loss: 1.3247  loss_cls: 0.4006  loss_mask: 0.0802  loss_dice: 0.8439
2025/06/23 17:40:18 - mmengine - INFO - Epoch(train)  [3][ 780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:16  time: 0.3308  data_time: 0.0015  memory: 2245  grad_norm: 140.0244  loss: 1.2975  loss_cls: 0.3634  loss_mask: 0.1040  loss_dice: 0.8301
2025/06/23 17:40:21 - mmengine - INFO - Epoch(train)  [3][ 790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:13  time: 0.3325  data_time: 0.0039  memory: 2227  grad_norm: 62.6061  loss: 1.1053  loss_cls: 0.3285  loss_mask: 0.0836  loss_dice: 0.6932
2025/06/23 17:40:25 - mmengine - INFO - Epoch(train)  [3][ 800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:10  time: 0.3429  data_time: 0.0019  memory: 2203  grad_norm: 34.9920  loss: 1.2125  loss_cls: 0.3135  loss_mask: 0.1039  loss_dice: 0.7952
2025/06/23 17:40:28 - mmengine - INFO - Epoch(train)  [3][ 810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:06  time: 0.3319  data_time: 0.0043  memory: 2285  grad_norm: 86.7335  loss: 1.5249  loss_cls: 0.3759  loss_mask: 0.1608  loss_dice: 0.9882
2025/06/23 17:40:31 - mmengine - INFO - Epoch(train)  [3][ 820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:03  time: 0.3267  data_time: 0.0024  memory: 2209  grad_norm: 162.9237  loss: 1.1542  loss_cls: 0.2611  loss_mask: 0.1643  loss_dice: 0.7288
2025/06/23 17:40:35 - mmengine - INFO - Epoch(train)  [3][ 830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:42:00  time: 0.3334  data_time: 0.0033  memory: 2505  grad_norm: 88.8980  loss: 1.6356  loss_cls: 0.5028  loss_mask: 0.1174  loss_dice: 1.0153
2025/06/23 17:40:38 - mmengine - INFO - Epoch(train)  [3][ 840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:56  time: 0.3301  data_time: 0.0037  memory: 2293  grad_norm: 90.9122  loss: 1.3557  loss_cls: 0.3968  loss_mask: 0.1624  loss_dice: 0.7964
2025/06/23 17:40:41 - mmengine - INFO - Epoch(train)  [3][ 850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:53  time: 0.3238  data_time: 0.0036  memory: 2191  grad_norm: 77.9455  loss: 1.1558  loss_cls: 0.3314  loss_mask: 0.1308  loss_dice: 0.6935
2025/06/23 17:40:44 - mmengine - INFO - Epoch(train)  [3][ 860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:49  time: 0.3295  data_time: 0.0036  memory: 2285  grad_norm: 136.9765  loss: 1.3034  loss_cls: 0.3901  loss_mask: 0.0830  loss_dice: 0.8303
2025/06/23 17:40:48 - mmengine - INFO - Epoch(train)  [3][ 870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:46  time: 0.3328  data_time: 0.0051  memory: 2534  grad_norm: 46.4442  loss: 1.1842  loss_cls: 0.3271  loss_mask: 0.0848  loss_dice: 0.7723
2025/06/23 17:40:51 - mmengine - INFO - Epoch(train)  [3][ 880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:43  time: 0.3289  data_time: 0.0033  memory: 2278  grad_norm: 185.5865  loss: 1.6564  loss_cls: 0.4785  loss_mask: 0.1209  loss_dice: 1.0571
2025/06/23 17:40:54 - mmengine - INFO - Epoch(train)  [3][ 890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:39  time: 0.3268  data_time: 0.0027  memory: 2372  grad_norm: 246.0722  loss: 1.4935  loss_cls: 0.2664  loss_mask: 0.1183  loss_dice: 1.1088
2025/06/23 17:40:58 - mmengine - INFO - Epoch(train)  [3][ 900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:36  time: 0.3316  data_time: 0.0026  memory: 2321  grad_norm: 135.1762  loss: 1.4071  loss_cls: 0.3064  loss_mask: 0.1612  loss_dice: 0.9395
2025/06/23 17:41:01 - mmengine - INFO - Epoch(train)  [3][ 910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:33  time: 0.3399  data_time: 0.0039  memory: 2351  grad_norm: 65.6886  loss: 1.6034  loss_cls: 0.4064  loss_mask: 0.1320  loss_dice: 1.0649
2025/06/23 17:41:04 - mmengine - INFO - Epoch(train)  [3][ 920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:30  time: 0.3406  data_time: 0.0035  memory: 2534  grad_norm: 293.7466  loss: 1.7432  loss_cls: 0.3674  loss_mask: 0.1451  loss_dice: 1.2307
2025/06/23 17:41:08 - mmengine - INFO - Epoch(train)  [3][ 930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:27  time: 0.3402  data_time: 0.0031  memory: 2387  grad_norm: 129.3743  loss: 1.4692  loss_cls: 0.3404  loss_mask: 0.1124  loss_dice: 1.0164
2025/06/23 17:41:11 - mmengine - INFO - Epoch(train)  [3][ 940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:24  time: 0.3440  data_time: 0.0034  memory: 2387  grad_norm: 117.1494  loss: 1.4739  loss_cls: 0.4916  loss_mask: 0.0833  loss_dice: 0.8991
2025/06/23 17:41:15 - mmengine - INFO - Epoch(train)  [3][ 950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:20  time: 0.3297  data_time: 0.0034  memory: 2271  grad_norm: 137.8206  loss: 1.2580  loss_cls: 0.3592  loss_mask: 0.1134  loss_dice: 0.7854
2025/06/23 17:41:18 - mmengine - INFO - Epoch(train)  [3][ 960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:17  time: 0.3350  data_time: 0.0032  memory: 2293  grad_norm: 70.6139  loss: 1.4334  loss_cls: 0.3427  loss_mask: 0.2638  loss_dice: 0.8269
2025/06/23 17:41:21 - mmengine - INFO - Epoch(train)  [3][ 970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:14  time: 0.3309  data_time: 0.0027  memory: 2314  grad_norm: 141.8054  loss: 1.5976  loss_cls: 0.3246  loss_mask: 0.0991  loss_dice: 1.1739
2025/06/23 17:41:25 - mmengine - INFO - Epoch(train)  [3][ 980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:11  time: 0.3311  data_time: 0.0038  memory: 2409  grad_norm: 42.8338  loss: 1.4174  loss_cls: 0.3996  loss_mask: 0.1359  loss_dice: 0.8820
2025/06/23 17:41:28 - mmengine - INFO - Epoch(train)  [3][ 990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:07  time: 0.3289  data_time: 0.0027  memory: 2358  grad_norm: 291.9878  loss: 1.3473  loss_cls: 0.3696  loss_mask: 0.1164  loss_dice: 0.8612
2025/06/23 17:41:31 - mmengine - INFO - Epoch(train)  [3][1000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:04  time: 0.3324  data_time: 0.0025  memory: 2358  grad_norm: 326.7956  loss: 1.6805  loss_cls: 0.6217  loss_mask: 0.0913  loss_dice: 0.9675
2025/06/23 17:41:34 - mmengine - INFO - Epoch(train)  [3][1010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:41:00  time: 0.3272  data_time: 0.0043  memory: 2425  grad_norm: 95.3459  loss: 1.1082  loss_cls: 0.3162  loss_mask: 0.0838  loss_dice: 0.7081
2025/06/23 17:41:38 - mmengine - INFO - Epoch(train)  [3][1020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:57  time: 0.3258  data_time: 0.0041  memory: 2148  grad_norm: 178.8618  loss: 0.9889  loss_cls: 0.2976  loss_mask: 0.1082  loss_dice: 0.5831
2025/06/23 17:41:41 - mmengine - INFO - Epoch(train)  [3][1030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:54  time: 0.3278  data_time: 0.0021  memory: 2417  grad_norm: 141.7741  loss: 1.7222  loss_cls: 0.6257  loss_mask: 0.1760  loss_dice: 0.9205
2025/06/23 17:41:44 - mmengine - INFO - Epoch(train)  [3][1040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:50  time: 0.3239  data_time: 0.0030  memory: 2239  grad_norm: 89.6193  loss: 1.1020  loss_cls: 0.4045  loss_mask: 0.0732  loss_dice: 0.6243
2025/06/23 17:41:47 - mmengine - INFO - Epoch(train)  [3][1050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:46  time: 0.3259  data_time: 0.0039  memory: 2191  grad_norm: 71.2940  loss: 1.7250  loss_cls: 0.4733  loss_mask: 0.0908  loss_dice: 1.1609
2025/06/23 17:41:51 - mmengine - INFO - Epoch(train)  [3][1060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:43  time: 0.3260  data_time: 0.0021  memory: 2293  grad_norm: 152.8533  loss: 1.6104  loss_cls: 0.4590  loss_mask: 0.1535  loss_dice: 0.9979
2025/06/23 17:41:54 - mmengine - INFO - Epoch(train)  [3][1070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:39  time: 0.3256  data_time: 0.0032  memory: 2336  grad_norm: 192.5383  loss: 1.2791  loss_cls: 0.3297  loss_mask: 0.0677  loss_dice: 0.8818
2025/06/23 17:41:57 - mmengine - INFO - Epoch(train)  [3][1080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:36  time: 0.3259  data_time: 0.0033  memory: 2409  grad_norm: 81.7673  loss: 1.0221  loss_cls: 0.2964  loss_mask: 0.1071  loss_dice: 0.6186
2025/06/23 17:42:01 - mmengine - INFO - Epoch(train)  [3][1090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:32  time: 0.3271  data_time: 0.0017  memory: 2394  grad_norm: 65.1797  loss: 1.3769  loss_cls: 0.3898  loss_mask: 0.1591  loss_dice: 0.8280
2025/06/23 17:42:04 - mmengine - INFO - Epoch(train)  [3][1100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:29  time: 0.3270  data_time: 0.0027  memory: 2343  grad_norm: 74.3969  loss: 1.2248  loss_cls: 0.4383  loss_mask: 0.1072  loss_dice: 0.6793
2025/06/23 17:42:07 - mmengine - INFO - Epoch(train)  [3][1110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:26  time: 0.3300  data_time: 0.0007  memory: 2461  grad_norm: 69.7688  loss: 1.5103  loss_cls: 0.4770  loss_mask: 0.0919  loss_dice: 0.9415
2025/06/23 17:42:10 - mmengine - INFO - Epoch(train)  [3][1120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:22  time: 0.3258  data_time: 0.0051  memory: 2329  grad_norm: 79.9019  loss: 1.5164  loss_cls: 0.3936  loss_mask: 0.1340  loss_dice: 0.9888
2025/06/23 17:42:14 - mmengine - INFO - Epoch(train)  [3][1130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:19  time: 0.3276  data_time: 0.0028  memory: 2505  grad_norm: 244.7051  loss: 0.9679  loss_cls: 0.2715  loss_mask: 0.0971  loss_dice: 0.5993
2025/06/23 17:42:17 - mmengine - INFO - Epoch(train)  [3][1140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:15  time: 0.3249  data_time: 0.0038  memory: 2372  grad_norm: 48.5937  loss: 1.0682  loss_cls: 0.3171  loss_mask: 0.0811  loss_dice: 0.6701
2025/06/23 17:42:20 - mmengine - INFO - Epoch(train)  [3][1150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:12  time: 0.3302  data_time: 0.0035  memory: 2505  grad_norm: 87.8677  loss: 1.3948  loss_cls: 0.4052  loss_mask: 0.1123  loss_dice: 0.8773
2025/06/23 17:42:23 - mmengine - INFO - Epoch(train)  [3][1160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:08  time: 0.3252  data_time: 0.0046  memory: 2285  grad_norm: 27.5910  loss: 1.1618  loss_cls: 0.2695  loss_mask: 0.0640  loss_dice: 0.8283
2025/06/23 17:42:27 - mmengine - INFO - Epoch(train)  [3][1170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:05  time: 0.3297  data_time: 0.0031  memory: 2505  grad_norm: 267.4489  loss: 1.7240  loss_cls: 0.6483  loss_mask: 0.1195  loss_dice: 0.9562
2025/06/23 17:42:30 - mmengine - INFO - Epoch(train)  [3][1180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:40:01  time: 0.3225  data_time: 0.0018  memory: 2166  grad_norm: 237.4984  loss: 0.7344  loss_cls: 0.1906  loss_mask: 0.0669  loss_dice: 0.4769
2025/06/23 17:42:33 - mmengine - INFO - Epoch(train)  [3][1190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:58  time: 0.3288  data_time: 0.0039  memory: 2483  grad_norm: 45.7168  loss: 1.3863  loss_cls: 0.3917  loss_mask: 0.0985  loss_dice: 0.8961
2025/06/23 17:42:37 - mmengine - INFO - Epoch(train)  [3][1200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:54  time: 0.3274  data_time: 0.0085  memory: 2402  grad_norm: 72.3700  loss: 1.1976  loss_cls: 0.3954  loss_mask: 0.0900  loss_dice: 0.7122
2025/06/23 17:42:40 - mmengine - INFO - Epoch(train)  [3][1210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:51  time: 0.3263  data_time: 0.0034  memory: 2271  grad_norm: 79.1120  loss: 1.0831  loss_cls: 0.3553  loss_mask: 0.0900  loss_dice: 0.6378
2025/06/23 17:42:43 - mmengine - INFO - Epoch(train)  [3][1220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:47  time: 0.3262  data_time: 0.0015  memory: 2321  grad_norm: 106.1779  loss: 1.4684  loss_cls: 0.4435  loss_mask: 0.1981  loss_dice: 0.8268
2025/06/23 17:42:46 - mmengine - INFO - Epoch(train)  [3][1230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:44  time: 0.3247  data_time: 0.0022  memory: 2314  grad_norm: 55.3702  loss: 1.3380  loss_cls: 0.3778  loss_mask: 0.1194  loss_dice: 0.8408
2025/06/23 17:42:50 - mmengine - INFO - Epoch(train)  [3][1240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:41  time: 0.3288  data_time: 0.0021  memory: 2351  grad_norm: 74.7655  loss: 1.9125  loss_cls: 0.5947  loss_mask: 0.1302  loss_dice: 1.1875
2025/06/23 17:42:53 - mmengine - INFO - Epoch(train)  [3][1250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:37  time: 0.3272  data_time: 0.0015  memory: 2478  grad_norm: 65.0758  loss: 1.2368  loss_cls: 0.4259  loss_mask: 0.0834  loss_dice: 0.7275
2025/06/23 17:42:56 - mmengine - INFO - Epoch(train)  [3][1260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:34  time: 0.3242  data_time: 0.0040  memory: 2051  grad_norm: 194.6403  loss: 1.4323  loss_cls: 0.3626  loss_mask: 0.0737  loss_dice: 0.9961
2025/06/23 17:42:59 - mmengine - INFO - Epoch(train)  [3][1270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:30  time: 0.3264  data_time: 0.0034  memory: 2527  grad_norm: 136.3385  loss: 1.2649  loss_cls: 0.3438  loss_mask: 0.1209  loss_dice: 0.8002
2025/06/23 17:43:03 - mmengine - INFO - Epoch(train)  [3][1280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:27  time: 0.3273  data_time: 0.0020  memory: 2365  grad_norm: 53.9890  loss: 0.8531  loss_cls: 0.2143  loss_mask: 0.0638  loss_dice: 0.5751
2025/06/23 17:43:06 - mmengine - INFO - Epoch(train)  [3][1290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:23  time: 0.3225  data_time: 0.0013  memory: 2069  grad_norm: 83.8321  loss: 1.0851  loss_cls: 0.2572  loss_mask: 0.1350  loss_dice: 0.6929
2025/06/23 17:43:09 - mmengine - INFO - Epoch(train)  [3][1300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:19  time: 0.3256  data_time: 0.0026  memory: 2321  grad_norm: 76.1550  loss: 1.0425  loss_cls: 0.2818  loss_mask: 0.0787  loss_dice: 0.6821
2025/06/23 17:43:12 - mmengine - INFO - Epoch(train)  [3][1310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:16  time: 0.3287  data_time: 0.0018  memory: 2365  grad_norm: 166.6142  loss: 1.3870  loss_cls: 0.4041  loss_mask: 0.1159  loss_dice: 0.8671
2025/06/23 17:43:16 - mmengine - INFO - Epoch(train)  [3][1320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:13  time: 0.3268  data_time: 0.0024  memory: 2409  grad_norm: 650.9391  loss: 1.1092  loss_cls: 0.2953  loss_mask: 0.0803  loss_dice: 0.7336
2025/06/23 17:43:19 - mmengine - INFO - Epoch(train)  [3][1330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:09  time: 0.3264  data_time: 0.0017  memory: 2365  grad_norm: 57.7437  loss: 1.3365  loss_cls: 0.4213  loss_mask: 0.1640  loss_dice: 0.7512
2025/06/23 17:43:22 - mmengine - INFO - Epoch(train)  [3][1340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:06  time: 0.3313  data_time: 0.0069  memory: 2454  grad_norm: 64.1467  loss: 1.5751  loss_cls: 0.3842  loss_mask: 0.1245  loss_dice: 1.0665
2025/06/23 17:43:26 - mmengine - INFO - Epoch(train)  [3][1350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:39:02  time: 0.3266  data_time: 0.0023  memory: 2203  grad_norm: 88.3803  loss: 1.1632  loss_cls: 0.2675  loss_mask: 0.0792  loss_dice: 0.8164
2025/06/23 17:43:29 - mmengine - INFO - Epoch(train)  [3][1360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:59  time: 0.3280  data_time: 0.0006  memory: 2351  grad_norm: 110.3014  loss: 1.2813  loss_cls: 0.3638  loss_mask: 0.1478  loss_dice: 0.7697
2025/06/23 17:43:32 - mmengine - INFO - Epoch(train)  [3][1370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:55  time: 0.3263  data_time: 0.0030  memory: 2372  grad_norm: 53.0333  loss: 1.4540  loss_cls: 0.3536  loss_mask: 0.0782  loss_dice: 1.0222
2025/06/23 17:43:35 - mmengine - INFO - Epoch(train)  [3][1380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:52  time: 0.3262  data_time: 0.0024  memory: 2130  grad_norm: 118.6144  loss: 1.2866  loss_cls: 0.3847  loss_mask: 0.1144  loss_dice: 0.7875
2025/06/23 17:43:39 - mmengine - INFO - Epoch(train)  [3][1390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:49  time: 0.3271  data_time: 0.0007  memory: 2233  grad_norm: 117.4031  loss: 1.2698  loss_cls: 0.3526  loss_mask: 0.1088  loss_dice: 0.8084
2025/06/23 17:43:42 - mmengine - INFO - Epoch(train)  [3][1400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:45  time: 0.3232  data_time: 0.0021  memory: 2166  grad_norm: 67.0846  loss: 1.1717  loss_cls: 0.2807  loss_mask: 0.1575  loss_dice: 0.7336
2025/06/23 17:43:45 - mmengine - INFO - Epoch(train)  [3][1410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:41  time: 0.3232  data_time: 0.0039  memory: 2215  grad_norm: 58.0715  loss: 0.8397  loss_cls: 0.2102  loss_mask: 0.1249  loss_dice: 0.5047
2025/06/23 17:43:48 - mmengine - INFO - Epoch(train)  [3][1420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:38  time: 0.3250  data_time: 0.0009  memory: 2172  grad_norm: 67.7565  loss: 1.4189  loss_cls: 0.3792  loss_mask: 0.2398  loss_dice: 0.7999
2025/06/23 17:43:52 - mmengine - INFO - Epoch(train)  [3][1430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:34  time: 0.3259  data_time: 0.0045  memory: 2285  grad_norm: 147.1384  loss: 1.4680  loss_cls: 0.3983  loss_mask: 0.0939  loss_dice: 0.9758
2025/06/23 17:43:55 - mmengine - INFO - Epoch(train)  [3][1440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:31  time: 0.3238  data_time: 0.0032  memory: 2154  grad_norm: 54.0884  loss: 0.7369  loss_cls: 0.1878  loss_mask: 0.0629  loss_dice: 0.4862
2025/06/23 17:43:58 - mmengine - INFO - Epoch(train)  [3][1450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:27  time: 0.3264  data_time: 0.0019  memory: 2571  grad_norm: 55.8915  loss: 1.3001  loss_cls: 0.2974  loss_mask: 0.0852  loss_dice: 0.9175
2025/06/23 17:44:01 - mmengine - INFO - Epoch(train)  [3][1460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:24  time: 0.3270  data_time: 0.0016  memory: 2293  grad_norm: 43.8720  loss: 1.2569  loss_cls: 0.3620  loss_mask: 0.1034  loss_dice: 0.7915
2025/06/23 17:44:02 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:44:05 - mmengine - INFO - Epoch(train)  [3][1470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:21  time: 0.3270  data_time: 0.0067  memory: 2285  grad_norm: 659.1254  loss: 1.6010  loss_cls: 0.4592  loss_mask: 0.1169  loss_dice: 1.0249
2025/06/23 17:44:08 - mmengine - INFO - Epoch(train)  [3][1480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:17  time: 0.3268  data_time: 0.0016  memory: 2300  grad_norm: 61.4237  loss: 1.1509  loss_cls: 0.2594  loss_mask: 0.0982  loss_dice: 0.7933
2025/06/23 17:44:11 - mmengine - INFO - Epoch(train)  [3][1490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:14  time: 0.3256  data_time: 0.0019  memory: 2358  grad_norm: 43.2548  loss: 1.2696  loss_cls: 0.2937  loss_mask: 0.0789  loss_dice: 0.8970
2025/06/23 17:44:14 - mmengine - INFO - Epoch(train)  [3][1500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:10  time: 0.3270  data_time: 0.0041  memory: 2439  grad_norm: 86.0170  loss: 1.4919  loss_cls: 0.4308  loss_mask: 0.1563  loss_dice: 0.9047
2025/06/23 17:44:18 - mmengine - INFO - Epoch(train)  [3][1510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:07  time: 0.3240  data_time: 0.0011  memory: 2197  grad_norm: 52.7082  loss: 1.1020  loss_cls: 0.3556  loss_mask: 0.0642  loss_dice: 0.6822
2025/06/23 17:44:21 - mmengine - INFO - Epoch(train)  [3][1520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:03  time: 0.3240  data_time: 0.0031  memory: 2227  grad_norm: 67.8726  loss: 1.0718  loss_cls: 0.3057  loss_mask: 0.0536  loss_dice: 0.7124
2025/06/23 17:44:24 - mmengine - INFO - Epoch(train)  [3][1530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:38:00  time: 0.3300  data_time: 0.0015  memory: 2519  grad_norm: 74.7478  loss: 1.6312  loss_cls: 0.5092  loss_mask: 0.1005  loss_dice: 1.0215
2025/06/23 17:44:27 - mmengine - INFO - Epoch(train)  [3][1540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:56  time: 0.3274  data_time: 0.0009  memory: 2278  grad_norm: 174.3196  loss: 1.9422  loss_cls: 0.3859  loss_mask: 0.1029  loss_dice: 1.4533
2025/06/23 17:44:31 - mmengine - INFO - Epoch(train)  [3][1550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:53  time: 0.3255  data_time: 0.0022  memory: 2093  grad_norm: 80.0023  loss: 1.1267  loss_cls: 0.3070  loss_mask: 0.2127  loss_dice: 0.6070
2025/06/23 17:44:34 - mmengine - INFO - Epoch(train)  [3][1560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:49  time: 0.3282  data_time: 0.0053  memory: 2387  grad_norm: 54.5453  loss: 1.4979  loss_cls: 0.4590  loss_mask: 0.0943  loss_dice: 0.9446
2025/06/23 17:44:37 - mmengine - INFO - Epoch(train)  [3][1570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:46  time: 0.3265  data_time: 0.0045  memory: 2118  grad_norm: 86.4480  loss: 1.5365  loss_cls: 0.3753  loss_mask: 0.1399  loss_dice: 1.0213
2025/06/23 17:44:41 - mmengine - INFO - Epoch(train)  [3][1580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:43  time: 0.3325  data_time: 0.0061  memory: 2336  grad_norm: 39.2993  loss: 1.4904  loss_cls: 0.4762  loss_mask: 0.1177  loss_dice: 0.8965
2025/06/23 17:44:44 - mmengine - INFO - Epoch(train)  [3][1590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:39  time: 0.3300  data_time: 0.0024  memory: 2478  grad_norm: 61.7218  loss: 1.7999  loss_cls: 0.5427  loss_mask: 0.1246  loss_dice: 1.1327
2025/06/23 17:44:47 - mmengine - INFO - Epoch(train)  [3][1600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:36  time: 0.3326  data_time: 0.0034  memory: 2541  grad_norm: 58.8637  loss: 2.0673  loss_cls: 0.5453  loss_mask: 0.1746  loss_dice: 1.3474
2025/06/23 17:44:50 - mmengine - INFO - Epoch(train)  [3][1610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:32  time: 0.3238  data_time: 0.0023  memory: 2245  grad_norm: 48.3720  loss: 1.2531  loss_cls: 0.4106  loss_mask: 0.0567  loss_dice: 0.7859
2025/06/23 17:44:54 - mmengine - INFO - Epoch(train)  [3][1620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:29  time: 0.3291  data_time: 0.0014  memory: 2358  grad_norm: 144.0593  loss: 1.6147  loss_cls: 0.4732  loss_mask: 0.1072  loss_dice: 1.0344
2025/06/23 17:44:57 - mmengine - INFO - Epoch(train)  [3][1630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:26  time: 0.3292  data_time: 0.0010  memory: 2483  grad_norm: 55.6950  loss: 1.5822  loss_cls: 0.3997  loss_mask: 0.1168  loss_dice: 1.0657
2025/06/23 17:45:00 - mmengine - INFO - Epoch(train)  [3][1640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:22  time: 0.3269  data_time: 0.0030  memory: 2300  grad_norm: 35.2064  loss: 1.2327  loss_cls: 0.3357  loss_mask: 0.0904  loss_dice: 0.8066
2025/06/23 17:45:04 - mmengine - INFO - Epoch(train)  [3][1650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:19  time: 0.3253  data_time: 0.0033  memory: 2251  grad_norm: 46.7383  loss: 1.1033  loss_cls: 0.3908  loss_mask: 0.1001  loss_dice: 0.6124
2025/06/23 17:45:07 - mmengine - INFO - Epoch(train)  [3][1660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:15  time: 0.3268  data_time: 0.0046  memory: 2245  grad_norm: 46.4859  loss: 1.6240  loss_cls: 0.4673  loss_mask: 0.1339  loss_dice: 1.0227
2025/06/23 17:45:10 - mmengine - INFO - Epoch(train)  [3][1670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:12  time: 0.3291  data_time: 0.0017  memory: 2308  grad_norm: 175.4594  loss: 1.4172  loss_cls: 0.5139  loss_mask: 0.1190  loss_dice: 0.7844
2025/06/23 17:45:13 - mmengine - INFO - Epoch(train)  [3][1680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:08  time: 0.3247  data_time: 0.0011  memory: 2154  grad_norm: 72.5431  loss: 1.1167  loss_cls: 0.3341  loss_mask: 0.1144  loss_dice: 0.6682
2025/06/23 17:45:17 - mmengine - INFO - Epoch(train)  [3][1690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:05  time: 0.3288  data_time: 0.0016  memory: 2527  grad_norm: 38.9785  loss: 1.5824  loss_cls: 0.4710  loss_mask: 0.1287  loss_dice: 0.9827
2025/06/23 17:45:20 - mmengine - INFO - Epoch(train)  [3][1700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:37:02  time: 0.3281  data_time: 0.0021  memory: 2490  grad_norm: 72.7574  loss: 1.7672  loss_cls: 0.4559  loss_mask: 0.1475  loss_dice: 1.1638
2025/06/23 17:45:23 - mmengine - INFO - Epoch(train)  [3][1710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:58  time: 0.3268  data_time: 0.0021  memory: 2343  grad_norm: 62.8090  loss: 1.2924  loss_cls: 0.3831  loss_mask: 0.0937  loss_dice: 0.8155
2025/06/23 17:45:26 - mmengine - INFO - Epoch(train)  [3][1720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:55  time: 0.3265  data_time: 0.0039  memory: 2215  grad_norm: 122.9990  loss: 1.5384  loss_cls: 0.5462  loss_mask: 0.1411  loss_dice: 0.8512
2025/06/23 17:45:30 - mmengine - INFO - Epoch(train)  [3][1730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:51  time: 0.3261  data_time: 0.0032  memory: 2124  grad_norm: 46.1029  loss: 1.4867  loss_cls: 0.2631  loss_mask: 0.1128  loss_dice: 1.1108
2025/06/23 17:45:33 - mmengine - INFO - Epoch(train)  [3][1740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:48  time: 0.3240  data_time: 0.0036  memory: 2251  grad_norm: 44.2774  loss: 1.0311  loss_cls: 0.3386  loss_mask: 0.0589  loss_dice: 0.6336
2025/06/23 17:45:36 - mmengine - INFO - Epoch(train)  [3][1750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:44  time: 0.3291  data_time: 0.0050  memory: 2336  grad_norm: 59.2848  loss: 1.9703  loss_cls: 0.5384  loss_mask: 0.1907  loss_dice: 1.2412
2025/06/23 17:45:40 - mmengine - INFO - Epoch(train)  [3][1760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:41  time: 0.3284  data_time: 0.0015  memory: 2293  grad_norm: 89.1097  loss: 1.8688  loss_cls: 0.5236  loss_mask: 0.1243  loss_dice: 1.2209
2025/06/23 17:45:43 - mmengine - INFO - Epoch(train)  [3][1770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:38  time: 0.3268  data_time: 0.0039  memory: 2478  grad_norm: 84.8242  loss: 1.3610  loss_cls: 0.4639  loss_mask: 0.0951  loss_dice: 0.8020
2025/06/23 17:45:46 - mmengine - INFO - Epoch(train)  [3][1780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:34  time: 0.3238  data_time: 0.0029  memory: 2142  grad_norm: 88.0642  loss: 1.1501  loss_cls: 0.3354  loss_mask: 0.1828  loss_dice: 0.6319
2025/06/23 17:45:49 - mmengine - INFO - Epoch(train)  [3][1790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:31  time: 0.3299  data_time: 0.0033  memory: 2271  grad_norm: 62.1142  loss: 1.4267  loss_cls: 0.3739  loss_mask: 0.1618  loss_dice: 0.8910
2025/06/23 17:45:53 - mmengine - INFO - Epoch(train)  [3][1800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:27  time: 0.3273  data_time: 0.0027  memory: 2343  grad_norm: 39.9112  loss: 1.2849  loss_cls: 0.4019  loss_mask: 0.0907  loss_dice: 0.7923
2025/06/23 17:45:56 - mmengine - INFO - Epoch(train)  [3][1810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:24  time: 0.3261  data_time: 0.0016  memory: 2308  grad_norm: 96.7120  loss: 1.0240  loss_cls: 0.2523  loss_mask: 0.0801  loss_dice: 0.6916
2025/06/23 17:45:59 - mmengine - INFO - Epoch(train)  [3][1820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:20  time: 0.3266  data_time: 0.0045  memory: 2343  grad_norm: 75.1176  loss: 1.4389  loss_cls: 0.4197  loss_mask: 0.1310  loss_dice: 0.8883
2025/06/23 17:46:02 - mmengine - INFO - Epoch(train)  [3][1830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:17  time: 0.3304  data_time: 0.0052  memory: 2505  grad_norm: 56.2162  loss: 1.5720  loss_cls: 0.4191  loss_mask: 0.1116  loss_dice: 1.0413
2025/06/23 17:46:06 - mmengine - INFO - Epoch(train)  [3][1840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:14  time: 0.3257  data_time: 0.0029  memory: 2221  grad_norm: 26.1531  loss: 1.0382  loss_cls: 0.2881  loss_mask: 0.0640  loss_dice: 0.6862
2025/06/23 17:46:09 - mmengine - INFO - Epoch(train)  [3][1850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:10  time: 0.3216  data_time: 0.0032  memory: 2051  grad_norm: 226.2098  loss: 1.5381  loss_cls: 0.2209  loss_mask: 0.1795  loss_dice: 1.1378
2025/06/23 17:46:12 - mmengine - INFO - Epoch(train)  [3][1860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:06  time: 0.3267  data_time: 0.0061  memory: 2483  grad_norm: 78.8720  loss: 1.6237  loss_cls: 0.4220  loss_mask: 0.1123  loss_dice: 1.0893
2025/06/23 17:46:15 - mmengine - INFO - Epoch(train)  [3][1870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:36:03  time: 0.3251  data_time: 0.0049  memory: 2251  grad_norm: 35.8648  loss: 1.2325  loss_cls: 0.3437  loss_mask: 0.0964  loss_dice: 0.7924
2025/06/23 17:46:19 - mmengine - INFO - Epoch(train)  [3][1880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:59  time: 0.3251  data_time: 0.0015  memory: 2209  grad_norm: 45.6576  loss: 1.1939  loss_cls: 0.3444  loss_mask: 0.1125  loss_dice: 0.7369
2025/06/23 17:46:22 - mmengine - INFO - Epoch(train)  [3][1890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:56  time: 0.3305  data_time: 0.0017  memory: 2380  grad_norm: 41.1628  loss: 1.4377  loss_cls: 0.3112  loss_mask: 0.1139  loss_dice: 1.0126
2025/06/23 17:46:25 - mmengine - INFO - Epoch(train)  [3][1900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:53  time: 0.3276  data_time: 0.0023  memory: 2351  grad_norm: 57.1701  loss: 1.7086  loss_cls: 0.5027  loss_mask: 0.1227  loss_dice: 1.0832
2025/06/23 17:46:29 - mmengine - INFO - Epoch(train)  [3][1910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:49  time: 0.3254  data_time: 0.0020  memory: 2446  grad_norm: 63.5119  loss: 1.3738  loss_cls: 0.4240  loss_mask: 0.0832  loss_dice: 0.8665
2025/06/23 17:46:32 - mmengine - INFO - Epoch(train)  [3][1920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:46  time: 0.3271  data_time: 0.0076  memory: 2365  grad_norm: 67.4758  loss: 1.1093  loss_cls: 0.4350  loss_mask: 0.0931  loss_dice: 0.5812
2025/06/23 17:46:35 - mmengine - INFO - Epoch(train)  [3][1930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:42  time: 0.3279  data_time: 0.0056  memory: 2358  grad_norm: 75.3549  loss: 1.8297  loss_cls: 0.4993  loss_mask: 0.1876  loss_dice: 1.1428
2025/06/23 17:46:38 - mmengine - INFO - Epoch(train)  [3][1940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:39  time: 0.3242  data_time: 0.0039  memory: 2251  grad_norm: 105.6741  loss: 1.2316  loss_cls: 0.3341  loss_mask: 0.1579  loss_dice: 0.7396
2025/06/23 17:46:42 - mmengine - INFO - Epoch(train)  [3][1950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:36  time: 0.3288  data_time: 0.0034  memory: 2454  grad_norm: 52.8186  loss: 1.9347  loss_cls: 0.4579  loss_mask: 0.2393  loss_dice: 1.2375
2025/06/23 17:46:45 - mmengine - INFO - Epoch(train)  [3][1960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:32  time: 0.3308  data_time: 0.0053  memory: 2498  grad_norm: 53.4719  loss: 1.8987  loss_cls: 0.5118  loss_mask: 0.1644  loss_dice: 1.2225
2025/06/23 17:46:48 - mmengine - INFO - Epoch(train)  [3][1970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:29  time: 0.3269  data_time: 0.0034  memory: 2372  grad_norm: 41.5893  loss: 1.7197  loss_cls: 0.4731  loss_mask: 0.1197  loss_dice: 1.1269
2025/06/23 17:46:51 - mmengine - INFO - Epoch(train)  [3][1980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:25  time: 0.3263  data_time: 0.0005  memory: 2136  grad_norm: 49.6876  loss: 1.2013  loss_cls: 0.2544  loss_mask: 0.1147  loss_dice: 0.8322
2025/06/23 17:46:55 - mmengine - INFO - Epoch(train)  [3][1990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:22  time: 0.3249  data_time: 0.0022  memory: 2142  grad_norm: 31.7191  loss: 1.2571  loss_cls: 0.2672  loss_mask: 0.1179  loss_dice: 0.8721
2025/06/23 17:46:58 - mmengine - INFO - Epoch(train)  [3][2000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:18  time: 0.3211  data_time: 0.0041  memory: 2058  grad_norm: 76.3961  loss: 0.7383  loss_cls: 0.2847  loss_mask: 0.0614  loss_dice: 0.3921
2025/06/23 17:47:01 - mmengine - INFO - Epoch(train)  [3][2010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:15  time: 0.3309  data_time: 0.0024  memory: 2372  grad_norm: 57.1905  loss: 1.7786  loss_cls: 0.4900  loss_mask: 0.1626  loss_dice: 1.1260
2025/06/23 17:47:05 - mmengine - INFO - Epoch(train)  [3][2020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:11  time: 0.3271  data_time: 0.0012  memory: 2233  grad_norm: 60.5263  loss: 1.6989  loss_cls: 0.4590  loss_mask: 0.1389  loss_dice: 1.1011
2025/06/23 17:47:08 - mmengine - INFO - Epoch(train)  [3][2030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:08  time: 0.3273  data_time: 0.0026  memory: 2387  grad_norm: 76.1249  loss: 1.2272  loss_cls: 0.2658  loss_mask: 0.1480  loss_dice: 0.8134
2025/06/23 17:47:11 - mmengine - INFO - Epoch(train)  [3][2040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:05  time: 0.3267  data_time: 0.0007  memory: 2314  grad_norm: 65.1603  loss: 1.6223  loss_cls: 0.5180  loss_mask: 0.1926  loss_dice: 0.9117
2025/06/23 17:47:14 - mmengine - INFO - Epoch(train)  [3][2050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:35:01  time: 0.3239  data_time: 0.0027  memory: 2215  grad_norm: 22.5840  loss: 0.7210  loss_cls: 0.1694  loss_mask: 0.0615  loss_dice: 0.4900
2025/06/23 17:47:18 - mmengine - INFO - Epoch(train)  [3][2060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:58  time: 0.3257  data_time: 0.0005  memory: 2417  grad_norm: 65.2777  loss: 1.5299  loss_cls: 0.4160  loss_mask: 0.1131  loss_dice: 1.0007
2025/06/23 17:47:21 - mmengine - INFO - Epoch(train)  [3][2070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:54  time: 0.3280  data_time: 0.0055  memory: 2454  grad_norm: 69.2185  loss: 1.5085  loss_cls: 0.4550  loss_mask: 0.1472  loss_dice: 0.9062
2025/06/23 17:47:24 - mmengine - INFO - Epoch(train)  [3][2080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:51  time: 0.3234  data_time: 0.0053  memory: 2209  grad_norm: 74.5017  loss: 1.4196  loss_cls: 0.4803  loss_mask: 0.1222  loss_dice: 0.8171
2025/06/23 17:47:27 - mmengine - INFO - Epoch(train)  [3][2090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:47  time: 0.3268  data_time: 0.0031  memory: 2343  grad_norm: 106.3006  loss: 1.8305  loss_cls: 0.4462  loss_mask: 0.1341  loss_dice: 1.2501
2025/06/23 17:47:31 - mmengine - INFO - Epoch(train)  [3][2100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:44  time: 0.3261  data_time: 0.0045  memory: 2191  grad_norm: 49.7674  loss: 1.8247  loss_cls: 0.5490  loss_mask: 0.1148  loss_dice: 1.1609
2025/06/23 17:47:34 - mmengine - INFO - Epoch(train)  [3][2110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:40  time: 0.3289  data_time: 0.0039  memory: 2343  grad_norm: 57.6974  loss: 1.5237  loss_cls: 0.4581  loss_mask: 0.1359  loss_dice: 0.9297
2025/06/23 17:47:37 - mmengine - INFO - Epoch(train)  [3][2120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:37  time: 0.3274  data_time: 0.0007  memory: 2166  grad_norm: 88.0693  loss: 2.2747  loss_cls: 0.5289  loss_mask: 0.1744  loss_dice: 1.5714
2025/06/23 17:47:40 - mmengine - INFO - Epoch(train)  [3][2130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:34  time: 0.3244  data_time: 0.0020  memory: 2191  grad_norm: 69.5702  loss: 1.5391  loss_cls: 0.3489  loss_mask: 0.1927  loss_dice: 0.9975
2025/06/23 17:47:44 - mmengine - INFO - Epoch(train)  [3][2140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:30  time: 0.3260  data_time: 0.0018  memory: 2519  grad_norm: 153.3909  loss: 1.4570  loss_cls: 0.3580  loss_mask: 0.1197  loss_dice: 0.9793
2025/06/23 17:47:47 - mmengine - INFO - Epoch(train)  [3][2150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:27  time: 0.3245  data_time: 0.0036  memory: 2454  grad_norm: 93.3813  loss: 1.1584  loss_cls: 0.2876  loss_mask: 0.1412  loss_dice: 0.7296
2025/06/23 17:47:50 - mmengine - INFO - Epoch(train)  [3][2160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:23  time: 0.3281  data_time: 0.0032  memory: 2446  grad_norm: 46.3023  loss: 1.6951  loss_cls: 0.4801  loss_mask: 0.1227  loss_dice: 1.0923
2025/06/23 17:47:53 - mmengine - INFO - Epoch(train)  [3][2170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:20  time: 0.3283  data_time: 0.0042  memory: 2432  grad_norm: 51.5491  loss: 1.2602  loss_cls: 0.3906  loss_mask: 0.1649  loss_dice: 0.7047
2025/06/23 17:47:57 - mmengine - INFO - Epoch(train)  [3][2180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:16  time: 0.3268  data_time: 0.0024  memory: 2454  grad_norm: 58.3586  loss: 1.5228  loss_cls: 0.4251  loss_mask: 0.0966  loss_dice: 1.0010
2025/06/23 17:48:00 - mmengine - INFO - Epoch(train)  [3][2190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:13  time: 0.3274  data_time: 0.0029  memory: 2505  grad_norm: 286.4617  loss: 1.9304  loss_cls: 0.3340  loss_mask: 0.6664  loss_dice: 0.9300
2025/06/23 17:48:03 - mmengine - INFO - Epoch(train)  [3][2200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:10  time: 0.3261  data_time: 0.0029  memory: 2209  grad_norm: 72.8138  loss: 1.1386  loss_cls: 0.2684  loss_mask: 0.0897  loss_dice: 0.7805
2025/06/23 17:48:07 - mmengine - INFO - Epoch(train)  [3][2210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:06  time: 0.3263  data_time: 0.0058  memory: 2271  grad_norm: 562.0055  loss: 1.7126  loss_cls: 0.5710  loss_mask: 0.1035  loss_dice: 1.0381
2025/06/23 17:48:10 - mmengine - INFO - Epoch(train)  [3][2220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:34:03  time: 0.3260  data_time: 0.0042  memory: 2271  grad_norm: 50.0880  loss: 1.5527  loss_cls: 0.4308  loss_mask: 0.1413  loss_dice: 0.9805
2025/06/23 17:48:13 - mmengine - INFO - Epoch(train)  [3][2230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:59  time: 0.3283  data_time: 0.0055  memory: 2336  grad_norm: 82.0764  loss: 1.4111  loss_cls: 0.4082  loss_mask: 0.1530  loss_dice: 0.8500
2025/06/23 17:48:16 - mmengine - INFO - Epoch(train)  [3][2240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:56  time: 0.3259  data_time: 0.0058  memory: 2209  grad_norm: 96.0000  loss: 1.5520  loss_cls: 0.5645  loss_mask: 0.1202  loss_dice: 0.8673
2025/06/23 17:48:20 - mmengine - INFO - Epoch(train)  [3][2250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:52  time: 0.3252  data_time: 0.0039  memory: 2314  grad_norm: 62.3012  loss: 2.2273  loss_cls: 0.5125  loss_mask: 0.3451  loss_dice: 1.3697
2025/06/23 17:48:23 - mmengine - INFO - Epoch(train)  [3][2260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:49  time: 0.3246  data_time: 0.0013  memory: 2124  grad_norm: 85.0599  loss: 1.3828  loss_cls: 0.3311  loss_mask: 0.1620  loss_dice: 0.8897
2025/06/23 17:48:26 - mmengine - INFO - Epoch(train)  [3][2270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:45  time: 0.3251  data_time: 0.0038  memory: 2402  grad_norm: 46.8537  loss: 1.2530  loss_cls: 0.3041  loss_mask: 0.0589  loss_dice: 0.8900
2025/06/23 17:48:29 - mmengine - INFO - Epoch(train)  [3][2280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:42  time: 0.3349  data_time: 0.0085  memory: 2483  grad_norm: 95.7624  loss: 1.8124  loss_cls: 0.5594  loss_mask: 0.1601  loss_dice: 1.0929
2025/06/23 17:48:33 - mmengine - INFO - Epoch(train)  [3][2290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:39  time: 0.3339  data_time: 0.0014  memory: 2505  grad_norm: 70.0513  loss: 2.0339  loss_cls: 0.6647  loss_mask: 0.1187  loss_dice: 1.2505
2025/06/23 17:48:36 - mmengine - INFO - Epoch(train)  [3][2300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:36  time: 0.3260  data_time: 0.0019  memory: 2251  grad_norm: 48.1738  loss: 1.4621  loss_cls: 0.3588  loss_mask: 0.1083  loss_dice: 0.9950
2025/06/23 17:48:39 - mmengine - INFO - Epoch(train)  [3][2310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:32  time: 0.3271  data_time: 0.0023  memory: 2505  grad_norm: 39.3189  loss: 1.4060  loss_cls: 0.3945  loss_mask: 0.0909  loss_dice: 0.9205
2025/06/23 17:48:43 - mmengine - INFO - Epoch(train)  [3][2320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:29  time: 0.3246  data_time: 0.0046  memory: 2221  grad_norm: 46.4899  loss: 1.2699  loss_cls: 0.3702  loss_mask: 0.1543  loss_dice: 0.7454
2025/06/23 17:48:46 - mmengine - INFO - Epoch(train)  [3][2330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:25  time: 0.3271  data_time: 0.0025  memory: 2239  grad_norm: 130.6040  loss: 1.5182  loss_cls: 0.4899  loss_mask: 0.1070  loss_dice: 0.9212
2025/06/23 17:48:49 - mmengine - INFO - Epoch(train)  [3][2340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:22  time: 0.3297  data_time: 0.0025  memory: 2321  grad_norm: 250.5085  loss: 1.6742  loss_cls: 0.4926  loss_mask: 0.1255  loss_dice: 1.0561
2025/06/23 17:48:52 - mmengine - INFO - Epoch(train)  [3][2350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:18  time: 0.3252  data_time: 0.0005  memory: 2541  grad_norm: 48.7949  loss: 1.2246  loss_cls: 0.3135  loss_mask: 0.1055  loss_dice: 0.8055
2025/06/23 17:48:56 - mmengine - INFO - Epoch(train)  [3][2360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:15  time: 0.3243  data_time: 0.0034  memory: 2215  grad_norm: 262.6797  loss: 1.8300  loss_cls: 0.4599  loss_mask: 0.3632  loss_dice: 1.0070
2025/06/23 17:48:59 - mmengine - INFO - Epoch(train)  [3][2370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:11  time: 0.3250  data_time: 0.0027  memory: 2321  grad_norm: 86.8560  loss: 1.6895  loss_cls: 0.4593  loss_mask: 0.1311  loss_dice: 1.0991
2025/06/23 17:49:02 - mmengine - INFO - Epoch(train)  [3][2380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:08  time: 0.3319  data_time: 0.0052  memory: 2483  grad_norm: 60.9725  loss: 1.6396  loss_cls: 0.4021  loss_mask: 0.1199  loss_dice: 1.1176
2025/06/23 17:49:05 - mmengine - INFO - Epoch(train)  [3][2390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:05  time: 0.3275  data_time: 0.0020  memory: 2351  grad_norm: 126.9504  loss: 1.4473  loss_cls: 0.3933  loss_mask: 0.1342  loss_dice: 0.9197
2025/06/23 17:49:09 - mmengine - INFO - Epoch(train)  [3][2400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:33:01  time: 0.3274  data_time: 0.0023  memory: 2245  grad_norm: 57.3761  loss: 1.4499  loss_cls: 0.4725  loss_mask: 0.1170  loss_dice: 0.8604
2025/06/23 17:49:12 - mmengine - INFO - Epoch(train)  [3][2410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:58  time: 0.3313  data_time: 0.0023  memory: 2278  grad_norm: 86.8096  loss: 0.9394  loss_cls: 0.2826  loss_mask: 0.0454  loss_dice: 0.6114
2025/06/23 17:49:15 - mmengine - INFO - Epoch(train)  [3][2420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:55  time: 0.3254  data_time: 0.0013  memory: 2257  grad_norm: 41.3085  loss: 1.1788  loss_cls: 0.3706  loss_mask: 0.0937  loss_dice: 0.7146
2025/06/23 17:49:19 - mmengine - INFO - Epoch(train)  [3][2430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:51  time: 0.3302  data_time: 0.0016  memory: 2257  grad_norm: 50.8762  loss: 1.4539  loss_cls: 0.4158  loss_mask: 0.0781  loss_dice: 0.9601
2025/06/23 17:49:22 - mmengine - INFO - Epoch(train)  [3][2440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:48  time: 0.3318  data_time: 0.0046  memory: 2519  grad_norm: 35.3312  loss: 0.9835  loss_cls: 0.2653  loss_mask: 0.0636  loss_dice: 0.6547
2025/06/23 17:49:25 - mmengine - INFO - Epoch(train)  [3][2450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:45  time: 0.3323  data_time: 0.0052  memory: 2483  grad_norm: 147.2421  loss: 1.6510  loss_cls: 0.5020  loss_mask: 0.1237  loss_dice: 1.0253
2025/06/23 17:49:29 - mmengine - INFO - Epoch(train)  [3][2460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:41  time: 0.3272  data_time: 0.0012  memory: 2490  grad_norm: 61.8722  loss: 1.4848  loss_cls: 0.4313  loss_mask: 0.1219  loss_dice: 0.9317
2025/06/23 17:49:29 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:49:32 - mmengine - INFO - Epoch(train)  [3][2470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:38  time: 0.3289  data_time: 0.0019  memory: 2069  grad_norm: 113.4668  loss: 1.3380  loss_cls: 0.3067  loss_mask: 0.1344  loss_dice: 0.8969
2025/06/23 17:49:35 - mmengine - INFO - Epoch(train)  [3][2480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:35  time: 0.3320  data_time: 0.0031  memory: 2483  grad_norm: 88.3098  loss: 1.3794  loss_cls: 0.3083  loss_mask: 0.1611  loss_dice: 0.9100
2025/06/23 17:49:38 - mmengine - INFO - Epoch(train)  [3][2490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:31  time: 0.3326  data_time: 0.0023  memory: 2184  grad_norm: 207.9945  loss: 1.5285  loss_cls: 0.4412  loss_mask: 0.2028  loss_dice: 0.8845
2025/06/23 17:49:42 - mmengine - INFO - Epoch(train)  [3][2500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:28  time: 0.3384  data_time: 0.0034  memory: 2505  grad_norm: 46.1812  loss: 0.9431  loss_cls: 0.2623  loss_mask: 0.0766  loss_dice: 0.6042
2025/06/23 17:49:45 - mmengine - INFO - Epoch(train)  [3][2510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:25  time: 0.3415  data_time: 0.0052  memory: 2172  grad_norm: 92.4880  loss: 1.4830  loss_cls: 0.3947  loss_mask: 0.1143  loss_dice: 0.9740
2025/06/23 17:49:49 - mmengine - INFO - Epoch(train)  [3][2520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:22  time: 0.3354  data_time: 0.0032  memory: 2402  grad_norm: 55.1418  loss: 1.5902  loss_cls: 0.5033  loss_mask: 0.0889  loss_dice: 0.9980
2025/06/23 17:49:52 - mmengine - INFO - Epoch(train)  [3][2530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:19  time: 0.3320  data_time: 0.0028  memory: 2271  grad_norm: 198.0575  loss: 1.4061  loss_cls: 0.4460  loss_mask: 0.0919  loss_dice: 0.8682
2025/06/23 17:49:55 - mmengine - INFO - Epoch(train)  [3][2540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:16  time: 0.3364  data_time: 0.0029  memory: 2142  grad_norm: 64.4758  loss: 1.3272  loss_cls: 0.4616  loss_mask: 0.1044  loss_dice: 0.7612
2025/06/23 17:49:59 - mmengine - INFO - Epoch(train)  [3][2550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:12  time: 0.3298  data_time: 0.0020  memory: 2093  grad_norm: 32.7062  loss: 0.9675  loss_cls: 0.2483  loss_mask: 0.0693  loss_dice: 0.6499
2025/06/23 17:50:02 - mmengine - INFO - Epoch(train)  [3][2560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:09  time: 0.3325  data_time: 0.0027  memory: 2351  grad_norm: 37.2560  loss: 1.4631  loss_cls: 0.3779  loss_mask: 0.1109  loss_dice: 0.9742
2025/06/23 17:50:05 - mmengine - INFO - Epoch(train)  [3][2570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:06  time: 0.3317  data_time: 0.0029  memory: 2178  grad_norm: 88.5615  loss: 1.3129  loss_cls: 0.3213  loss_mask: 0.0920  loss_dice: 0.8996
2025/06/23 17:50:08 - mmengine - INFO - Epoch(train)  [3][2580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:32:02  time: 0.3273  data_time: 0.0025  memory: 2178  grad_norm: 80.6286  loss: 1.6440  loss_cls: 0.4142  loss_mask: 0.2648  loss_dice: 0.9650
2025/06/23 17:50:12 - mmengine - INFO - Epoch(train)  [3][2590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:59  time: 0.3352  data_time: 0.0019  memory: 2372  grad_norm: 98.0165  loss: 1.5886  loss_cls: 0.4329  loss_mask: 0.0955  loss_dice: 1.0602
2025/06/23 17:50:15 - mmengine - INFO - Epoch(train)  [3][2600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:56  time: 0.3336  data_time: 0.0037  memory: 2343  grad_norm: 46.9465  loss: 1.1723  loss_cls: 0.4188  loss_mask: 0.0830  loss_dice: 0.6705
2025/06/23 17:50:18 - mmengine - INFO - Epoch(train)  [3][2610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:53  time: 0.3301  data_time: 0.0044  memory: 2191  grad_norm: 75.2304  loss: 0.8169  loss_cls: 0.2684  loss_mask: 0.0604  loss_dice: 0.4881
2025/06/23 17:50:22 - mmengine - INFO - Epoch(train)  [3][2620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:49  time: 0.3308  data_time: 0.0025  memory: 2087  grad_norm: 53.0643  loss: 1.6315  loss_cls: 0.4504  loss_mask: 0.1118  loss_dice: 1.0693
2025/06/23 17:50:25 - mmengine - INFO - Epoch(train)  [3][2630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:46  time: 0.3246  data_time: 0.0023  memory: 2245  grad_norm: 74.1238  loss: 0.8958  loss_cls: 0.2678  loss_mask: 0.0451  loss_dice: 0.5829
2025/06/23 17:50:28 - mmengine - INFO - Epoch(train)  [3][2640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:42  time: 0.3281  data_time: 0.0012  memory: 2118  grad_norm: 45.4214  loss: 1.0833  loss_cls: 0.3175  loss_mask: 0.1099  loss_dice: 0.6559
2025/06/23 17:50:32 - mmengine - INFO - Epoch(train)  [3][2650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:39  time: 0.3282  data_time: 0.0014  memory: 2271  grad_norm: 93.1486  loss: 1.1833  loss_cls: 0.3387  loss_mask: 0.1074  loss_dice: 0.7372
2025/06/23 17:50:35 - mmengine - INFO - Epoch(train)  [3][2660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:36  time: 0.3352  data_time: 0.0023  memory: 2425  grad_norm: 56.6597  loss: 1.6367  loss_cls: 0.4862  loss_mask: 0.2199  loss_dice: 0.9306
2025/06/23 17:50:38 - mmengine - INFO - Epoch(train)  [3][2670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:33  time: 0.3300  data_time: 0.0028  memory: 2221  grad_norm: 97.7251  loss: 1.7113  loss_cls: 0.5893  loss_mask: 0.1544  loss_dice: 0.9676
2025/06/23 17:50:42 - mmengine - INFO - Epoch(train)  [3][2680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:29  time: 0.3322  data_time: 0.0031  memory: 2184  grad_norm: 129.3699  loss: 0.8317  loss_cls: 0.2289  loss_mask: 0.0556  loss_dice: 0.5472
2025/06/23 17:50:45 - mmengine - INFO - Epoch(train)  [3][2690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:26  time: 0.3303  data_time: 0.0012  memory: 2417  grad_norm: 268.5615  loss: 1.2394  loss_cls: 0.3412  loss_mask: 0.1009  loss_dice: 0.7973
2025/06/23 17:50:48 - mmengine - INFO - Epoch(train)  [3][2700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:23  time: 0.3350  data_time: 0.0038  memory: 2468  grad_norm: 93.7590  loss: 1.3063  loss_cls: 0.3779  loss_mask: 0.1167  loss_dice: 0.8117
2025/06/23 17:50:52 - mmengine - INFO - Epoch(train)  [3][2710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:19  time: 0.3295  data_time: 0.0017  memory: 2118  grad_norm: 22.4762  loss: 0.7532  loss_cls: 0.2480  loss_mask: 0.0650  loss_dice: 0.4401
2025/06/23 17:50:55 - mmengine - INFO - Epoch(train)  [3][2720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:16  time: 0.3361  data_time: 0.0023  memory: 2358  grad_norm: 63.3113  loss: 1.8918  loss_cls: 0.6837  loss_mask: 0.1065  loss_dice: 1.1016
2025/06/23 17:50:58 - mmengine - INFO - Epoch(train)  [3][2730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:13  time: 0.3334  data_time: 0.0027  memory: 2278  grad_norm: 74.0224  loss: 1.2332  loss_cls: 0.3592  loss_mask: 0.1029  loss_dice: 0.7710
2025/06/23 17:51:02 - mmengine - INFO - Epoch(train)  [3][2740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:10  time: 0.3347  data_time: 0.0011  memory: 2285  grad_norm: 148.7915  loss: 1.6155  loss_cls: 0.5080  loss_mask: 0.2592  loss_dice: 0.8483
2025/06/23 17:51:05 - mmengine - INFO - Epoch(train)  [3][2750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:06  time: 0.3242  data_time: 0.0020  memory: 2278  grad_norm: 17.5567  loss: 0.5172  loss_cls: 0.1647  loss_mask: 0.0485  loss_dice: 0.3040
2025/06/23 17:51:08 - mmengine - INFO - Epoch(train)  [3][2760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:03  time: 0.3326  data_time: 0.0023  memory: 2329  grad_norm: 292.0006  loss: 1.4421  loss_cls: 0.3584  loss_mask: 0.2243  loss_dice: 0.8594
2025/06/23 17:51:11 - mmengine - INFO - Epoch(train)  [3][2770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:31:00  time: 0.3263  data_time: 0.0038  memory: 2191  grad_norm: 61.7688  loss: 0.9781  loss_cls: 0.3888  loss_mask: 0.0675  loss_dice: 0.5219
2025/06/23 17:51:15 - mmengine - INFO - Epoch(train)  [3][2780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:56  time: 0.3370  data_time: 0.0019  memory: 2380  grad_norm: 92.8587  loss: 1.3048  loss_cls: 0.4530  loss_mask: 0.0815  loss_dice: 0.7703
2025/06/23 17:51:18 - mmengine - INFO - Epoch(train)  [3][2790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:53  time: 0.3397  data_time: 0.0021  memory: 2257  grad_norm: 71.2210  loss: 1.5870  loss_cls: 0.6449  loss_mask: 0.1124  loss_dice: 0.8298
2025/06/23 17:51:22 - mmengine - INFO - Epoch(train)  [3][2800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:50  time: 0.3332  data_time: 0.0054  memory: 2483  grad_norm: 71.1687  loss: 1.6148  loss_cls: 0.5201  loss_mask: 0.1778  loss_dice: 0.9170
2025/06/23 17:51:25 - mmengine - INFO - Epoch(train)  [3][2810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:47  time: 0.3314  data_time: 0.0018  memory: 2278  grad_norm: 75.1619  loss: 1.7560  loss_cls: 0.4840  loss_mask: 0.1614  loss_dice: 1.1105
2025/06/23 17:51:28 - mmengine - INFO - Epoch(train)  [3][2820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:43  time: 0.3296  data_time: 0.0043  memory: 2197  grad_norm: 50.5944  loss: 1.2960  loss_cls: 0.3414  loss_mask: 0.1280  loss_dice: 0.8265
2025/06/23 17:51:31 - mmengine - INFO - Epoch(train)  [3][2830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:40  time: 0.3291  data_time: 0.0022  memory: 2239  grad_norm: 137.3834  loss: 1.3395  loss_cls: 0.4544  loss_mask: 0.0717  loss_dice: 0.8134
2025/06/23 17:51:35 - mmengine - INFO - Epoch(train)  [3][2840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:37  time: 0.3278  data_time: 0.0012  memory: 2321  grad_norm: 116.1540  loss: 1.6141  loss_cls: 0.4749  loss_mask: 0.2096  loss_dice: 0.9296
2025/06/23 17:51:38 - mmengine - INFO - Epoch(train)  [3][2850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:33  time: 0.3324  data_time: 0.0021  memory: 2432  grad_norm: 106.3895  loss: 1.2361  loss_cls: 0.3373  loss_mask: 0.1275  loss_dice: 0.7713
2025/06/23 17:51:41 - mmengine - INFO - Epoch(train)  [3][2860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:30  time: 0.3299  data_time: 0.0027  memory: 2432  grad_norm: 34.8553  loss: 1.3475  loss_cls: 0.4249  loss_mask: 0.1138  loss_dice: 0.8087
2025/06/23 17:51:45 - mmengine - INFO - Epoch(train)  [3][2870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:27  time: 0.3363  data_time: 0.0042  memory: 2439  grad_norm: 52.6529  loss: 0.8525  loss_cls: 0.2641  loss_mask: 0.0581  loss_dice: 0.5303
2025/06/23 17:51:48 - mmengine - INFO - Epoch(train)  [3][2880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:24  time: 0.3327  data_time: 0.0029  memory: 2278  grad_norm: 41.5272  loss: 0.8372  loss_cls: 0.2979  loss_mask: 0.0598  loss_dice: 0.4796
2025/06/23 17:51:51 - mmengine - INFO - Epoch(train)  [3][2890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:21  time: 0.3385  data_time: 0.0015  memory: 2409  grad_norm: 37.0151  loss: 1.3289  loss_cls: 0.4642  loss_mask: 0.0871  loss_dice: 0.7776
2025/06/23 17:51:55 - mmengine - INFO - Epoch(train)  [3][2900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:18  time: 0.3405  data_time: 0.0042  memory: 2197  grad_norm: 30.9704  loss: 1.4115  loss_cls: 0.4789  loss_mask: 0.1005  loss_dice: 0.8320
2025/06/23 17:51:58 - mmengine - INFO - Epoch(train)  [3][2910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:14  time: 0.3327  data_time: 0.0022  memory: 2257  grad_norm: 26.2279  loss: 1.2996  loss_cls: 0.5046  loss_mask: 0.1070  loss_dice: 0.6880
2025/06/23 17:52:01 - mmengine - INFO - Epoch(train)  [3][2920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:11  time: 0.3321  data_time: 0.0022  memory: 2358  grad_norm: 85.7444  loss: 1.4136  loss_cls: 0.4270  loss_mask: 0.1023  loss_dice: 0.8843
2025/06/23 17:52:05 - mmengine - INFO - Epoch(train)  [3][2930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:07  time: 0.3236  data_time: 0.0022  memory: 2184  grad_norm: 52.5403  loss: 1.0750  loss_cls: 0.2802  loss_mask: 0.1331  loss_dice: 0.6618
2025/06/23 17:52:08 - mmengine - INFO - Epoch(train)  [3][2940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:04  time: 0.3301  data_time: 0.0044  memory: 2343  grad_norm: 48.9764  loss: 1.7588  loss_cls: 0.5821  loss_mask: 0.1148  loss_dice: 1.0620
2025/06/23 17:52:11 - mmengine - INFO - Epoch(train)  [3][2950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:30:01  time: 0.3236  data_time: 0.0011  memory: 2425  grad_norm: 25.1613  loss: 0.8676  loss_cls: 0.1995  loss_mask: 0.1160  loss_dice: 0.5521
2025/06/23 17:52:15 - mmengine - INFO - Epoch(train)  [3][2960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:58  time: 0.3402  data_time: 0.0054  memory: 2563  grad_norm: 63.9451  loss: 1.7361  loss_cls: 0.4656  loss_mask: 0.1521  loss_dice: 1.1183
2025/06/23 17:52:18 - mmengine - INFO - Epoch(train)  [3][2970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:54  time: 0.3376  data_time: 0.0012  memory: 2278  grad_norm: 40.2684  loss: 1.1417  loss_cls: 0.3316  loss_mask: 0.0551  loss_dice: 0.7550
2025/06/23 17:52:21 - mmengine - INFO - Epoch(train)  [3][2980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:51  time: 0.3320  data_time: 0.0036  memory: 2321  grad_norm: 59.4115  loss: 1.4893  loss_cls: 0.5526  loss_mask: 0.0939  loss_dice: 0.8429
2025/06/23 17:52:25 - mmengine - INFO - Epoch(train)  [3][2990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:48  time: 0.3274  data_time: 0.0019  memory: 2300  grad_norm: 45.8648  loss: 1.2753  loss_cls: 0.3993  loss_mask: 0.0784  loss_dice: 0.7976
2025/06/23 17:52:28 - mmengine - INFO - Epoch(train)  [3][3000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:44  time: 0.3249  data_time: 0.0014  memory: 2300  grad_norm: 38.8212  loss: 1.1326  loss_cls: 0.3086  loss_mask: 0.1045  loss_dice: 0.7194
2025/06/23 17:52:31 - mmengine - INFO - Epoch(train)  [3][3010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:41  time: 0.3260  data_time: 0.0031  memory: 2172  grad_norm: 39.7815  loss: 1.2042  loss_cls: 0.2662  loss_mask: 0.1764  loss_dice: 0.7616
2025/06/23 17:52:34 - mmengine - INFO - Epoch(train)  [3][3020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:38  time: 0.3283  data_time: 0.0026  memory: 2184  grad_norm: 48.9826  loss: 1.0493  loss_cls: 0.3322  loss_mask: 0.0884  loss_dice: 0.6286
2025/06/23 17:52:38 - mmengine - INFO - Epoch(train)  [3][3030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:34  time: 0.3297  data_time: 0.0024  memory: 2118  grad_norm: 25.4889  loss: 0.9232  loss_cls: 0.3452  loss_mask: 0.0877  loss_dice: 0.4902
2025/06/23 17:52:41 - mmengine - INFO - Epoch(train)  [3][3040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:31  time: 0.3332  data_time: 0.0029  memory: 2271  grad_norm: 108.8190  loss: 1.6003  loss_cls: 0.5625  loss_mask: 0.1134  loss_dice: 0.9243
2025/06/23 17:52:44 - mmengine - INFO - Epoch(train)  [3][3050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:28  time: 0.3420  data_time: 0.0040  memory: 2380  grad_norm: 32.8734  loss: 1.4261  loss_cls: 0.4144  loss_mask: 0.1098  loss_dice: 0.9019
2025/06/23 17:52:48 - mmengine - INFO - Epoch(train)  [3][3060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:24  time: 0.3269  data_time: 0.0011  memory: 2172  grad_norm: 172.9476  loss: 1.3955  loss_cls: 0.4144  loss_mask: 0.1614  loss_dice: 0.8197
2025/06/23 17:52:51 - mmengine - INFO - Epoch(train)  [3][3070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:21  time: 0.3312  data_time: 0.0032  memory: 2563  grad_norm: 41.8364  loss: 1.7983  loss_cls: 0.4367  loss_mask: 0.2123  loss_dice: 1.1493
2025/06/23 17:52:54 - mmengine - INFO - Epoch(train)  [3][3080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:18  time: 0.3333  data_time: 0.0033  memory: 2417  grad_norm: 60.5161  loss: 1.3321  loss_cls: 0.4436  loss_mask: 0.1040  loss_dice: 0.7846
2025/06/23 17:52:58 - mmengine - INFO - Epoch(train)  [3][3090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:15  time: 0.3272  data_time: 0.0017  memory: 2209  grad_norm: 79.3947  loss: 1.1624  loss_cls: 0.3845  loss_mask: 0.1020  loss_dice: 0.6759
2025/06/23 17:53:01 - mmengine - INFO - Epoch(train)  [3][3100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:11  time: 0.3286  data_time: 0.0015  memory: 2203  grad_norm: 115.4590  loss: 2.1025  loss_cls: 0.6632  loss_mask: 0.1822  loss_dice: 1.2571
2025/06/23 17:53:04 - mmengine - INFO - Epoch(train)  [3][3110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:08  time: 0.3338  data_time: 0.0026  memory: 2358  grad_norm: 94.5219  loss: 1.8554  loss_cls: 0.5440  loss_mask: 0.1825  loss_dice: 1.1288
2025/06/23 17:53:08 - mmengine - INFO - Epoch(train)  [3][3120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:05  time: 0.3297  data_time: 0.0033  memory: 2314  grad_norm: 41.5622  loss: 1.6443  loss_cls: 0.5760  loss_mask: 0.1119  loss_dice: 0.9563
2025/06/23 17:53:11 - mmengine - INFO - Epoch(train)  [3][3130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:29:01  time: 0.3260  data_time: 0.0041  memory: 2300  grad_norm: 56.2971  loss: 1.6266  loss_cls: 0.5749  loss_mask: 0.0765  loss_dice: 0.9753
2025/06/23 17:53:14 - mmengine - INFO - Epoch(train)  [3][3140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:58  time: 0.3312  data_time: 0.0037  memory: 2227  grad_norm: 72.6726  loss: 1.5230  loss_cls: 0.4241  loss_mask: 0.1305  loss_dice: 0.9684
2025/06/23 17:53:17 - mmengine - INFO - Epoch(train)  [3][3150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:55  time: 0.3350  data_time: 0.0011  memory: 2402  grad_norm: 52.2587  loss: 1.8658  loss_cls: 0.6399  loss_mask: 0.1026  loss_dice: 1.1233
2025/06/23 17:53:21 - mmengine - INFO - Epoch(train)  [3][3160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:52  time: 0.3374  data_time: 0.0023  memory: 2432  grad_norm: 56.3723  loss: 2.1761  loss_cls: 0.5981  loss_mask: 0.1535  loss_dice: 1.4244
2025/06/23 17:53:24 - mmengine - INFO - Epoch(train)  [3][3170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:48  time: 0.3317  data_time: 0.0016  memory: 2166  grad_norm: 131.6124  loss: 1.6263  loss_cls: 0.2783  loss_mask: 0.3076  loss_dice: 1.0404
2025/06/23 17:53:27 - mmengine - INFO - Epoch(train)  [3][3180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:45  time: 0.3292  data_time: 0.0024  memory: 2446  grad_norm: 75.5845  loss: 1.7300  loss_cls: 0.5173  loss_mask: 0.1628  loss_dice: 1.0499
2025/06/23 17:53:31 - mmengine - INFO - Epoch(train)  [3][3190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:42  time: 0.3319  data_time: 0.0024  memory: 2365  grad_norm: 128.3016  loss: 1.8748  loss_cls: 0.5857  loss_mask: 0.1657  loss_dice: 1.1235
2025/06/23 17:53:34 - mmengine - INFO - Epoch(train)  [3][3200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:38  time: 0.3261  data_time: 0.0022  memory: 2191  grad_norm: 40.7698  loss: 1.0914  loss_cls: 0.3452  loss_mask: 0.0650  loss_dice: 0.6812
2025/06/23 17:53:37 - mmengine - INFO - Epoch(train)  [3][3210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:35  time: 0.3333  data_time: 0.0047  memory: 2336  grad_norm: 80.5457  loss: 1.3926  loss_cls: 0.3831  loss_mask: 0.1858  loss_dice: 0.8237
2025/06/23 17:53:41 - mmengine - INFO - Epoch(train)  [3][3220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:32  time: 0.3318  data_time: 0.0022  memory: 2454  grad_norm: 64.6973  loss: 1.5178  loss_cls: 0.4060  loss_mask: 0.1434  loss_dice: 0.9684
2025/06/23 17:53:44 - mmengine - INFO - Epoch(train)  [3][3230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:28  time: 0.3251  data_time: 0.0014  memory: 2075  grad_norm: 40.9456  loss: 1.3328  loss_cls: 0.3559  loss_mask: 0.1773  loss_dice: 0.7996
2025/06/23 17:53:47 - mmengine - INFO - Epoch(train)  [3][3240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:25  time: 0.3341  data_time: 0.0019  memory: 2425  grad_norm: 88.3837  loss: 1.5303  loss_cls: 0.3472  loss_mask: 0.1603  loss_dice: 1.0229
2025/06/23 17:53:51 - mmengine - INFO - Epoch(train)  [3][3250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:22  time: 0.3330  data_time: 0.0023  memory: 2358  grad_norm: 63.5567  loss: 1.6690  loss_cls: 0.5026  loss_mask: 0.1143  loss_dice: 1.0521
2025/06/23 17:53:54 - mmengine - INFO - Epoch(train)  [3][3260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:18  time: 0.3308  data_time: 0.0021  memory: 2432  grad_norm: 36.2385  loss: 1.4658  loss_cls: 0.3525  loss_mask: 0.1361  loss_dice: 0.9772
2025/06/23 17:53:57 - mmengine - INFO - Epoch(train)  [3][3270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:15  time: 0.3256  data_time: 0.0039  memory: 2329  grad_norm: 82.1195  loss: 0.9505  loss_cls: 0.3453  loss_mask: 0.1130  loss_dice: 0.4922
2025/06/23 17:54:00 - mmengine - INFO - Epoch(train)  [3][3280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:12  time: 0.3311  data_time: 0.0020  memory: 2409  grad_norm: 60.2220  loss: 1.6645  loss_cls: 0.5863  loss_mask: 0.1268  loss_dice: 0.9514
2025/06/23 17:54:04 - mmengine - INFO - Epoch(train)  [3][3290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:08  time: 0.3300  data_time: 0.0012  memory: 2093  grad_norm: 76.6583  loss: 1.3237  loss_cls: 0.3654  loss_mask: 0.0941  loss_dice: 0.8642
2025/06/23 17:54:07 - mmengine - INFO - Epoch(train)  [3][3300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:05  time: 0.3288  data_time: 0.0019  memory: 2160  grad_norm: 44.3925  loss: 1.2818  loss_cls: 0.3719  loss_mask: 0.1632  loss_dice: 0.7467
2025/06/23 17:54:10 - mmengine - INFO - Epoch(train)  [3][3310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:28:02  time: 0.3269  data_time: 0.0011  memory: 2527  grad_norm: 50.3902  loss: 1.6348  loss_cls: 0.4053  loss_mask: 0.1060  loss_dice: 1.1235
2025/06/23 17:54:14 - mmengine - INFO - Epoch(train)  [3][3320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:58  time: 0.3355  data_time: 0.0018  memory: 2505  grad_norm: 80.6593  loss: 1.6377  loss_cls: 0.5182  loss_mask: 0.1137  loss_dice: 1.0057
2025/06/23 17:54:17 - mmengine - INFO - Epoch(train)  [3][3330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:55  time: 0.3285  data_time: 0.0025  memory: 2372  grad_norm: 78.9071  loss: 1.5275  loss_cls: 0.3721  loss_mask: 0.0966  loss_dice: 1.0588
2025/06/23 17:54:20 - mmengine - INFO - Epoch(train)  [3][3340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:52  time: 0.3286  data_time: 0.0035  memory: 2468  grad_norm: 45.9025  loss: 1.4769  loss_cls: 0.3431  loss_mask: 0.0950  loss_dice: 1.0388
2025/06/23 17:54:24 - mmengine - INFO - Epoch(train)  [3][3350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:48  time: 0.3309  data_time: 0.0021  memory: 2490  grad_norm: 58.6299  loss: 1.6460  loss_cls: 0.4251  loss_mask: 0.1469  loss_dice: 1.0741
2025/06/23 17:54:27 - mmengine - INFO - Epoch(train)  [3][3360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:45  time: 0.3272  data_time: 0.0017  memory: 2380  grad_norm: 49.9896  loss: 1.5672  loss_cls: 0.4345  loss_mask: 0.1635  loss_dice: 0.9692
2025/06/23 17:54:30 - mmengine - INFO - Epoch(train)  [3][3370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:41  time: 0.3247  data_time: 0.0034  memory: 2215  grad_norm: 45.1889  loss: 1.0655  loss_cls: 0.3795  loss_mask: 0.0986  loss_dice: 0.5873
2025/06/23 17:54:33 - mmengine - INFO - Epoch(train)  [3][3380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:38  time: 0.3295  data_time: 0.0041  memory: 2329  grad_norm: 56.5278  loss: 1.6188  loss_cls: 0.5527  loss_mask: 0.0990  loss_dice: 0.9670
2025/06/23 17:54:37 - mmengine - INFO - Epoch(train)  [3][3390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:35  time: 0.3281  data_time: 0.0017  memory: 2439  grad_norm: 44.5483  loss: 2.2791  loss_cls: 0.8038  loss_mask: 0.1232  loss_dice: 1.3522
2025/06/23 17:54:40 - mmengine - INFO - Epoch(train)  [3][3400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:31  time: 0.3298  data_time: 0.0044  memory: 2343  grad_norm: 90.2225  loss: 1.5803  loss_cls: 0.4631  loss_mask: 0.1558  loss_dice: 0.9615
2025/06/23 17:54:43 - mmengine - INFO - Epoch(train)  [3][3410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:28  time: 0.3272  data_time: 0.0035  memory: 2124  grad_norm: 37.2211  loss: 1.0284  loss_cls: 0.2834  loss_mask: 0.1231  loss_dice: 0.6219
2025/06/23 17:54:47 - mmengine - INFO - Epoch(train)  [3][3420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:25  time: 0.3298  data_time: 0.0034  memory: 2541  grad_norm: 210.6188  loss: 2.3568  loss_cls: 0.4503  loss_mask: 0.6684  loss_dice: 1.2381
2025/06/23 17:54:50 - mmengine - INFO - Epoch(train)  [3][3430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:21  time: 0.3221  data_time: 0.0032  memory: 2166  grad_norm: 45.6171  loss: 0.8285  loss_cls: 0.1998  loss_mask: 0.0713  loss_dice: 0.5574
2025/06/23 17:54:53 - mmengine - INFO - Epoch(train)  [3][3440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:18  time: 0.3268  data_time: 0.0075  memory: 2387  grad_norm: 50.6782  loss: 1.3573  loss_cls: 0.3271  loss_mask: 0.2348  loss_dice: 0.7954
2025/06/23 17:54:56 - mmengine - INFO - Epoch(train)  [3][3450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:14  time: 0.3282  data_time: 0.0060  memory: 2271  grad_norm: 69.6253  loss: 1.5449  loss_cls: 0.4006  loss_mask: 0.2313  loss_dice: 0.9130
2025/06/23 17:55:00 - mmengine - INFO - Epoch(train)  [3][3460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:11  time: 0.3269  data_time: 0.0052  memory: 2343  grad_norm: 121.6155  loss: 1.4425  loss_cls: 0.4282  loss_mask: 0.1445  loss_dice: 0.8698
2025/06/23 17:55:00 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:55:03 - mmengine - INFO - Epoch(train)  [3][3470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:07  time: 0.3225  data_time: 0.0021  memory: 2058  grad_norm: 61.5809  loss: 0.7157  loss_cls: 0.1666  loss_mask: 0.0599  loss_dice: 0.4893
2025/06/23 17:55:06 - mmengine - INFO - Epoch(train)  [3][3480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:04  time: 0.3278  data_time: 0.0022  memory: 2365  grad_norm: 55.5371  loss: 1.4707  loss_cls: 0.4432  loss_mask: 0.1261  loss_dice: 0.9014
2025/06/23 17:55:09 - mmengine - INFO - Epoch(train)  [3][3490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:27:01  time: 0.3247  data_time: 0.0022  memory: 2245  grad_norm: 75.5050  loss: 1.1113  loss_cls: 0.3079  loss_mask: 0.0901  loss_dice: 0.7133
2025/06/23 17:55:13 - mmengine - INFO - Epoch(train)  [3][3500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:57  time: 0.3326  data_time: 0.0043  memory: 2308  grad_norm: 93.0647  loss: 1.5837  loss_cls: 0.4412  loss_mask: 0.1383  loss_dice: 1.0041
2025/06/23 17:55:16 - mmengine - INFO - Epoch(train)  [3][3510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:54  time: 0.3319  data_time: 0.0021  memory: 2468  grad_norm: 69.7229  loss: 1.6773  loss_cls: 0.6308  loss_mask: 0.1063  loss_dice: 0.9401
2025/06/23 17:55:19 - mmengine - INFO - Epoch(train)  [3][3520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:51  time: 0.3301  data_time: 0.0042  memory: 2490  grad_norm: 53.5600  loss: 1.5366  loss_cls: 0.4279  loss_mask: 0.0681  loss_dice: 1.0405
2025/06/23 17:55:23 - mmengine - INFO - Epoch(train)  [3][3530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:47  time: 0.3265  data_time: 0.0015  memory: 2184  grad_norm: 143.8812  loss: 1.3206  loss_cls: 0.3721  loss_mask: 0.1001  loss_dice: 0.8484
2025/06/23 17:55:26 - mmengine - INFO - Epoch(train)  [3][3540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:44  time: 0.3260  data_time: 0.0005  memory: 2308  grad_norm: 64.2532  loss: 1.4261  loss_cls: 0.3619  loss_mask: 0.1012  loss_dice: 0.9629
2025/06/23 17:55:29 - mmengine - INFO - Epoch(train)  [3][3550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:40  time: 0.3254  data_time: 0.0056  memory: 2417  grad_norm: 61.2988  loss: 1.3050  loss_cls: 0.3692  loss_mask: 0.1974  loss_dice: 0.7384
2025/06/23 17:55:32 - mmengine - INFO - Epoch(train)  [3][3560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:37  time: 0.3319  data_time: 0.0008  memory: 2527  grad_norm: 83.3918  loss: 1.9461  loss_cls: 0.5458  loss_mask: 0.2667  loss_dice: 1.1336
2025/06/23 17:55:36 - mmengine - INFO - Epoch(train)  [3][3570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:34  time: 0.3281  data_time: 0.0019  memory: 2197  grad_norm: 65.1643  loss: 1.6930  loss_cls: 0.5228  loss_mask: 0.1264  loss_dice: 1.0437
2025/06/23 17:55:39 - mmengine - INFO - Epoch(train)  [3][3580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:31  time: 0.3368  data_time: 0.0030  memory: 2483  grad_norm: 64.8144  loss: 2.1300  loss_cls: 0.7812  loss_mask: 0.1399  loss_dice: 1.2089
2025/06/23 17:55:42 - mmengine - INFO - Epoch(train)  [3][3590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:27  time: 0.3321  data_time: 0.0021  memory: 2293  grad_norm: 117.6373  loss: 1.4638  loss_cls: 0.5274  loss_mask: 0.1303  loss_dice: 0.8061
2025/06/23 17:55:46 - mmengine - INFO - Epoch(train)  [3][3600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:24  time: 0.3369  data_time: 0.0035  memory: 2380  grad_norm: 46.4313  loss: 2.5894  loss_cls: 0.6201  loss_mask: 0.2209  loss_dice: 1.7483
2025/06/23 17:55:49 - mmengine - INFO - Epoch(train)  [3][3610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:21  time: 0.3333  data_time: 0.0024  memory: 2387  grad_norm: 62.9145  loss: 1.4223  loss_cls: 0.4513  loss_mask: 0.1217  loss_dice: 0.8493
2025/06/23 17:55:52 - mmengine - INFO - Epoch(train)  [3][3620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:18  time: 0.3349  data_time: 0.0018  memory: 2372  grad_norm: 111.3052  loss: 2.4150  loss_cls: 0.6455  loss_mask: 0.3251  loss_dice: 1.4444
2025/06/23 17:55:56 - mmengine - INFO - Epoch(train)  [3][3630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:14  time: 0.3334  data_time: 0.0017  memory: 2498  grad_norm: 50.5956  loss: 1.6674  loss_cls: 0.4528  loss_mask: 0.1165  loss_dice: 1.0981
2025/06/23 17:55:59 - mmengine - INFO - Epoch(train)  [3][3640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:11  time: 0.3305  data_time: 0.0035  memory: 2461  grad_norm: 76.5887  loss: 1.7146  loss_cls: 0.4008  loss_mask: 0.1575  loss_dice: 1.1563
2025/06/23 17:56:02 - mmengine - INFO - Epoch(train)  [3][3650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:08  time: 0.3290  data_time: 0.0029  memory: 2336  grad_norm: 64.1164  loss: 1.8266  loss_cls: 0.5307  loss_mask: 0.2255  loss_dice: 1.0704
2025/06/23 17:56:06 - mmengine - INFO - Epoch(train)  [3][3660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:04  time: 0.3290  data_time: 0.0022  memory: 2519  grad_norm: 72.7843  loss: 1.6212  loss_cls: 0.4813  loss_mask: 0.1213  loss_dice: 1.0185
2025/06/23 17:56:09 - mmengine - INFO - Epoch(train)  [3][3670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:26:01  time: 0.3302  data_time: 0.0031  memory: 2534  grad_norm: 48.2729  loss: 1.0569  loss_cls: 0.3268  loss_mask: 0.0558  loss_dice: 0.6744
2025/06/23 17:56:12 - mmengine - INFO - Epoch(train)  [3][3680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:25:58  time: 0.3313  data_time: 0.0022  memory: 2380  grad_norm: 124.3096  loss: 2.3231  loss_cls: 0.5495  loss_mask: 0.1914  loss_dice: 1.5822
2025/06/23 17:56:16 - mmengine - INFO - Epoch(train)  [3][3690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:25:55  time: 0.3351  data_time: 0.0025  memory: 2308  grad_norm: 82.8235  loss: 1.9918  loss_cls: 0.6129  loss_mask: 0.1717  loss_dice: 1.2073
2025/06/23 17:56:19 - mmengine - INFO - Epoch(train)  [3][3700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:25:51  time: 0.3327  data_time: 0.0025  memory: 2358  grad_norm: 137.0456  loss: 1.3989  loss_cls: 0.4353  loss_mask: 0.0880  loss_dice: 0.8755
2025/06/23 17:56:22 - mmengine - INFO - Epoch(train)  [3][3710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:25:48  time: 0.3312  data_time: 0.0029  memory: 2239  grad_norm: 81.4343  loss: 1.5948  loss_cls: 0.4005  loss_mask: 0.1319  loss_dice: 1.0625
2025/06/23 17:56:26 - mmengine - INFO - Epoch(train)  [3][3720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:25:45  time: 0.3312  data_time: 0.0025  memory: 2409  grad_norm: 83.6458  loss: 1.4939  loss_cls: 0.3769  loss_mask: 0.1199  loss_dice: 0.9971
2025/06/23 17:56:29 - mmengine - INFO - Epoch(train)  [3][3730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:25:41  time: 0.3297  data_time: 0.0015  memory: 2285  grad_norm: 88.7284  loss: 1.6808  loss_cls: 0.4336  loss_mask: 0.1649  loss_dice: 1.0824
2025/06/23 17:56:32 - mmengine - INFO - Epoch(train)  [3][3740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:25:38  time: 0.3292  data_time: 0.0021  memory: 2336  grad_norm: 84.4655  loss: 1.7458  loss_cls: 0.5161  loss_mask: 0.1776  loss_dice: 1.0520
2025/06/23 17:56:35 - mmengine - INFO - Epoch(train)  [3][3750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:25:35  time: 0.3270  data_time: 0.0040  memory: 2245  grad_norm: 107.9271  loss: 1.5584  loss_cls: 0.4777  loss_mask: 0.0959  loss_dice: 0.9848
2025/06/23 17:56:39 - mmengine - INFO - Epoch(train)  [3][3760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 2:25:31  time: 0.3288  data_time: 0.0032  memory: 2454  grad_norm: 105.5803  loss: 1.7048  loss_cls: 0.4958  loss_mask: 0.1493  loss_dice: 1.0597
2025/06/23 17:56:42 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250623_164931
2025/06/23 17:56:42 - mmengine - INFO - Saving checkpoint at 3 epochs
2025/06/23 17:56:59 - mmengine - INFO - Epoch(val)  [3][ 10/209]    eta: 0:03:09  time: 0.9500  data_time: 0.5642  memory: 2106  
