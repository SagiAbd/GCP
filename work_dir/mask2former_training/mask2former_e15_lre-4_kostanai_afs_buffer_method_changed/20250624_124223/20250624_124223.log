2025/06/24 12:42:27 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: win32
    Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1011863722
    GPU 0: NVIDIA GeForce GTX 1660
    CUDA_HOME: C:\Users\Sagi\Miniconda3\envs\gcp-env\Library
    NVCC: Not Available
    MSVC: Оптимизирующий компилятор Microsoft (R) C/C++ версии 19.43.34810 для x64
    GCC: n/a
    PyTorch: 2.3.0
    PyTorch compiling details: PyTorch built with:
  - C++ Version: 201703
  - MSVC 192930151
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.8.1  (built against CUDA 12.0)
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.8.1, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.18.0
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1011863722
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/06/24 12:42:29 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=2, enable=True)
backend_args = None
batch_augments = [
    dict(
        img_pad_value=0,
        mask_pad_value=0,
        pad_mask=True,
        pad_seg=True,
        seg_pad_value=255,
        size=(
            512,
            512,
        ),
        type='BatchFixedSizePad'),
]
crop_size = (
    512,
    512,
)
data_preprocessor = dict(
    batch_augments=[
        dict(
            img_pad_value=0,
            mask_pad_value=0,
            pad_mask=True,
            pad_seg=True,
            seg_pad_value=255,
            size=(
                512,
                512,
            ),
            type='BatchFixedSizePad'),
    ],
    bgr_to_rgb=True,
    mask_pad_value=0,
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    pad_mask=True,
    pad_seg=True,
    pad_size_divisor=32,
    seg_pad_value=255,
    std=[
        58.395,
        57.12,
        57.375,
    ],
    type='DetDataPreprocessor')
data_root = 'data/kostanai'
dataset_type = 'WHUMixVectorDataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=True,
        interval=1,
        max_keep_ckpts=1,
        save_last=True,
        type='CheckpointHook'),
    logger=dict(interval=10, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    val_loss_wandb=dict(interval=1, type='ValLossWandbHook'),
    visualization=dict(draw=True, interval=10, type='TanmlhVisualizationHook'))
default_scope = 'mmdet'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
img_norm_cfg = dict(
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    std=[
        58.395,
        57.12,
        57.375,
    ],
    to_rgb=True)
launcher = 'none'
load_from = 'checkpoints/mask2former_r50_pretrained_50e_whu-mix-vector.pth'
log_config = dict(hooks=[
    dict(type='TextLoggerHook'),
    dict(
        init_kwargs=dict(
            allow_val_change=True,
            group='mask2former_training',
            name='mask2former_e15_lre-4_kostanai_afs_buffer_method_changed',
            project='building-segmentation-gcp',
            resume='never'),
        interval=10,
        log_checkpoint=True,
        log_checkpoint_metadata=True,
        num_eval_images=10,
        type='MMDetWandbHook'),
])
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=10)
max_epochs = 10
model = dict(
    backbone=dict(
        depth=50,
        frozen_stages=-1,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=False, type='BN'),
        norm_eval=True,
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mask_pad_value=0,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=True,
        pad_seg=True,
        pad_size_divisor=32,
        seg_pad_value=255,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    init_cfg=None,
    panoptic_fusion_head=dict(
        init_cfg=None,
        loss_panoptic=None,
        num_stuff_classes=0,
        num_things_classes=1,
        type='MaskFormerFusionHead'),
    panoptic_head=dict(
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=True),
        num_queries=300,
        num_stuff_classes=0,
        num_things_classes=1,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(
        filter_low_score=False,
        instance_on=True,
        iou_thr=0.8,
        max_per_image=200,
        panoptic_on=False,
        semantic_on=False),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='ClassificationCost', weight=2.0),
                dict(
                    type='CrossEntropyLossCost', use_sigmoid=True, weight=5.0),
                dict(eps=1.0, pred_act=True, type='DiceCost', weight=5.0),
            ],
            type='HungarianAssigner'),
        importance_sample_ratio=0.75,
        num_points=12544,
        oversample_ratio=3.0,
        sampler=dict(type='MaskPseudoSampler')),
    type='Mask2Former')
num_classes = 1
num_stuff_classes = 0
num_things_classes = 1
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(decay_mult=1.0, lr_mult=0.1),
            level_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_feat=dict(decay_mult=0.0, lr_mult=1.0)),
        norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0, by_epoch=False, end=1000, start_factor=0.001,
        type='LinearLR'),
    dict(
        begin=0,
        by_epoch=True,
        end=10,
        gamma=0.1,
        milestones=[
            40,
        ],
        type='MultiStepLR'),
]
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='test/test.json',
        backend_args=None,
        data_prefix=dict(img='test/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=1,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = [
    dict(
        ann_file='data/kostanai/test/test.json',
        backend_args=None,
        metric=[
            'segm',
        ],
        type='CocoMetric'),
]
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=False,
        with_mask=True,
        with_poly_json=False),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=10, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=1,
    dataset=dict(
        ann_file='train/train.json',
        backend_args=None,
        data_prefix=dict(img='train/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True,
                with_poly_json=False),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                direction=[
                    'horizontal',
                    'vertical',
                    'diagonal',
                ],
                prob=0.75,
                type='RandomFlip'),
            dict(prob=0.75, type='Rotate90'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        type='WHUMixVectorDataset'),
    num_workers=1,
    persistent_workers=False,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True,
        with_poly_json=False),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        direction=[
            'horizontal',
            'vertical',
            'diagonal',
        ],
        prob=0.75,
        type='RandomFlip'),
    dict(prob=0.75, type='Rotate90'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='val/val.json',
        backend_args=None,
        data_prefix=dict(img='val/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=1,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = [
    dict(
        ann_file='data/kostanai/val/val.json',
        backend_args=None,
        metric=[
            'segm',
        ],
        type='CocoMetric'),
]
vis_backends = [
    dict(
        init_kwargs=dict(
            allow_val_change=True,
            group='mask2former_training',
            name='mask2former_e15_lre-4_kostanai_afs_buffer_method_changed',
            project='building-segmentation-gcp',
            resume='never'),
        save_dir=
        'work_dir\\mask2former_training\\mask2former_e15_lre-4_kostanai_afs_buffer_method_changed\\wandb',
        type='WandbVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='TanmlhVisualizer',
    vis_backends=[
        dict(
            init_kwargs=dict(
                allow_val_change=True,
                group='mask2former_training',
                name='mask2former_e15_lre-4_kostanai_afs_buffer_method_changed',
                project='building-segmentation-gcp',
                resume='never'),
            save_dir=
            'work_dir\\mask2former_training\\mask2former_e15_lre-4_kostanai_afs_buffer_method_changed\\wandb',
            type='WandbVisBackend'),
    ])
work_dir = 'work_dir\\mask2former_training\\mask2former_e15_lre-4_kostanai_afs_buffer_method_changed'

2025/06/24 12:42:34 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/06/24 12:42:34 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) ValLossWandbHook                   
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.0.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.0.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.0.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.0.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.0.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.0.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.1.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.1.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.1.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.1.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.1.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.1.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.2.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.2.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.2.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.2.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.2.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer1.2.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer2.0.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer2.0.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr=1e-05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:weight_decay=0.05
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr_mult=0.1
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:decay_mult=1.0
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer2.0.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - WARNING - backbone.layer2.0.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:39 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.0.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.0.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.1.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.1.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.1.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.1.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.1.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.1.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.2.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.2.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.2.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.2.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.2.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.2.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.3.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.3.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.3.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.3.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.3.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer2.3.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.0.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.0.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.0.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.0.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.0.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.0.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.1.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.1.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.1.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.1.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.1.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.1.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.2.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.2.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.2.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.2.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.2.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.2.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.3.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.3.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.3.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.3.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.3.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.3.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.4.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.4.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.4.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.4.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.4.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.4.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.5.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.5.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.5.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.5.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.5.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer3.5.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.0.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.0.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.0.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.0.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.0.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.0.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.1.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.1.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.1.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.1.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.1.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.1.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.2.bn1.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.2.bn1.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.2.bn2.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.2.bn2.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1e-05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.1
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.2.bn3.weight is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - WARNING - backbone.layer4.2.bn3.bias is skipped since its requires_grad=False
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.bias:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr=0.0001
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr_mult=1.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:decay_mult=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr=0.0001
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr_mult=1.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:decay_mult=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr=0.0001
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:weight_decay=0.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr_mult=1.0
2025/06/24 12:42:40 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:decay_mult=0.0
2025/06/24 12:42:40 - mmengine - INFO - LR is set based on batch size of 2 and the current batch size is 1. Scaling the original LR by 0.5.
2025/06/24 12:42:42 - mmengine - INFO - load model from: torchvision://resnet50
2025/06/24 12:42:42 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
2025/06/24 12:42:42 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

panoptic_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 512, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.query_embed.weight - torch.Size([300, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.query_feat.weight - torch.Size([300, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.cls_embed.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.cls_embed.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  
2025/06/24 12:42:43 - mmengine - INFO - Load checkpoint from checkpoints/mask2former_r50_pretrained_50e_whu-mix-vector.pth
2025/06/24 12:42:43 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/06/24 12:42:43 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/06/24 12:42:43 - mmengine - INFO - Checkpoints will be saved to D:\Sagi\GCP\GCP\work_dir\mask2former_training\mask2former_e15_lre-4_kostanai_afs_buffer_method_changed.
2025/06/24 12:42:51 - mmengine - INFO - Epoch(train)  [1][  10/3769]  base_lr: 1.0000e-06 lr: 5.0000e-08  eta: 8:19:13  time: 0.7949  data_time: 0.3974  memory: 2268  grad_norm: 52.4687  loss: 1.4194  loss_cls: 0.4192  loss_mask: 0.0796  loss_dice: 0.9205
2025/06/24 12:42:54 - mmengine - INFO - Epoch(train)  [1][  20/3769]  base_lr: 2.0000e-06 lr: 1.0000e-07  eta: 5:54:38  time: 0.3348  data_time: 0.0033  memory: 2430  grad_norm: 70.1463  loss: 1.3820  loss_cls: 0.3869  loss_mask: 0.1247  loss_dice: 0.8704
2025/06/24 12:42:58 - mmengine - INFO - Epoch(train)  [1][  30/3769]  base_lr: 3.0000e-06 lr: 1.5000e-07  eta: 5:07:17  time: 0.3390  data_time: 0.0035  memory: 2291  grad_norm: 43.6087  loss: 1.1544  loss_cls: 0.3235  loss_mask: 0.0558  loss_dice: 0.7751
2025/06/24 12:43:01 - mmengine - INFO - Epoch(train)  [1][  40/3769]  base_lr: 4.0000e-06 lr: 2.0000e-07  eta: 4:42:08  time: 0.3298  data_time: 0.0038  memory: 2385  grad_norm: 76.5110  loss: 0.7900  loss_cls: 0.2060  loss_mask: 0.0525  loss_dice: 0.5316
2025/06/24 12:43:04 - mmengine - INFO - Epoch(train)  [1][  50/3769]  base_lr: 5.0000e-06 lr: 2.5000e-07  eta: 4:27:27  time: 0.3331  data_time: 0.0041  memory: 2385  grad_norm: 75.1837  loss: 1.5393  loss_cls: 0.4298  loss_mask: 0.1455  loss_dice: 0.9640
2025/06/24 12:43:08 - mmengine - INFO - Epoch(train)  [1][  60/3769]  base_lr: 6.0000e-06 lr: 3.0000e-07  eta: 4:17:03  time: 0.3276  data_time: 0.0040  memory: 2225  grad_norm: 53.2420  loss: 1.3618  loss_cls: 0.1874  loss_mask: 0.1162  loss_dice: 1.0581
2025/06/24 12:43:11 - mmengine - INFO - Epoch(train)  [1][  70/3769]  base_lr: 7.0000e-06 lr: 3.5000e-07  eta: 4:09:49  time: 0.3298  data_time: 0.0031  memory: 2237  grad_norm: 53.3720  loss: 1.0033  loss_cls: 0.2978  loss_mask: 0.0896  loss_dice: 0.6160
2025/06/24 12:43:14 - mmengine - INFO - Epoch(train)  [1][  80/3769]  base_lr: 8.0000e-06 lr: 4.0000e-07  eta: 4:04:06  time: 0.3263  data_time: 0.0020  memory: 2364  grad_norm: 36.4646  loss: 1.2464  loss_cls: 0.2933  loss_mask: 0.1376  loss_dice: 0.8155
2025/06/24 12:43:17 - mmengine - INFO - Epoch(train)  [1][  90/3769]  base_lr: 9.0000e-06 lr: 4.5000e-07  eta: 3:59:33  time: 0.3251  data_time: 0.0030  memory: 2183  grad_norm: 78.7391  loss: 1.3792  loss_cls: 0.3714  loss_mask: 0.1772  loss_dice: 0.8306
2025/06/24 12:43:21 - mmengine - INFO - Epoch(train)  [1][ 100/3769]  base_lr: 1.0000e-05 lr: 5.0000e-07  eta: 3:56:15  time: 0.3307  data_time: 0.0042  memory: 2489  grad_norm: 45.9697  loss: 1.2005  loss_cls: 0.3074  loss_mask: 0.0816  loss_dice: 0.8114
2025/06/24 12:43:24 - mmengine - INFO - Epoch(train)  [1][ 110/3769]  base_lr: 1.1000e-05 lr: 5.5000e-07  eta: 3:53:06  time: 0.3227  data_time: 0.0039  memory: 2249  grad_norm: 38.5871  loss: 0.9767  loss_cls: 0.1816  loss_mask: 0.1341  loss_dice: 0.6610
2025/06/24 12:43:27 - mmengine - INFO - Epoch(train)  [1][ 120/3769]  base_lr: 1.2000e-05 lr: 6.0000e-07  eta: 3:50:31  time: 0.3241  data_time: 0.0018  memory: 2116  grad_norm: 99.0847  loss: 1.8026  loss_cls: 0.2038  loss_mask: 0.5770  loss_dice: 1.0218
2025/06/24 12:43:30 - mmengine - INFO - Epoch(train)  [1][ 130/3769]  base_lr: 1.3000e-05 lr: 6.5000e-07  eta: 3:48:25  time: 0.3258  data_time: 0.0023  memory: 2356  grad_norm: 22.3777  loss: 0.9515  loss_cls: 0.1645  loss_mask: 0.1024  loss_dice: 0.6845
2025/06/24 12:43:34 - mmengine - INFO - Epoch(train)  [1][ 140/3769]  base_lr: 1.4000e-05 lr: 7.0000e-07  eta: 3:46:45  time: 0.3291  data_time: 0.0053  memory: 2525  grad_norm: 26.4895  loss: 0.8139  loss_cls: 0.1956  loss_mask: 0.0753  loss_dice: 0.5430
2025/06/24 12:43:37 - mmengine - INFO - Epoch(train)  [1][ 150/3769]  base_lr: 1.5000e-05 lr: 7.5000e-07  eta: 3:45:11  time: 0.3259  data_time: 0.0032  memory: 2327  grad_norm: 165.4562  loss: 0.6768  loss_cls: 0.2010  loss_mask: 0.0419  loss_dice: 0.4338
2025/06/24 12:43:40 - mmengine - INFO - Epoch(train)  [1][ 160/3769]  base_lr: 1.6000e-05 lr: 8.0000e-07  eta: 3:43:55  time: 0.3293  data_time: 0.0028  memory: 2407  grad_norm: 20.7384  loss: 1.5473  loss_cls: 0.1832  loss_mask: 0.1492  loss_dice: 1.2150
2025/06/24 12:43:43 - mmengine - INFO - Epoch(train)  [1][ 170/3769]  base_lr: 1.7000e-05 lr: 8.5000e-07  eta: 3:42:38  time: 0.3247  data_time: 0.0037  memory: 2262  grad_norm: 34.0581  loss: 0.8160  loss_cls: 0.1819  loss_mask: 0.0646  loss_dice: 0.5696
2025/06/24 12:43:47 - mmengine - INFO - Epoch(train)  [1][ 180/3769]  base_lr: 1.8000e-05 lr: 9.0000e-07  eta: 3:41:35  time: 0.3275  data_time: 0.0021  memory: 2104  grad_norm: 48.3351  loss: 1.4797  loss_cls: 0.2921  loss_mask: 0.2230  loss_dice: 0.9646
2025/06/24 12:43:50 - mmengine - INFO - Epoch(train)  [1][ 190/3769]  base_lr: 1.9000e-05 lr: 9.5000e-07  eta: 3:40:47  time: 0.3321  data_time: 0.0015  memory: 2327  grad_norm: 39.3548  loss: 1.1695  loss_cls: 0.2661  loss_mask: 0.1668  loss_dice: 0.7365
2025/06/24 12:43:53 - mmengine - INFO - Epoch(train)  [1][ 200/3769]  base_lr: 2.0000e-05 lr: 1.0000e-06  eta: 3:40:01  time: 0.3306  data_time: 0.0036  memory: 2298  grad_norm: 57.3851  loss: 0.9202  loss_cls: 0.1540  loss_mask: 0.0898  loss_dice: 0.6764
2025/06/24 12:43:57 - mmengine - INFO - Epoch(train)  [1][ 210/3769]  base_lr: 2.1000e-05 lr: 1.0500e-06  eta: 3:39:15  time: 0.3284  data_time: 0.0020  memory: 2452  grad_norm: 16.9831  loss: 0.8187  loss_cls: 0.1559  loss_mask: 0.0536  loss_dice: 0.6091
2025/06/24 12:44:00 - mmengine - INFO - Epoch(train)  [1][ 220/3769]  base_lr: 2.2000e-05 lr: 1.1000e-06  eta: 3:38:38  time: 0.3314  data_time: 0.0053  memory: 2276  grad_norm: 76.7902  loss: 1.2085  loss_cls: 0.3578  loss_mask: 0.0944  loss_dice: 0.7563
2025/06/24 12:44:03 - mmengine - INFO - Epoch(train)  [1][ 230/3769]  base_lr: 2.3000e-05 lr: 1.1500e-06  eta: 3:37:55  time: 0.3257  data_time: 0.0055  memory: 2349  grad_norm: 17.9014  loss: 0.9314  loss_cls: 0.1706  loss_mask: 0.0632  loss_dice: 0.6975
2025/06/24 12:44:06 - mmengine - INFO - Epoch(train)  [1][ 240/3769]  base_lr: 2.4000e-05 lr: 1.2000e-06  eta: 3:37:11  time: 0.3232  data_time: 0.0024  memory: 2305  grad_norm: 53.3132  loss: 0.7383  loss_cls: 0.1805  loss_mask: 0.1147  loss_dice: 0.4431
2025/06/24 12:44:10 - mmengine - INFO - Epoch(train)  [1][ 250/3769]  base_lr: 2.5000e-05 lr: 1.2500e-06  eta: 3:36:43  time: 0.3316  data_time: 0.0031  memory: 2489  grad_norm: 65.2262  loss: 1.5177  loss_cls: 0.4067  loss_mask: 0.1585  loss_dice: 0.9525
2025/06/24 12:44:13 - mmengine - INFO - Epoch(train)  [1][ 260/3769]  base_lr: 2.6000e-05 lr: 1.3000e-06  eta: 3:36:08  time: 0.3250  data_time: 0.0011  memory: 2452  grad_norm: 137.7144  loss: 0.7529  loss_cls: 0.1986  loss_mask: 0.0572  loss_dice: 0.4970
2025/06/24 12:44:16 - mmengine - INFO - Epoch(train)  [1][ 270/3769]  base_lr: 2.7000e-05 lr: 1.3500e-06  eta: 3:35:35  time: 0.3256  data_time: 0.0016  memory: 2195  grad_norm: 89.9004  loss: 1.0017  loss_cls: 0.3263  loss_mask: 0.0978  loss_dice: 0.5776
2025/06/24 12:44:20 - mmengine - INFO - Epoch(train)  [1][ 280/3769]  base_lr: 2.8000e-05 lr: 1.4000e-06  eta: 3:35:06  time: 0.3266  data_time: 0.0028  memory: 2378  grad_norm: 249.9555  loss: 0.9387  loss_cls: 0.2099  loss_mask: 0.0897  loss_dice: 0.6390
2025/06/24 12:44:23 - mmengine - INFO - Epoch(train)  [1][ 290/3769]  base_lr: 2.9000e-05 lr: 1.4500e-06  eta: 3:34:39  time: 0.3266  data_time: 0.0031  memory: 2189  grad_norm: 46.4952  loss: 0.9860  loss_cls: 0.2760  loss_mask: 0.0922  loss_dice: 0.6179
2025/06/24 12:44:26 - mmengine - INFO - Epoch(train)  [1][ 300/3769]  base_lr: 3.0000e-05 lr: 1.5000e-06  eta: 3:34:14  time: 0.3271  data_time: 0.0006  memory: 2467  grad_norm: 21.4745  loss: 0.9045  loss_cls: 0.2440  loss_mask: 0.1206  loss_dice: 0.5399
2025/06/24 12:44:29 - mmengine - INFO - Epoch(train)  [1][ 310/3769]  base_lr: 3.1000e-05 lr: 1.5500e-06  eta: 3:33:51  time: 0.3272  data_time: 0.0044  memory: 2320  grad_norm: 87.2944  loss: 1.1110  loss_cls: 0.2575  loss_mask: 0.1542  loss_dice: 0.6994
2025/06/24 12:44:33 - mmengine - INFO - Epoch(train)  [1][ 320/3769]  base_lr: 3.2000e-05 lr: 1.6000e-06  eta: 3:33:34  time: 0.3321  data_time: 0.0068  memory: 2540  grad_norm: 23.2029  loss: 0.9555  loss_cls: 0.2263  loss_mask: 0.0974  loss_dice: 0.6318
2025/06/24 12:44:36 - mmengine - INFO - Epoch(train)  [1][ 330/3769]  base_lr: 3.3000e-05 lr: 1.6500e-06  eta: 3:33:13  time: 0.3275  data_time: 0.0026  memory: 2276  grad_norm: 49.3479  loss: 0.8802  loss_cls: 0.2578  loss_mask: 0.0496  loss_dice: 0.5729
2025/06/24 12:44:39 - mmengine - INFO - Epoch(train)  [1][ 340/3769]  base_lr: 3.4000e-05 lr: 1.7000e-06  eta: 3:32:51  time: 0.3248  data_time: 0.0024  memory: 2237  grad_norm: 25.3204  loss: 1.1089  loss_cls: 0.2205  loss_mask: 0.1384  loss_dice: 0.7500
2025/06/24 12:44:42 - mmengine - INFO - Epoch(train)  [1][ 350/3769]  base_lr: 3.5000e-05 lr: 1.7500e-06  eta: 3:32:34  time: 0.3300  data_time: 0.0030  memory: 2327  grad_norm: 75.7792  loss: 1.2613  loss_cls: 0.4573  loss_mask: 0.1051  loss_dice: 0.6990
2025/06/24 12:44:46 - mmengine - INFO - Epoch(train)  [1][ 360/3769]  base_lr: 3.6000e-05 lr: 1.8000e-06  eta: 3:32:15  time: 0.3265  data_time: 0.0032  memory: 2201  grad_norm: 17.7039  loss: 0.8271  loss_cls: 0.1976  loss_mask: 0.1154  loss_dice: 0.5142
2025/06/24 12:44:49 - mmengine - INFO - Epoch(train)  [1][ 370/3769]  base_lr: 3.7000e-05 lr: 1.8500e-06  eta: 3:32:03  time: 0.3326  data_time: 0.0048  memory: 2371  grad_norm: 92.6631  loss: 1.4633  loss_cls: 0.3741  loss_mask: 0.1360  loss_dice: 0.9532
2025/06/24 12:44:52 - mmengine - INFO - Epoch(train)  [1][ 380/3769]  base_lr: 3.8000e-05 lr: 1.9000e-06  eta: 3:31:50  time: 0.3310  data_time: 0.0031  memory: 2342  grad_norm: 20.6147  loss: 0.8732  loss_cls: 0.1534  loss_mask: 0.1043  loss_dice: 0.6154
2025/06/24 12:44:56 - mmengine - INFO - Epoch(train)  [1][ 390/3769]  base_lr: 3.9000e-05 lr: 1.9500e-06  eta: 3:31:33  time: 0.3263  data_time: 0.0060  memory: 2385  grad_norm: 64.0808  loss: 1.3935  loss_cls: 0.2273  loss_mask: 0.1645  loss_dice: 1.0017
2025/06/24 12:44:59 - mmengine - INFO - Epoch(train)  [1][ 400/3769]  base_lr: 4.0000e-05 lr: 2.0000e-06  eta: 3:31:18  time: 0.3285  data_time: 0.0044  memory: 2086  grad_norm: 157.9542  loss: 1.1971  loss_cls: 0.2804  loss_mask: 0.1760  loss_dice: 0.7408
2025/06/24 12:45:02 - mmengine - INFO - Epoch(train)  [1][ 410/3769]  base_lr: 4.1000e-05 lr: 2.0500e-06  eta: 3:31:08  time: 0.3321  data_time: 0.0032  memory: 2416  grad_norm: 22.9323  loss: 1.1422  loss_cls: 0.2503  loss_mask: 0.1784  loss_dice: 0.7135
2025/06/24 12:45:06 - mmengine - INFO - Epoch(train)  [1][ 420/3769]  base_lr: 4.2000e-05 lr: 2.1000e-06  eta: 3:30:56  time: 0.3300  data_time: 0.0054  memory: 2171  grad_norm: 34.8529  loss: 0.9462  loss_cls: 0.2873  loss_mask: 0.0935  loss_dice: 0.5654
2025/06/24 12:45:09 - mmengine - INFO - Epoch(train)  [1][ 430/3769]  base_lr: 4.3000e-05 lr: 2.1500e-06  eta: 3:30:42  time: 0.3279  data_time: 0.0033  memory: 2207  grad_norm: 126.4547  loss: 1.0902  loss_cls: 0.1836  loss_mask: 0.2488  loss_dice: 0.6578
2025/06/24 12:45:12 - mmengine - INFO - Epoch(train)  [1][ 440/3769]  base_lr: 4.4000e-05 lr: 2.2000e-06  eta: 3:30:32  time: 0.3305  data_time: 0.0007  memory: 2378  grad_norm: 15.9728  loss: 0.9566  loss_cls: 0.2717  loss_mask: 0.0706  loss_dice: 0.6143
2025/06/24 12:45:15 - mmengine - INFO - Epoch(train)  [1][ 450/3769]  base_lr: 4.5000e-05 lr: 2.2500e-06  eta: 3:30:19  time: 0.3281  data_time: 0.0044  memory: 2349  grad_norm: 26.4574  loss: 1.0807  loss_cls: 0.3481  loss_mask: 0.0632  loss_dice: 0.6694
2025/06/24 12:45:19 - mmengine - INFO - Epoch(train)  [1][ 460/3769]  base_lr: 4.6000e-05 lr: 2.3000e-06  eta: 3:30:09  time: 0.3303  data_time: 0.0042  memory: 2378  grad_norm: 36.4317  loss: 0.9825  loss_cls: 0.1841  loss_mask: 0.0911  loss_dice: 0.7073
2025/06/24 12:45:22 - mmengine - INFO - Epoch(train)  [1][ 470/3769]  base_lr: 4.7000e-05 lr: 2.3500e-06  eta: 3:29:53  time: 0.3228  data_time: 0.0034  memory: 2177  grad_norm: 92.3654  loss: 1.0858  loss_cls: 0.3278  loss_mask: 0.0750  loss_dice: 0.6830
2025/06/24 12:45:25 - mmengine - INFO - Epoch(train)  [1][ 480/3769]  base_lr: 4.8000e-05 lr: 2.4000e-06  eta: 3:29:42  time: 0.3288  data_time: 0.0030  memory: 2445  grad_norm: 31.6058  loss: 1.3572  loss_cls: 0.3904  loss_mask: 0.1348  loss_dice: 0.8320
2025/06/24 12:45:28 - mmengine - INFO - Epoch(train)  [1][ 490/3769]  base_lr: 4.9000e-05 lr: 2.4500e-06  eta: 3:29:27  time: 0.3235  data_time: 0.0035  memory: 2213  grad_norm: 68.7268  loss: 1.0721  loss_cls: 0.1801  loss_mask: 0.0819  loss_dice: 0.8100
2025/06/24 12:45:32 - mmengine - INFO - Epoch(train)  [1][ 500/3769]  base_lr: 5.0000e-05 lr: 2.5000e-06  eta: 3:29:13  time: 0.3231  data_time: 0.0060  memory: 2213  grad_norm: 34.1440  loss: 0.8088  loss_cls: 0.2351  loss_mask: 0.0792  loss_dice: 0.4945
2025/06/24 12:45:35 - mmengine - INFO - Epoch(train)  [1][ 510/3769]  base_lr: 5.1000e-05 lr: 2.5500e-06  eta: 3:28:59  time: 0.3229  data_time: 0.0040  memory: 2171  grad_norm: 32.0391  loss: 0.6891  loss_cls: 0.2183  loss_mask: 0.0631  loss_dice: 0.4077
2025/06/24 12:45:38 - mmengine - INFO - Epoch(train)  [1][ 520/3769]  base_lr: 5.2000e-05 lr: 2.6000e-06  eta: 3:28:47  time: 0.3250  data_time: 0.0019  memory: 2356  grad_norm: 13.2183  loss: 0.8803  loss_cls: 0.1846  loss_mask: 0.0920  loss_dice: 0.6037
2025/06/24 12:45:41 - mmengine - INFO - Epoch(train)  [1][ 530/3769]  base_lr: 5.3000e-05 lr: 2.6500e-06  eta: 3:28:36  time: 0.3267  data_time: 0.0028  memory: 2474  grad_norm: 28.7701  loss: 0.8305  loss_cls: 0.2197  loss_mask: 0.0463  loss_dice: 0.5646
2025/06/24 12:45:45 - mmengine - INFO - Epoch(train)  [1][ 540/3769]  base_lr: 5.4000e-05 lr: 2.7000e-06  eta: 3:28:26  time: 0.3275  data_time: 0.0028  memory: 2349  grad_norm: 26.6498  loss: 1.0172  loss_cls: 0.2370  loss_mask: 0.0710  loss_dice: 0.7092
2025/06/24 12:45:48 - mmengine - INFO - Epoch(train)  [1][ 550/3769]  base_lr: 5.5000e-05 lr: 2.7500e-06  eta: 3:28:16  time: 0.3263  data_time: 0.0020  memory: 2416  grad_norm: 30.9939  loss: 0.7535  loss_cls: 0.1824  loss_mask: 0.0915  loss_dice: 0.4796
2025/06/24 12:45:51 - mmengine - INFO - Epoch(train)  [1][ 560/3769]  base_lr: 5.6000e-05 lr: 2.8000e-06  eta: 3:28:06  time: 0.3260  data_time: 0.0047  memory: 2445  grad_norm: 90.8030  loss: 1.0795  loss_cls: 0.3044  loss_mask: 0.1011  loss_dice: 0.6740
2025/06/24 12:45:54 - mmengine - INFO - Epoch(train)  [1][ 570/3769]  base_lr: 5.7000e-05 lr: 2.8500e-06  eta: 3:27:54  time: 0.3237  data_time: 0.0029  memory: 2255  grad_norm: 151.3489  loss: 1.0871  loss_cls: 0.2805  loss_mask: 0.0643  loss_dice: 0.7423
2025/06/24 12:45:58 - mmengine - INFO - Epoch(train)  [1][ 580/3769]  base_lr: 5.8000e-05 lr: 2.9000e-06  eta: 3:27:42  time: 0.3215  data_time: 0.0063  memory: 2152  grad_norm: 53.8824  loss: 0.4879  loss_cls: 0.1346  loss_mask: 0.0271  loss_dice: 0.3262
2025/06/24 12:46:01 - mmengine - INFO - Epoch(train)  [1][ 590/3769]  base_lr: 5.9000e-05 lr: 2.9500e-06  eta: 3:27:33  time: 0.3269  data_time: 0.0045  memory: 2349  grad_norm: 34.5748  loss: 1.1331  loss_cls: 0.2790  loss_mask: 0.0831  loss_dice: 0.7710
2025/06/24 12:46:04 - mmengine - INFO - Epoch(train)  [1][ 600/3769]  base_lr: 6.0000e-05 lr: 3.0000e-06  eta: 3:27:23  time: 0.3257  data_time: 0.0059  memory: 2276  grad_norm: 27.8100  loss: 1.0616  loss_cls: 0.1673  loss_mask: 0.1539  loss_dice: 0.7404
2025/06/24 12:46:07 - mmengine - INFO - Epoch(train)  [1][ 610/3769]  base_lr: 6.1000e-05 lr: 3.0500e-06  eta: 3:27:14  time: 0.3254  data_time: 0.0032  memory: 2171  grad_norm: 59.6427  loss: 0.9807  loss_cls: 0.2224  loss_mask: 0.1064  loss_dice: 0.6519
2025/06/24 12:46:11 - mmengine - INFO - Epoch(train)  [1][ 620/3769]  base_lr: 6.2000e-05 lr: 3.1000e-06  eta: 3:27:06  time: 0.3285  data_time: 0.0034  memory: 2467  grad_norm: 172.4493  loss: 1.1658  loss_cls: 0.3278  loss_mask: 0.1068  loss_dice: 0.7312
2025/06/24 12:46:14 - mmengine - INFO - Epoch(train)  [1][ 630/3769]  base_lr: 6.3000e-05 lr: 3.1500e-06  eta: 3:26:57  time: 0.3252  data_time: 0.0034  memory: 2481  grad_norm: 83.7253  loss: 0.9579  loss_cls: 0.1934  loss_mask: 0.0799  loss_dice: 0.6846
2025/06/24 12:46:17 - mmengine - INFO - Epoch(train)  [1][ 640/3769]  base_lr: 6.4000e-05 lr: 3.2000e-06  eta: 3:26:49  time: 0.3267  data_time: 0.0020  memory: 2356  grad_norm: 29.0940  loss: 1.1678  loss_cls: 0.2672  loss_mask: 0.1296  loss_dice: 0.7710
2025/06/24 12:46:21 - mmengine - INFO - Epoch(train)  [1][ 650/3769]  base_lr: 6.5000e-05 lr: 3.2500e-06  eta: 3:26:42  time: 0.3295  data_time: 0.0043  memory: 2540  grad_norm: 29.1416  loss: 0.9984  loss_cls: 0.2046  loss_mask: 0.0902  loss_dice: 0.7035
2025/06/24 12:46:24 - mmengine - INFO - Epoch(train)  [1][ 660/3769]  base_lr: 6.6000e-05 lr: 3.3000e-06  eta: 3:26:36  time: 0.3291  data_time: 0.0026  memory: 2554  grad_norm: 22.8813  loss: 0.8181  loss_cls: 0.2065  loss_mask: 0.0943  loss_dice: 0.5172
2025/06/24 12:46:27 - mmengine - INFO - Epoch(train)  [1][ 670/3769]  base_lr: 6.7000e-05 lr: 3.3500e-06  eta: 3:26:29  time: 0.3294  data_time: 0.0040  memory: 2510  grad_norm: 22.8438  loss: 1.5994  loss_cls: 0.3646  loss_mask: 0.1114  loss_dice: 1.1235
2025/06/24 12:46:30 - mmengine - INFO - Epoch(train)  [1][ 680/3769]  base_lr: 6.8000e-05 lr: 3.4000e-06  eta: 3:26:22  time: 0.3272  data_time: 0.0018  memory: 2518  grad_norm: 55.4639  loss: 0.7999  loss_cls: 0.2226  loss_mask: 0.0944  loss_dice: 0.4830
2025/06/24 12:46:34 - mmengine - INFO - Epoch(train)  [1][ 690/3769]  base_lr: 6.9000e-05 lr: 3.4500e-06  eta: 3:26:14  time: 0.3252  data_time: 0.0011  memory: 2342  grad_norm: 54.9198  loss: 1.9688  loss_cls: 0.4935  loss_mask: 0.4054  loss_dice: 1.0699
2025/06/24 12:46:37 - mmengine - INFO - Epoch(train)  [1][ 700/3769]  base_lr: 7.0000e-05 lr: 3.5000e-06  eta: 3:26:05  time: 0.3250  data_time: 0.0055  memory: 2276  grad_norm: 83.8908  loss: 1.0768  loss_cls: 0.2737  loss_mask: 0.0619  loss_dice: 0.7412
2025/06/24 12:46:40 - mmengine - INFO - Epoch(train)  [1][ 710/3769]  base_lr: 7.1000e-05 lr: 3.5500e-06  eta: 3:25:58  time: 0.3266  data_time: 0.0028  memory: 2276  grad_norm: 72.4266  loss: 1.0039  loss_cls: 0.2622  loss_mask: 0.0940  loss_dice: 0.6478
2025/06/24 12:46:43 - mmengine - INFO - Epoch(train)  [1][ 720/3769]  base_lr: 7.2000e-05 lr: 3.6000e-06  eta: 3:25:52  time: 0.3289  data_time: 0.0010  memory: 2532  grad_norm: 28.1060  loss: 0.9375  loss_cls: 0.2239  loss_mask: 0.0787  loss_dice: 0.6349
2025/06/24 12:46:47 - mmengine - INFO - Epoch(train)  [1][ 730/3769]  base_lr: 7.3000e-05 lr: 3.6500e-06  eta: 3:25:45  time: 0.3269  data_time: 0.0033  memory: 2320  grad_norm: 42.6476  loss: 1.1297  loss_cls: 0.2646  loss_mask: 0.1033  loss_dice: 0.7618
2025/06/24 12:46:50 - mmengine - INFO - Epoch(train)  [1][ 740/3769]  base_lr: 7.4000e-05 lr: 3.7000e-06  eta: 3:25:39  time: 0.3294  data_time: 0.0024  memory: 2452  grad_norm: 56.1661  loss: 1.0696  loss_cls: 0.2473  loss_mask: 0.1072  loss_dice: 0.7151
2025/06/24 12:46:53 - mmengine - INFO - Epoch(train)  [1][ 750/3769]  base_lr: 7.5000e-05 lr: 3.7500e-06  eta: 3:25:32  time: 0.3263  data_time: 0.0034  memory: 2291  grad_norm: 46.7075  loss: 1.0472  loss_cls: 0.2159  loss_mask: 0.0602  loss_dice: 0.7712
2025/06/24 12:46:57 - mmengine - INFO - Epoch(train)  [1][ 760/3769]  base_lr: 7.6000e-05 lr: 3.8000e-06  eta: 3:25:25  time: 0.3260  data_time: 0.0045  memory: 2356  grad_norm: 65.7657  loss: 0.9768  loss_cls: 0.2280  loss_mask: 0.0970  loss_dice: 0.6519
2025/06/24 12:47:00 - mmengine - INFO - Epoch(train)  [1][ 770/3769]  base_lr: 7.7000e-05 lr: 3.8500e-06  eta: 3:25:20  time: 0.3298  data_time: 0.0031  memory: 2467  grad_norm: 121.6574  loss: 1.2410  loss_cls: 0.3264  loss_mask: 0.1884  loss_dice: 0.7262
2025/06/24 12:47:03 - mmengine - INFO - Epoch(train)  [1][ 780/3769]  base_lr: 7.8000e-05 lr: 3.9000e-06  eta: 3:25:12  time: 0.3233  data_time: 0.0007  memory: 2213  grad_norm: 28.6418  loss: 0.6584  loss_cls: 0.1357  loss_mask: 0.0475  loss_dice: 0.4751
2025/06/24 12:47:06 - mmengine - INFO - Epoch(train)  [1][ 790/3769]  base_lr: 7.9000e-05 lr: 3.9500e-06  eta: 3:25:04  time: 0.3240  data_time: 0.0020  memory: 2134  grad_norm: 185.0696  loss: 0.9432  loss_cls: 0.1673  loss_mask: 0.1161  loss_dice: 0.6598
2025/06/24 12:47:10 - mmengine - INFO - Epoch(train)  [1][ 800/3769]  base_lr: 8.0000e-05 lr: 4.0000e-06  eta: 3:24:57  time: 0.3268  data_time: 0.0050  memory: 2219  grad_norm: 43.2387  loss: 1.0595  loss_cls: 0.2454  loss_mask: 0.0648  loss_dice: 0.7493
2025/06/24 12:47:13 - mmengine - INFO - Epoch(train)  [1][ 810/3769]  base_lr: 8.1000e-05 lr: 4.0500e-06  eta: 3:24:51  time: 0.3274  data_time: 0.0039  memory: 2438  grad_norm: 53.5358  loss: 0.8957  loss_cls: 0.2096  loss_mask: 0.0931  loss_dice: 0.5930
2025/06/24 12:47:16 - mmengine - INFO - Epoch(train)  [1][ 820/3769]  base_lr: 8.2000e-05 lr: 4.1000e-06  eta: 3:24:45  time: 0.3270  data_time: 0.0019  memory: 2276  grad_norm: 144.0407  loss: 0.7972  loss_cls: 0.1521  loss_mask: 0.0868  loss_dice: 0.5584
2025/06/24 12:47:19 - mmengine - INFO - Epoch(train)  [1][ 830/3769]  base_lr: 8.3000e-05 lr: 4.1500e-06  eta: 3:24:40  time: 0.3292  data_time: 0.0040  memory: 2364  grad_norm: 141.2386  loss: 1.2370  loss_cls: 0.2796  loss_mask: 0.0796  loss_dice: 0.8778
2025/06/24 12:47:23 - mmengine - INFO - Epoch(train)  [1][ 840/3769]  base_lr: 8.4000e-05 lr: 4.2000e-06  eta: 3:24:33  time: 0.3239  data_time: 0.0029  memory: 2462  grad_norm: 152.7951  loss: 1.7336  loss_cls: 0.1518  loss_mask: 0.6484  loss_dice: 0.9334
2025/06/24 12:47:26 - mmengine - INFO - Epoch(train)  [1][ 850/3769]  base_lr: 8.5000e-05 lr: 4.2500e-06  eta: 3:24:27  time: 0.3282  data_time: 0.0021  memory: 2430  grad_norm: 125.5912  loss: 0.9901  loss_cls: 0.2171  loss_mask: 0.2036  loss_dice: 0.5694
2025/06/24 12:47:29 - mmengine - INFO - Epoch(train)  [1][ 860/3769]  base_lr: 8.6000e-05 lr: 4.3000e-06  eta: 3:24:21  time: 0.3253  data_time: 0.0021  memory: 2284  grad_norm: 39.6847  loss: 0.9369  loss_cls: 0.1151  loss_mask: 0.1937  loss_dice: 0.6281
2025/06/24 12:47:33 - mmengine - INFO - Epoch(train)  [1][ 870/3769]  base_lr: 8.7000e-05 lr: 4.3500e-06  eta: 3:24:16  time: 0.3309  data_time: 0.0036  memory: 2305  grad_norm: 83.4750  loss: 1.2783  loss_cls: 0.3685  loss_mask: 0.0948  loss_dice: 0.8151
2025/06/24 12:47:36 - mmengine - INFO - Epoch(train)  [1][ 880/3769]  base_lr: 8.8000e-05 lr: 4.4000e-06  eta: 3:24:12  time: 0.3296  data_time: 0.0028  memory: 2291  grad_norm: 52.4157  loss: 0.9860  loss_cls: 0.1791  loss_mask: 0.0872  loss_dice: 0.7197
2025/06/24 12:47:39 - mmengine - INFO - Epoch(train)  [1][ 890/3769]  base_lr: 8.9000e-05 lr: 4.4500e-06  eta: 3:24:07  time: 0.3304  data_time: 0.0031  memory: 2320  grad_norm: 155.2505  loss: 1.2614  loss_cls: 0.3836  loss_mask: 0.0913  loss_dice: 0.7864
2025/06/24 12:47:42 - mmengine - INFO - Epoch(train)  [1][ 900/3769]  base_lr: 9.0000e-05 lr: 4.5000e-06  eta: 3:24:04  time: 0.3324  data_time: 0.0013  memory: 2320  grad_norm: 179.2210  loss: 1.4235  loss_cls: 0.2971  loss_mask: 0.1636  loss_dice: 0.9629
2025/06/24 12:47:46 - mmengine - INFO - Epoch(train)  [1][ 910/3769]  base_lr: 9.1000e-05 lr: 4.5500e-06  eta: 3:24:01  time: 0.3339  data_time: 0.0063  memory: 2349  grad_norm: 119.8446  loss: 0.9722  loss_cls: 0.2496  loss_mask: 0.0721  loss_dice: 0.6504
2025/06/24 12:47:49 - mmengine - INFO - Epoch(train)  [1][ 920/3769]  base_lr: 9.2000e-05 lr: 4.6000e-06  eta: 3:23:55  time: 0.3264  data_time: 0.0033  memory: 2219  grad_norm: 21.4717  loss: 0.8773  loss_cls: 0.2055  loss_mask: 0.0664  loss_dice: 0.6054
2025/06/24 12:47:52 - mmengine - INFO - Epoch(train)  [1][ 930/3769]  base_lr: 9.3000e-05 lr: 4.6500e-06  eta: 3:23:52  time: 0.3344  data_time: 0.0050  memory: 2438  grad_norm: 143.7027  loss: 1.3718  loss_cls: 0.4999  loss_mask: 0.0874  loss_dice: 0.7846
2025/06/24 12:47:56 - mmengine - INFO - Epoch(train)  [1][ 940/3769]  base_lr: 9.4000e-05 lr: 4.7000e-06  eta: 3:23:47  time: 0.3276  data_time: 0.0022  memory: 2407  grad_norm: 148.1184  loss: 0.8944  loss_cls: 0.1965  loss_mask: 0.0804  loss_dice: 0.6175
2025/06/24 12:47:59 - mmengine - INFO - Epoch(train)  [1][ 950/3769]  base_lr: 9.5000e-05 lr: 4.7500e-06  eta: 3:23:40  time: 0.3241  data_time: 0.0008  memory: 2298  grad_norm: 19.2241  loss: 0.5561  loss_cls: 0.1210  loss_mask: 0.0552  loss_dice: 0.3798
2025/06/24 12:48:02 - mmengine - INFO - Epoch(train)  [1][ 960/3769]  base_lr: 9.6000e-05 lr: 4.8000e-06  eta: 3:23:36  time: 0.3299  data_time: 0.0048  memory: 2489  grad_norm: 22.5880  loss: 1.0464  loss_cls: 0.2576  loss_mask: 0.0813  loss_dice: 0.7075
2025/06/24 12:48:06 - mmengine - INFO - Epoch(train)  [1][ 970/3769]  base_lr: 9.7000e-05 lr: 4.8500e-06  eta: 3:23:32  time: 0.3297  data_time: 0.0032  memory: 2452  grad_norm: 45.6053  loss: 1.2311  loss_cls: 0.2637  loss_mask: 0.1718  loss_dice: 0.7956
2025/06/24 12:48:09 - mmengine - INFO - Epoch(train)  [1][ 980/3769]  base_lr: 9.8000e-05 lr: 4.9000e-06  eta: 3:23:29  time: 0.3348  data_time: 0.0043  memory: 2430  grad_norm: 51.4535  loss: 1.3199  loss_cls: 0.3083  loss_mask: 0.1017  loss_dice: 0.9099
2025/06/24 12:48:12 - mmengine - INFO - Epoch(train)  [1][ 990/3769]  base_lr: 9.9000e-05 lr: 4.9500e-06  eta: 3:23:25  time: 0.3307  data_time: 0.0056  memory: 2554  grad_norm: 29.5107  loss: 1.1740  loss_cls: 0.2484  loss_mask: 0.1048  loss_dice: 0.8208
2025/06/24 12:48:15 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250624_124223
2025/06/24 12:48:15 - mmengine - INFO - Epoch(train)  [1][1000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:20  time: 0.3275  data_time: 0.0019  memory: 2213  grad_norm: 196.0536  loss: 1.0095  loss_cls: 0.2881  loss_mask: 0.0953  loss_dice: 0.6261
2025/06/24 12:48:19 - mmengine - INFO - Epoch(train)  [1][1010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:17  time: 0.3335  data_time: 0.0045  memory: 2481  grad_norm: 46.0795  loss: 1.0563  loss_cls: 0.3097  loss_mask: 0.0510  loss_dice: 0.6956
2025/06/24 12:48:22 - mmengine - INFO - Epoch(train)  [1][1020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:13  time: 0.3301  data_time: 0.0017  memory: 2438  grad_norm: 14.8251  loss: 0.7013  loss_cls: 0.1577  loss_mask: 0.0668  loss_dice: 0.4767
2025/06/24 12:48:25 - mmengine - INFO - Epoch(train)  [1][1030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:08  time: 0.3291  data_time: 0.0032  memory: 2164  grad_norm: 58.4003  loss: 0.9476  loss_cls: 0.2582  loss_mask: 0.1274  loss_dice: 0.5621
2025/06/24 12:48:29 - mmengine - INFO - Epoch(train)  [1][1040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:23:03  time: 0.3283  data_time: 0.0028  memory: 2385  grad_norm: 23.5263  loss: 1.2772  loss_cls: 0.1943  loss_mask: 0.0954  loss_dice: 0.9876
2025/06/24 12:48:32 - mmengine - INFO - Epoch(train)  [1][1050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:58  time: 0.3258  data_time: 0.0023  memory: 2334  grad_norm: 33.9271  loss: 1.2817  loss_cls: 0.2919  loss_mask: 0.0807  loss_dice: 0.9091
2025/06/24 12:48:35 - mmengine - INFO - Epoch(train)  [1][1060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:52  time: 0.3257  data_time: 0.0005  memory: 2207  grad_norm: 30.4418  loss: 1.1940  loss_cls: 0.2765  loss_mask: 0.1396  loss_dice: 0.7778
2025/06/24 12:48:38 - mmengine - INFO - Epoch(train)  [1][1070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:46  time: 0.3235  data_time: 0.0020  memory: 2195  grad_norm: 29.3083  loss: 0.8285  loss_cls: 0.1669  loss_mask: 0.0718  loss_dice: 0.5898
2025/06/24 12:48:42 - mmengine - INFO - Epoch(train)  [1][1080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:40  time: 0.3261  data_time: 0.0044  memory: 2213  grad_norm: 21.7302  loss: 0.9576  loss_cls: 0.2610  loss_mask: 0.0684  loss_dice: 0.6281
2025/06/24 12:48:45 - mmengine - INFO - Epoch(train)  [1][1090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:38  time: 0.3359  data_time: 0.0033  memory: 2467  grad_norm: 22.1506  loss: 0.7861  loss_cls: 0.1785  loss_mask: 0.0555  loss_dice: 0.5521
2025/06/24 12:48:48 - mmengine - INFO - Epoch(train)  [1][1100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:34  time: 0.3301  data_time: 0.0033  memory: 2232  grad_norm: 23.5554  loss: 0.8576  loss_cls: 0.1790  loss_mask: 0.0583  loss_dice: 0.6204
2025/06/24 12:48:52 - mmengine - INFO - Epoch(train)  [1][1110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:31  time: 0.3311  data_time: 0.0018  memory: 2503  grad_norm: 17.6046  loss: 1.4347  loss_cls: 0.2364  loss_mask: 0.1093  loss_dice: 1.0890
2025/06/24 12:48:55 - mmengine - INFO - Epoch(train)  [1][1120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:26  time: 0.3286  data_time: 0.0009  memory: 2532  grad_norm: 56.2846  loss: 1.0063  loss_cls: 0.2633  loss_mask: 0.1038  loss_dice: 0.6393
2025/06/24 12:48:58 - mmengine - INFO - Epoch(train)  [1][1130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:23  time: 0.3332  data_time: 0.0028  memory: 2255  grad_norm: 22.3056  loss: 0.5731  loss_cls: 0.1583  loss_mask: 0.0456  loss_dice: 0.3692
2025/06/24 12:49:02 - mmengine - INFO - Epoch(train)  [1][1140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:22  time: 0.3389  data_time: 0.0041  memory: 2525  grad_norm: 107.4842  loss: 0.8590  loss_cls: 0.1850  loss_mask: 0.1044  loss_dice: 0.5696
2025/06/24 12:49:05 - mmengine - INFO - Epoch(train)  [1][1150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:18  time: 0.3307  data_time: 0.0016  memory: 2334  grad_norm: 146.4993  loss: 1.4295  loss_cls: 0.3497  loss_mask: 0.1897  loss_dice: 0.8902
2025/06/24 12:49:08 - mmengine - INFO - Epoch(train)  [1][1160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:16  time: 0.3357  data_time: 0.0026  memory: 2298  grad_norm: 72.4932  loss: 1.1626  loss_cls: 0.2884  loss_mask: 0.1078  loss_dice: 0.7664
2025/06/24 12:49:12 - mmengine - INFO - Epoch(train)  [1][1170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:10  time: 0.3251  data_time: 0.0021  memory: 2284  grad_norm: 21.6539  loss: 0.9260  loss_cls: 0.1925  loss_mask: 0.0938  loss_dice: 0.6397
2025/06/24 12:49:15 - mmengine - INFO - Epoch(train)  [1][1180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:05  time: 0.3260  data_time: 0.0018  memory: 2276  grad_norm: 126.5723  loss: 0.9613  loss_cls: 0.2443  loss_mask: 0.0973  loss_dice: 0.6196
2025/06/24 12:49:18 - mmengine - INFO - Epoch(train)  [1][1190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:22:01  time: 0.3282  data_time: 0.0014  memory: 2445  grad_norm: 36.9220  loss: 1.2964  loss_cls: 0.2611  loss_mask: 0.1493  loss_dice: 0.8860
2025/06/24 12:49:21 - mmengine - INFO - Epoch(train)  [1][1200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:58  time: 0.3339  data_time: 0.0035  memory: 2327  grad_norm: 31.3306  loss: 0.7948  loss_cls: 0.1524  loss_mask: 0.0805  loss_dice: 0.5619
2025/06/24 12:49:25 - mmengine - INFO - Epoch(train)  [1][1210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:53  time: 0.3285  data_time: 0.0043  memory: 2378  grad_norm: 38.3787  loss: 1.0125  loss_cls: 0.2531  loss_mask: 0.1265  loss_dice: 0.6329
2025/06/24 12:49:28 - mmengine - INFO - Epoch(train)  [1][1220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:48  time: 0.3266  data_time: 0.0048  memory: 2284  grad_norm: 20.0865  loss: 0.6337  loss_cls: 0.1762  loss_mask: 0.0604  loss_dice: 0.3971
2025/06/24 12:49:31 - mmengine - INFO - Epoch(train)  [1][1230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:43  time: 0.3251  data_time: 0.0029  memory: 2452  grad_norm: 33.0491  loss: 1.1240  loss_cls: 0.3037  loss_mask: 0.0667  loss_dice: 0.7536
2025/06/24 12:49:34 - mmengine - INFO - Epoch(train)  [1][1240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:38  time: 0.3255  data_time: 0.0058  memory: 2342  grad_norm: 17.0508  loss: 0.9416  loss_cls: 0.2463  loss_mask: 0.0728  loss_dice: 0.6224
2025/06/24 12:49:38 - mmengine - INFO - Epoch(train)  [1][1250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:33  time: 0.3258  data_time: 0.0013  memory: 2510  grad_norm: 23.7642  loss: 0.9279  loss_cls: 0.1787  loss_mask: 0.0696  loss_dice: 0.6796
2025/06/24 12:49:41 - mmengine - INFO - Epoch(train)  [1][1260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:28  time: 0.3257  data_time: 0.0008  memory: 2320  grad_norm: 29.5578  loss: 1.2112  loss_cls: 0.2627  loss_mask: 0.1116  loss_dice: 0.8369
2025/06/24 12:49:44 - mmengine - INFO - Epoch(train)  [1][1270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:23  time: 0.3277  data_time: 0.0017  memory: 2284  grad_norm: 36.2206  loss: 1.0308  loss_cls: 0.2882  loss_mask: 0.0678  loss_dice: 0.6748
2025/06/24 12:49:48 - mmengine - INFO - Epoch(train)  [1][1280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:19  time: 0.3303  data_time: 0.0049  memory: 2364  grad_norm: 35.3787  loss: 0.9858  loss_cls: 0.2417  loss_mask: 0.0793  loss_dice: 0.6648
2025/06/24 12:49:51 - mmengine - INFO - Epoch(train)  [1][1290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:15  time: 0.3286  data_time: 0.0013  memory: 2291  grad_norm: 33.2802  loss: 1.0221  loss_cls: 0.2295  loss_mask: 0.0785  loss_dice: 0.7141
2025/06/24 12:49:54 - mmengine - INFO - Epoch(train)  [1][1300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:10  time: 0.3241  data_time: 0.0021  memory: 2481  grad_norm: 29.3106  loss: 0.8350  loss_cls: 0.1760  loss_mask: 0.0695  loss_dice: 0.5895
2025/06/24 12:49:57 - mmengine - INFO - Epoch(train)  [1][1310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:04  time: 0.3236  data_time: 0.0021  memory: 2183  grad_norm: 43.5273  loss: 1.1715  loss_cls: 0.2564  loss_mask: 0.0968  loss_dice: 0.8184
2025/06/24 12:50:01 - mmengine - INFO - Epoch(train)  [1][1320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:21:00  time: 0.3290  data_time: 0.0022  memory: 2497  grad_norm: 60.0186  loss: 1.6432  loss_cls: 0.4388  loss_mask: 0.1361  loss_dice: 1.0682
2025/06/24 12:50:04 - mmengine - INFO - Epoch(train)  [1][1330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:55  time: 0.3266  data_time: 0.0019  memory: 2312  grad_norm: 18.9150  loss: 0.9780  loss_cls: 0.1960  loss_mask: 0.0621  loss_dice: 0.7200
2025/06/24 12:50:07 - mmengine - INFO - Epoch(train)  [1][1340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:50  time: 0.3256  data_time: 0.0031  memory: 2225  grad_norm: 33.7228  loss: 0.9919  loss_cls: 0.2673  loss_mask: 0.0715  loss_dice: 0.6531
2025/06/24 12:50:10 - mmengine - INFO - Epoch(train)  [1][1350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:46  time: 0.3279  data_time: 0.0017  memory: 2349  grad_norm: 32.8091  loss: 1.2032  loss_cls: 0.2532  loss_mask: 0.0737  loss_dice: 0.8763
2025/06/24 12:50:14 - mmengine - INFO - Epoch(train)  [1][1360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:43  time: 0.3318  data_time: 0.0026  memory: 2255  grad_norm: 29.4374  loss: 1.1988  loss_cls: 0.2578  loss_mask: 0.0704  loss_dice: 0.8706
2025/06/24 12:50:17 - mmengine - INFO - Epoch(train)  [1][1370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:40  time: 0.3314  data_time: 0.0015  memory: 2371  grad_norm: 40.0087  loss: 1.8041  loss_cls: 0.3120  loss_mask: 0.2874  loss_dice: 1.2046
2025/06/24 12:50:20 - mmengine - INFO - Epoch(train)  [1][1380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:35  time: 0.3264  data_time: 0.0033  memory: 2518  grad_norm: 101.3937  loss: 1.1387  loss_cls: 0.3317  loss_mask: 0.0787  loss_dice: 0.7283
2025/06/24 12:50:24 - mmengine - INFO - Epoch(train)  [1][1390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:29  time: 0.3226  data_time: 0.0010  memory: 2237  grad_norm: 24.4932  loss: 0.8703  loss_cls: 0.1703  loss_mask: 0.1239  loss_dice: 0.5762
2025/06/24 12:50:27 - mmengine - INFO - Epoch(train)  [1][1400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:24  time: 0.3242  data_time: 0.0018  memory: 2312  grad_norm: 52.7585  loss: 1.1716  loss_cls: 0.1905  loss_mask: 0.0832  loss_dice: 0.8979
2025/06/24 12:50:30 - mmengine - INFO - Epoch(train)  [1][1410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:20  time: 0.3298  data_time: 0.0013  memory: 2068  grad_norm: 89.4342  loss: 0.9095  loss_cls: 0.1937  loss_mask: 0.0754  loss_dice: 0.6404
2025/06/24 12:50:33 - mmengine - INFO - Epoch(train)  [1][1420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:16  time: 0.3282  data_time: 0.0041  memory: 2430  grad_norm: 176.4020  loss: 1.3063  loss_cls: 0.3520  loss_mask: 0.1514  loss_dice: 0.8029
2025/06/24 12:50:37 - mmengine - INFO - Epoch(train)  [1][1430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:11  time: 0.3240  data_time: 0.0034  memory: 2489  grad_norm: 75.8870  loss: 1.1548  loss_cls: 0.2113  loss_mask: 0.0741  loss_dice: 0.8693
2025/06/24 12:50:40 - mmengine - INFO - Epoch(train)  [1][1440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:06  time: 0.3244  data_time: 0.0017  memory: 2122  grad_norm: 140.4336  loss: 1.1222  loss_cls: 0.1459  loss_mask: 0.0983  loss_dice: 0.8780
2025/06/24 12:50:43 - mmengine - INFO - Epoch(train)  [1][1450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:20:00  time: 0.3217  data_time: 0.0014  memory: 2158  grad_norm: 148.9041  loss: 0.9224  loss_cls: 0.2462  loss_mask: 0.0706  loss_dice: 0.6056
2025/06/24 12:50:46 - mmengine - INFO - Epoch(train)  [1][1460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:56  time: 0.3283  data_time: 0.0030  memory: 2481  grad_norm: 67.5828  loss: 0.7706  loss_cls: 0.2157  loss_mask: 0.0837  loss_dice: 0.4712
2025/06/24 12:50:50 - mmengine - INFO - Epoch(train)  [1][1470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:52  time: 0.3267  data_time: 0.0018  memory: 2416  grad_norm: 65.4553  loss: 1.2952  loss_cls: 0.3010  loss_mask: 0.1806  loss_dice: 0.8136
2025/06/24 12:50:53 - mmengine - INFO - Epoch(train)  [1][1480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:48  time: 0.3284  data_time: 0.0019  memory: 2298  grad_norm: 35.2204  loss: 1.2761  loss_cls: 0.2313  loss_mask: 0.1107  loss_dice: 0.9340
2025/06/24 12:50:56 - mmengine - INFO - Epoch(train)  [1][1490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:43  time: 0.3251  data_time: 0.0034  memory: 2364  grad_norm: 68.2463  loss: 0.9363  loss_cls: 0.1873  loss_mask: 0.0828  loss_dice: 0.6662
2025/06/24 12:50:59 - mmengine - INFO - Epoch(train)  [1][1500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:38  time: 0.3232  data_time: 0.0025  memory: 2237  grad_norm: 69.4176  loss: 1.3453  loss_cls: 0.2106  loss_mask: 0.3098  loss_dice: 0.8248
2025/06/24 12:51:03 - mmengine - INFO - Epoch(train)  [1][1510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:33  time: 0.3243  data_time: 0.0019  memory: 2334  grad_norm: 163.7852  loss: 1.1980  loss_cls: 0.2085  loss_mask: 0.0974  loss_dice: 0.8921
2025/06/24 12:51:06 - mmengine - INFO - Epoch(train)  [1][1520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:27  time: 0.3207  data_time: 0.0019  memory: 2201  grad_norm: 62.9088  loss: 0.9170  loss_cls: 0.1569  loss_mask: 0.0915  loss_dice: 0.6685
2025/06/24 12:51:09 - mmengine - INFO - Epoch(train)  [1][1530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:22  time: 0.3233  data_time: 0.0028  memory: 2098  grad_norm: 67.9102  loss: 0.5530  loss_cls: 0.0911  loss_mask: 0.1068  loss_dice: 0.3551
2025/06/24 12:51:12 - mmengine - INFO - Epoch(train)  [1][1540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:17  time: 0.3227  data_time: 0.0014  memory: 2140  grad_norm: 40.9004  loss: 1.0883  loss_cls: 0.3008  loss_mask: 0.0853  loss_dice: 0.7022
2025/06/24 12:51:16 - mmengine - INFO - Epoch(train)  [1][1550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:12  time: 0.3223  data_time: 0.0019  memory: 2110  grad_norm: 63.5116  loss: 1.0048  loss_cls: 0.2008  loss_mask: 0.0959  loss_dice: 0.7081
2025/06/24 12:51:19 - mmengine - INFO - Epoch(train)  [1][1560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:06  time: 0.3217  data_time: 0.0016  memory: 2195  grad_norm: 41.3731  loss: 0.8927  loss_cls: 0.1195  loss_mask: 0.0908  loss_dice: 0.6824
2025/06/24 12:51:22 - mmengine - INFO - Epoch(train)  [1][1570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:19:01  time: 0.3233  data_time: 0.0022  memory: 2312  grad_norm: 210.0287  loss: 1.0193  loss_cls: 0.2021  loss_mask: 0.1035  loss_dice: 0.7137
2025/06/24 12:51:25 - mmengine - INFO - Epoch(train)  [1][1580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:57  time: 0.3249  data_time: 0.0032  memory: 2219  grad_norm: 90.9129  loss: 1.9833  loss_cls: 0.3843  loss_mask: 0.2162  loss_dice: 1.3829
2025/06/24 12:51:28 - mmengine - INFO - Epoch(train)  [1][1590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:53  time: 0.3272  data_time: 0.0022  memory: 2356  grad_norm: 130.6906  loss: 1.2028  loss_cls: 0.2196  loss_mask: 0.1267  loss_dice: 0.8566
2025/06/24 12:51:32 - mmengine - INFO - Epoch(train)  [1][1600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:48  time: 0.3269  data_time: 0.0036  memory: 2371  grad_norm: 148.7608  loss: 1.3644  loss_cls: 0.2666  loss_mask: 0.1078  loss_dice: 0.9901
2025/06/24 12:51:35 - mmengine - INFO - Epoch(train)  [1][1610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:43  time: 0.3199  data_time: 0.0013  memory: 2116  grad_norm: 55.5633  loss: 0.9283  loss_cls: 0.1852  loss_mask: 0.1179  loss_dice: 0.6252
2025/06/24 12:51:38 - mmengine - INFO - Epoch(train)  [1][1620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:38  time: 0.3225  data_time: 0.0040  memory: 2093  grad_norm: 21.0746  loss: 0.7222  loss_cls: 0.1272  loss_mask: 0.0737  loss_dice: 0.5213
2025/06/24 12:51:41 - mmengine - INFO - Epoch(train)  [1][1630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:32  time: 0.3216  data_time: 0.0012  memory: 2057  grad_norm: 153.7499  loss: 1.7733  loss_cls: 0.2362  loss_mask: 0.3830  loss_dice: 1.1541
2025/06/24 12:51:45 - mmengine - INFO - Epoch(train)  [1][1640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:28  time: 0.3250  data_time: 0.0028  memory: 2298  grad_norm: 89.5014  loss: 0.7315  loss_cls: 0.1526  loss_mask: 0.0504  loss_dice: 0.5286
2025/06/24 12:51:48 - mmengine - INFO - Epoch(train)  [1][1650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:24  time: 0.3275  data_time: 0.0013  memory: 2237  grad_norm: 38.0025  loss: 1.0728  loss_cls: 0.2398  loss_mask: 0.0879  loss_dice: 0.7451
2025/06/24 12:51:51 - mmengine - INFO - Epoch(train)  [1][1660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:21  time: 0.3291  data_time: 0.0036  memory: 2356  grad_norm: 89.7675  loss: 1.1679  loss_cls: 0.2442  loss_mask: 0.1062  loss_dice: 0.8175
2025/06/24 12:51:55 - mmengine - INFO - Epoch(train)  [1][1670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:17  time: 0.3298  data_time: 0.0062  memory: 2334  grad_norm: 62.7939  loss: 1.0787  loss_cls: 0.2068  loss_mask: 0.0992  loss_dice: 0.7727
2025/06/24 12:51:58 - mmengine - INFO - Epoch(train)  [1][1680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:13  time: 0.3276  data_time: 0.0015  memory: 2116  grad_norm: 17.5298  loss: 0.7028  loss_cls: 0.0895  loss_mask: 0.0499  loss_dice: 0.5635
2025/06/24 12:52:01 - mmengine - INFO - Epoch(train)  [1][1690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:11  time: 0.3340  data_time: 0.0027  memory: 2320  grad_norm: 122.1962  loss: 1.5746  loss_cls: 0.2465  loss_mask: 0.2066  loss_dice: 1.1214
2025/06/24 12:52:04 - mmengine - INFO - Epoch(train)  [1][1700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:08  time: 0.3343  data_time: 0.0018  memory: 2269  grad_norm: 84.2102  loss: 1.1155  loss_cls: 0.2865  loss_mask: 0.0646  loss_dice: 0.7644
2025/06/24 12:52:08 - mmengine - INFO - Epoch(train)  [1][1710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:05  time: 0.3285  data_time: 0.0043  memory: 2393  grad_norm: 63.4507  loss: 0.7881  loss_cls: 0.1885  loss_mask: 0.0675  loss_dice: 0.5321
2025/06/24 12:52:11 - mmengine - INFO - Epoch(train)  [1][1720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:18:01  time: 0.3276  data_time: 0.0034  memory: 2189  grad_norm: 31.0762  loss: 0.7553  loss_cls: 0.1546  loss_mask: 0.0862  loss_dice: 0.5146
2025/06/24 12:52:14 - mmengine - INFO - Epoch(train)  [1][1730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:56  time: 0.3254  data_time: 0.0016  memory: 2364  grad_norm: 54.6288  loss: 0.9850  loss_cls: 0.2770  loss_mask: 0.0791  loss_dice: 0.6289
2025/06/24 12:52:18 - mmengine - INFO - Epoch(train)  [1][1740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:53  time: 0.3287  data_time: 0.0011  memory: 2503  grad_norm: 72.8949  loss: 1.0246  loss_cls: 0.2020  loss_mask: 0.0992  loss_dice: 0.7234
2025/06/24 12:52:21 - mmengine - INFO - Epoch(train)  [1][1750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:49  time: 0.3269  data_time: 0.0017  memory: 2276  grad_norm: 50.4053  loss: 1.1868  loss_cls: 0.2588  loss_mask: 0.0895  loss_dice: 0.8384
2025/06/24 12:52:24 - mmengine - INFO - Epoch(train)  [1][1760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:46  time: 0.3314  data_time: 0.0006  memory: 2452  grad_norm: 32.5621  loss: 1.1750  loss_cls: 0.2896  loss_mask: 0.0802  loss_dice: 0.8052
2025/06/24 12:52:27 - mmengine - INFO - Epoch(train)  [1][1770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:41  time: 0.3253  data_time: 0.0023  memory: 2183  grad_norm: 45.1840  loss: 1.5580  loss_cls: 0.3338  loss_mask: 0.0744  loss_dice: 1.1498
2025/06/24 12:52:31 - mmengine - INFO - Epoch(train)  [1][1780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:37  time: 0.3273  data_time: 0.0028  memory: 2356  grad_norm: 48.9048  loss: 1.1495  loss_cls: 0.2385  loss_mask: 0.0864  loss_dice: 0.8246
2025/06/24 12:52:34 - mmengine - INFO - Epoch(train)  [1][1790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:33  time: 0.3260  data_time: 0.0038  memory: 2327  grad_norm: 58.3880  loss: 1.3212  loss_cls: 0.2726  loss_mask: 0.1557  loss_dice: 0.8929
2025/06/24 12:52:37 - mmengine - INFO - Epoch(train)  [1][1800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:28  time: 0.3219  data_time: 0.0020  memory: 2269  grad_norm: 37.1557  loss: 0.8205  loss_cls: 0.2171  loss_mask: 0.0796  loss_dice: 0.5238
2025/06/24 12:52:40 - mmengine - INFO - Epoch(train)  [1][1810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:25  time: 0.3295  data_time: 0.0021  memory: 2503  grad_norm: 98.4372  loss: 1.0451  loss_cls: 0.2954  loss_mask: 0.0855  loss_dice: 0.6643
2025/06/24 12:52:44 - mmengine - INFO - Epoch(train)  [1][1820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:21  time: 0.3266  data_time: 0.0013  memory: 2371  grad_norm: 99.4447  loss: 1.2473  loss_cls: 0.2575  loss_mask: 0.1218  loss_dice: 0.8679
2025/06/24 12:52:47 - mmengine - INFO - Epoch(train)  [1][1830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:16  time: 0.3234  data_time: 0.0019  memory: 2134  grad_norm: 53.4163  loss: 1.0071  loss_cls: 0.1932  loss_mask: 0.1244  loss_dice: 0.6895
2025/06/24 12:52:50 - mmengine - INFO - Epoch(train)  [1][1840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:13  time: 0.3283  data_time: 0.0006  memory: 2276  grad_norm: 98.7358  loss: 1.3413  loss_cls: 0.3125  loss_mask: 0.1649  loss_dice: 0.8640
2025/06/24 12:52:54 - mmengine - INFO - Epoch(train)  [1][1850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:09  time: 0.3268  data_time: 0.0010  memory: 2291  grad_norm: 95.3797  loss: 0.7442  loss_cls: 0.1312  loss_mask: 0.0799  loss_dice: 0.5331
2025/06/24 12:52:57 - mmengine - INFO - Epoch(train)  [1][1860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:05  time: 0.3292  data_time: 0.0031  memory: 2320  grad_norm: 75.2673  loss: 1.0140  loss_cls: 0.1904  loss_mask: 0.0850  loss_dice: 0.7386
2025/06/24 12:53:00 - mmengine - INFO - Epoch(train)  [1][1870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:17:00  time: 0.3223  data_time: 0.0018  memory: 2349  grad_norm: 68.3796  loss: 1.1488  loss_cls: 0.2825  loss_mask: 0.0727  loss_dice: 0.7936
2025/06/24 12:53:03 - mmengine - INFO - Epoch(train)  [1][1880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:57  time: 0.3277  data_time: 0.0020  memory: 2093  grad_norm: 24.6576  loss: 1.1063  loss_cls: 0.1580  loss_mask: 0.0987  loss_dice: 0.8496
2025/06/24 12:53:07 - mmengine - INFO - Epoch(train)  [1][1890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:53  time: 0.3295  data_time: 0.0017  memory: 2467  grad_norm: 101.0638  loss: 1.1349  loss_cls: 0.3099  loss_mask: 0.0714  loss_dice: 0.7536
2025/06/24 12:53:10 - mmengine - INFO - Epoch(train)  [1][1900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:49  time: 0.3266  data_time: 0.0017  memory: 2407  grad_norm: 64.8291  loss: 0.9965  loss_cls: 0.1408  loss_mask: 0.0880  loss_dice: 0.7677
2025/06/24 12:53:13 - mmengine - INFO - Epoch(train)  [1][1910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:45  time: 0.3245  data_time: 0.0039  memory: 2225  grad_norm: 71.6738  loss: 1.0612  loss_cls: 0.2313  loss_mask: 0.0884  loss_dice: 0.7416
2025/06/24 12:53:16 - mmengine - INFO - Epoch(train)  [1][1920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:41  time: 0.3252  data_time: 0.0039  memory: 2298  grad_norm: 69.0982  loss: 1.0385  loss_cls: 0.2864  loss_mask: 0.1177  loss_dice: 0.6345
2025/06/24 12:53:20 - mmengine - INFO - Epoch(train)  [1][1930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:37  time: 0.3272  data_time: 0.0024  memory: 2312  grad_norm: 69.2949  loss: 1.1617  loss_cls: 0.2167  loss_mask: 0.0976  loss_dice: 0.8473
2025/06/24 12:53:23 - mmengine - INFO - Epoch(train)  [1][1940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:33  time: 0.3276  data_time: 0.0023  memory: 2503  grad_norm: 50.8392  loss: 1.1273  loss_cls: 0.2196  loss_mask: 0.1656  loss_dice: 0.7421
2025/06/24 12:53:26 - mmengine - INFO - Epoch(train)  [1][1950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:29  time: 0.3268  data_time: 0.0006  memory: 2423  grad_norm: 107.5224  loss: 1.0854  loss_cls: 0.2073  loss_mask: 0.0785  loss_dice: 0.7996
2025/06/24 12:53:30 - mmengine - INFO - Epoch(train)  [1][1960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:26  time: 0.3298  data_time: 0.0030  memory: 2378  grad_norm: 38.1191  loss: 1.2394  loss_cls: 0.2772  loss_mask: 0.1504  loss_dice: 0.8118
2025/06/24 12:53:33 - mmengine - INFO - Epoch(train)  [1][1970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:22  time: 0.3259  data_time: 0.0016  memory: 2378  grad_norm: 87.9247  loss: 1.2572  loss_cls: 0.3025  loss_mask: 0.1886  loss_dice: 0.7662
2025/06/24 12:53:36 - mmengine - INFO - Epoch(train)  [1][1980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:18  time: 0.3225  data_time: 0.0030  memory: 2195  grad_norm: 71.4173  loss: 0.8850  loss_cls: 0.1167  loss_mask: 0.1378  loss_dice: 0.6306
2025/06/24 12:53:39 - mmengine - INFO - Epoch(train)  [1][1990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:15  time: 0.3317  data_time: 0.0032  memory: 2525  grad_norm: 93.6716  loss: 1.6880  loss_cls: 0.4498  loss_mask: 0.1434  loss_dice: 1.0948
2025/06/24 12:53:43 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250624_124223
2025/06/24 12:53:43 - mmengine - INFO - Epoch(train)  [1][2000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:10  time: 0.3250  data_time: 0.0018  memory: 2201  grad_norm: 92.1618  loss: 1.4009  loss_cls: 0.4165  loss_mask: 0.1601  loss_dice: 0.8243
2025/06/24 12:53:46 - mmengine - INFO - Epoch(train)  [1][2010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:07  time: 0.3304  data_time: 0.0018  memory: 2540  grad_norm: 88.4283  loss: 1.3680  loss_cls: 0.2937  loss_mask: 0.3245  loss_dice: 0.7498
2025/06/24 12:53:49 - mmengine - INFO - Epoch(train)  [1][2020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:16:03  time: 0.3275  data_time: 0.0020  memory: 2276  grad_norm: 49.9740  loss: 1.3751  loss_cls: 0.2678  loss_mask: 0.1683  loss_dice: 0.9390
2025/06/24 12:53:52 - mmengine - INFO - Epoch(train)  [1][2030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:59  time: 0.3228  data_time: 0.0021  memory: 2385  grad_norm: 36.9883  loss: 0.7664  loss_cls: 0.1941  loss_mask: 0.0774  loss_dice: 0.4948
2025/06/24 12:53:56 - mmengine - INFO - Epoch(train)  [1][2040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:55  time: 0.3234  data_time: 0.0025  memory: 2445  grad_norm: 58.9241  loss: 1.1741  loss_cls: 0.2779  loss_mask: 0.0633  loss_dice: 0.8329
2025/06/24 12:53:59 - mmengine - INFO - Epoch(train)  [1][2050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:50  time: 0.3218  data_time: 0.0034  memory: 2116  grad_norm: 49.9482  loss: 1.0446  loss_cls: 0.2051  loss_mask: 0.1479  loss_dice: 0.6916
2025/06/24 12:54:02 - mmengine - INFO - Epoch(train)  [1][2060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:46  time: 0.3252  data_time: 0.0019  memory: 2312  grad_norm: 49.5356  loss: 1.6116  loss_cls: 0.3294  loss_mask: 0.1518  loss_dice: 1.1305
2025/06/24 12:54:05 - mmengine - INFO - Epoch(train)  [1][2070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:42  time: 0.3256  data_time: 0.0017  memory: 2356  grad_norm: 108.7295  loss: 1.1550  loss_cls: 0.2522  loss_mask: 0.1772  loss_dice: 0.7256
2025/06/24 12:54:09 - mmengine - INFO - Epoch(train)  [1][2080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:38  time: 0.3250  data_time: 0.0036  memory: 2342  grad_norm: 36.1548  loss: 0.5770  loss_cls: 0.1531  loss_mask: 0.0443  loss_dice: 0.3796
2025/06/24 12:54:12 - mmengine - INFO - Epoch(train)  [1][2090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:34  time: 0.3299  data_time: 0.0012  memory: 2407  grad_norm: 177.1748  loss: 1.1958  loss_cls: 0.3806  loss_mask: 0.1356  loss_dice: 0.6797
2025/06/24 12:54:15 - mmengine - INFO - Epoch(train)  [1][2100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:30  time: 0.3241  data_time: 0.0021  memory: 2334  grad_norm: 32.3366  loss: 0.9469  loss_cls: 0.1692  loss_mask: 0.1167  loss_dice: 0.6610
2025/06/24 12:54:18 - mmengine - INFO - Epoch(train)  [1][2110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:27  time: 0.3291  data_time: 0.0030  memory: 2518  grad_norm: 50.9770  loss: 1.3402  loss_cls: 0.2506  loss_mask: 0.1219  loss_dice: 0.9677
2025/06/24 12:54:22 - mmengine - INFO - Epoch(train)  [1][2120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:24  time: 0.3296  data_time: 0.0035  memory: 2327  grad_norm: 72.8835  loss: 1.1154  loss_cls: 0.2351  loss_mask: 0.1152  loss_dice: 0.7652
2025/06/24 12:54:25 - mmengine - INFO - Epoch(train)  [1][2130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:20  time: 0.3256  data_time: 0.0015  memory: 2152  grad_norm: 47.8996  loss: 1.5332  loss_cls: 0.2501  loss_mask: 0.1205  loss_dice: 1.1627
2025/06/24 12:54:28 - mmengine - INFO - Epoch(train)  [1][2140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:17  time: 0.3366  data_time: 0.0011  memory: 2356  grad_norm: 35.5934  loss: 1.2252  loss_cls: 0.2708  loss_mask: 0.1496  loss_dice: 0.8049
2025/06/24 12:54:32 - mmengine - INFO - Epoch(train)  [1][2150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:17  time: 0.3486  data_time: 0.0049  memory: 2305  grad_norm: 82.1294  loss: 1.0596  loss_cls: 0.2424  loss_mask: 0.1098  loss_dice: 0.7074
2025/06/24 12:54:35 - mmengine - INFO - Epoch(train)  [1][2160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:14  time: 0.3290  data_time: 0.0019  memory: 2320  grad_norm: 73.0237  loss: 1.4624  loss_cls: 0.3687  loss_mask: 0.1864  loss_dice: 0.9073
2025/06/24 12:54:38 - mmengine - INFO - Epoch(train)  [1][2170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:10  time: 0.3258  data_time: 0.0031  memory: 2207  grad_norm: 88.5568  loss: 1.1077  loss_cls: 0.2950  loss_mask: 0.0707  loss_dice: 0.7420
2025/06/24 12:54:42 - mmengine - INFO - Epoch(train)  [1][2180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:06  time: 0.3256  data_time: 0.0014  memory: 2140  grad_norm: 40.1395  loss: 0.6622  loss_cls: 0.1742  loss_mask: 0.0683  loss_dice: 0.4198
2025/06/24 12:54:45 - mmengine - INFO - Epoch(train)  [1][2190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:03  time: 0.3328  data_time: 0.0025  memory: 2378  grad_norm: 56.3426  loss: 1.4534  loss_cls: 0.4453  loss_mask: 0.1273  loss_dice: 0.8807
2025/06/24 12:54:48 - mmengine - INFO - Epoch(train)  [1][2200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:15:00  time: 0.3278  data_time: 0.0023  memory: 2305  grad_norm: 63.0719  loss: 1.3136  loss_cls: 0.2490  loss_mask: 0.1656  loss_dice: 0.8990
2025/06/24 12:54:52 - mmengine - INFO - Epoch(train)  [1][2210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:56  time: 0.3294  data_time: 0.0024  memory: 2445  grad_norm: 97.3816  loss: 1.4961  loss_cls: 0.2523  loss_mask: 0.2965  loss_dice: 0.9473
2025/06/24 12:54:55 - mmengine - INFO - Epoch(train)  [1][2220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:52  time: 0.3268  data_time: 0.0022  memory: 2371  grad_norm: 61.3541  loss: 1.1818  loss_cls: 0.3077  loss_mask: 0.1094  loss_dice: 0.7647
2025/06/24 12:54:58 - mmengine - INFO - Epoch(train)  [1][2230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:49  time: 0.3269  data_time: 0.0033  memory: 2298  grad_norm: 171.0876  loss: 1.1340  loss_cls: 0.2763  loss_mask: 0.1122  loss_dice: 0.7455
2025/06/24 12:55:01 - mmengine - INFO - Epoch(train)  [1][2240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:45  time: 0.3261  data_time: 0.0019  memory: 2474  grad_norm: 56.5482  loss: 1.4454  loss_cls: 0.3204  loss_mask: 0.0999  loss_dice: 1.0251
2025/06/24 12:55:05 - mmengine - INFO - Epoch(train)  [1][2250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:41  time: 0.3249  data_time: 0.0010  memory: 2342  grad_norm: 70.9763  loss: 1.0823  loss_cls: 0.2666  loss_mask: 0.0828  loss_dice: 0.7328
2025/06/24 12:55:08 - mmengine - INFO - Epoch(train)  [1][2260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:37  time: 0.3279  data_time: 0.0022  memory: 2334  grad_norm: 59.1287  loss: 1.3327  loss_cls: 0.4034  loss_mask: 0.0974  loss_dice: 0.8320
2025/06/24 12:55:11 - mmengine - INFO - Epoch(train)  [1][2270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:34  time: 0.3284  data_time: 0.0014  memory: 2371  grad_norm: 84.2080  loss: 1.2647  loss_cls: 0.2612  loss_mask: 0.1268  loss_dice: 0.8768
2025/06/24 12:55:14 - mmengine - INFO - Epoch(train)  [1][2280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:30  time: 0.3250  data_time: 0.0042  memory: 2164  grad_norm: 36.0491  loss: 1.1654  loss_cls: 0.2497  loss_mask: 0.1262  loss_dice: 0.7895
2025/06/24 12:55:18 - mmengine - INFO - Epoch(train)  [1][2290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:25  time: 0.3208  data_time: 0.0038  memory: 2074  grad_norm: 18.0041  loss: 0.7690  loss_cls: 0.1504  loss_mask: 0.0961  loss_dice: 0.5225
2025/06/24 12:55:21 - mmengine - INFO - Epoch(train)  [1][2300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:21  time: 0.3241  data_time: 0.0029  memory: 2334  grad_norm: 146.4084  loss: 1.6936  loss_cls: 0.3848  loss_mask: 0.1233  loss_dice: 1.1856
2025/06/24 12:55:24 - mmengine - INFO - Epoch(train)  [1][2310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:17  time: 0.3244  data_time: 0.0018  memory: 2269  grad_norm: 99.5299  loss: 0.9950  loss_cls: 0.2532  loss_mask: 0.0911  loss_dice: 0.6507
2025/06/24 12:55:27 - mmengine - INFO - Epoch(train)  [1][2320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:13  time: 0.3240  data_time: 0.0021  memory: 2213  grad_norm: 96.3960  loss: 1.6116  loss_cls: 0.2248  loss_mask: 0.0963  loss_dice: 1.2904
2025/06/24 12:55:31 - mmengine - INFO - Epoch(train)  [1][2330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:08  time: 0.3222  data_time: 0.0023  memory: 2284  grad_norm: 358.7357  loss: 1.3271  loss_cls: 0.2571  loss_mask: 0.1494  loss_dice: 0.9206
2025/06/24 12:55:34 - mmengine - INFO - Epoch(train)  [1][2340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:04  time: 0.3213  data_time: 0.0020  memory: 2237  grad_norm: 153.3643  loss: 1.2228  loss_cls: 0.1917  loss_mask: 0.2411  loss_dice: 0.7899
2025/06/24 12:55:37 - mmengine - INFO - Epoch(train)  [1][2350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:14:00  time: 0.3258  data_time: 0.0028  memory: 2467  grad_norm: 146.6736  loss: 1.4031  loss_cls: 0.2680  loss_mask: 0.1466  loss_dice: 0.9885
2025/06/24 12:55:40 - mmengine - INFO - Epoch(train)  [1][2360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:56  time: 0.3259  data_time: 0.0011  memory: 2371  grad_norm: 62.3832  loss: 1.5647  loss_cls: 0.3215  loss_mask: 0.2576  loss_dice: 0.9856
2025/06/24 12:55:44 - mmengine - INFO - Epoch(train)  [1][2370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:52  time: 0.3247  data_time: 0.0052  memory: 2356  grad_norm: 24.7446  loss: 1.1372  loss_cls: 0.2897  loss_mask: 0.0912  loss_dice: 0.7562
2025/06/24 12:55:47 - mmengine - INFO - Epoch(train)  [1][2380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:48  time: 0.3265  data_time: 0.0019  memory: 2371  grad_norm: 51.2140  loss: 1.4826  loss_cls: 0.4051  loss_mask: 0.1031  loss_dice: 0.9744
2025/06/24 12:55:50 - mmengine - INFO - Epoch(train)  [1][2390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:45  time: 0.3294  data_time: 0.0026  memory: 2164  grad_norm: 125.4504  loss: 1.2466  loss_cls: 0.2267  loss_mask: 0.0946  loss_dice: 0.9252
2025/06/24 12:55:53 - mmengine - INFO - Epoch(train)  [1][2400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:41  time: 0.3259  data_time: 0.0024  memory: 2237  grad_norm: 82.0319  loss: 1.1157  loss_cls: 0.2751  loss_mask: 0.0839  loss_dice: 0.7567
2025/06/24 12:55:57 - mmengine - INFO - Epoch(train)  [1][2410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:38  time: 0.3255  data_time: 0.0026  memory: 2284  grad_norm: 169.4012  loss: 1.1858  loss_cls: 0.3069  loss_mask: 0.1345  loss_dice: 0.7444
2025/06/24 12:56:00 - mmengine - INFO - Epoch(train)  [1][2420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:34  time: 0.3258  data_time: 0.0015  memory: 2349  grad_norm: 109.7957  loss: 1.8315  loss_cls: 0.4165  loss_mask: 0.2011  loss_dice: 1.2138
2025/06/24 12:56:03 - mmengine - INFO - Epoch(train)  [1][2430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:30  time: 0.3269  data_time: 0.0050  memory: 2400  grad_norm: 75.2276  loss: 2.1277  loss_cls: 0.4764  loss_mask: 0.1037  loss_dice: 1.5476
2025/06/24 12:56:06 - mmengine - INFO - Epoch(train)  [1][2440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:26  time: 0.3217  data_time: 0.0051  memory: 2305  grad_norm: 50.0761  loss: 1.1850  loss_cls: 0.2244  loss_mask: 0.1579  loss_dice: 0.8027
2025/06/24 12:56:10 - mmengine - INFO - Epoch(train)  [1][2450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:22  time: 0.3269  data_time: 0.0030  memory: 2416  grad_norm: 300.0345  loss: 1.5727  loss_cls: 0.3458  loss_mask: 0.1300  loss_dice: 1.0969
2025/06/24 12:56:13 - mmengine - INFO - Epoch(train)  [1][2460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:18  time: 0.3229  data_time: 0.0011  memory: 2237  grad_norm: 82.7608  loss: 1.6771  loss_cls: 0.3846  loss_mask: 0.1112  loss_dice: 1.1813
2025/06/24 12:56:16 - mmengine - INFO - Epoch(train)  [1][2470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:14  time: 0.3255  data_time: 0.0037  memory: 2503  grad_norm: 102.2520  loss: 1.3542  loss_cls: 0.3376  loss_mask: 0.0943  loss_dice: 0.9223
2025/06/24 12:56:19 - mmengine - INFO - Epoch(train)  [1][2480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:10  time: 0.3247  data_time: 0.0007  memory: 2269  grad_norm: 56.0541  loss: 1.1692  loss_cls: 0.2691  loss_mask: 0.1159  loss_dice: 0.7842
2025/06/24 12:56:23 - mmengine - INFO - Epoch(train)  [1][2490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:06  time: 0.3231  data_time: 0.0048  memory: 2298  grad_norm: 189.4174  loss: 1.3047  loss_cls: 0.3303  loss_mask: 0.1012  loss_dice: 0.8732
2025/06/24 12:56:26 - mmengine - INFO - Epoch(train)  [1][2500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:13:02  time: 0.3258  data_time: 0.0025  memory: 2371  grad_norm: 102.9257  loss: 1.5056  loss_cls: 0.4199  loss_mask: 0.2010  loss_dice: 0.8846
2025/06/24 12:56:29 - mmengine - INFO - Epoch(train)  [1][2510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:58  time: 0.3219  data_time: 0.0013  memory: 2110  grad_norm: 156.4009  loss: 1.1426  loss_cls: 0.3093  loss_mask: 0.1670  loss_dice: 0.6664
2025/06/24 12:56:32 - mmengine - INFO - Epoch(train)  [1][2520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:54  time: 0.3261  data_time: 0.0015  memory: 2481  grad_norm: 61.9892  loss: 1.1900  loss_cls: 0.3084  loss_mask: 0.0942  loss_dice: 0.7874
2025/06/24 12:56:36 - mmengine - INFO - Epoch(train)  [1][2530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:51  time: 0.3272  data_time: 0.0010  memory: 2518  grad_norm: 121.9183  loss: 1.5313  loss_cls: 0.3975  loss_mask: 0.1431  loss_dice: 0.9906
2025/06/24 12:56:39 - mmengine - INFO - Epoch(train)  [1][2540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:47  time: 0.3248  data_time: 0.0016  memory: 2320  grad_norm: 39.7348  loss: 1.2065  loss_cls: 0.3101  loss_mask: 0.1362  loss_dice: 0.7602
2025/06/24 12:56:42 - mmengine - INFO - Epoch(train)  [1][2550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:43  time: 0.3273  data_time: 0.0023  memory: 2393  grad_norm: 94.0434  loss: 1.2705  loss_cls: 0.3148  loss_mask: 0.1208  loss_dice: 0.8349
2025/06/24 12:56:45 - mmengine - INFO - Epoch(train)  [1][2560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:40  time: 0.3268  data_time: 0.0020  memory: 2349  grad_norm: 232.4601  loss: 1.7690  loss_cls: 0.4177  loss_mask: 0.1394  loss_dice: 1.2119
2025/06/24 12:56:49 - mmengine - INFO - Epoch(train)  [1][2570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:36  time: 0.3233  data_time: 0.0009  memory: 2371  grad_norm: 156.3970  loss: 1.1091  loss_cls: 0.3029  loss_mask: 0.1432  loss_dice: 0.6630
2025/06/24 12:56:52 - mmengine - INFO - Epoch(train)  [1][2580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:32  time: 0.3261  data_time: 0.0020  memory: 2232  grad_norm: 80.6371  loss: 1.4722  loss_cls: 0.4080  loss_mask: 0.0877  loss_dice: 0.9764
2025/06/24 12:56:55 - mmengine - INFO - Epoch(train)  [1][2590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:28  time: 0.3224  data_time: 0.0026  memory: 2110  grad_norm: 80.5095  loss: 1.0758  loss_cls: 0.2629  loss_mask: 0.0892  loss_dice: 0.7237
2025/06/24 12:56:58 - mmengine - INFO - Epoch(train)  [1][2600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:24  time: 0.3267  data_time: 0.0024  memory: 2452  grad_norm: 80.7743  loss: 1.1519  loss_cls: 0.3058  loss_mask: 0.1265  loss_dice: 0.7197
2025/06/24 12:57:02 - mmengine - INFO - Epoch(train)  [1][2610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:21  time: 0.3283  data_time: 0.0033  memory: 2407  grad_norm: 183.1452  loss: 1.4022  loss_cls: 0.4115  loss_mask: 0.0902  loss_dice: 0.9006
2025/06/24 12:57:05 - mmengine - INFO - Epoch(train)  [1][2620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:17  time: 0.3284  data_time: 0.0019  memory: 2378  grad_norm: 53.5056  loss: 1.2195  loss_cls: 0.3143  loss_mask: 0.0856  loss_dice: 0.8196
2025/06/24 12:57:08 - mmengine - INFO - Epoch(train)  [1][2630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:14  time: 0.3322  data_time: 0.0046  memory: 2532  grad_norm: 177.6867  loss: 2.3318  loss_cls: 0.4715  loss_mask: 0.5803  loss_dice: 1.2800
2025/06/24 12:57:12 - mmengine - INFO - Epoch(train)  [1][2640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:11  time: 0.3304  data_time: 0.0033  memory: 2284  grad_norm: 58.7599  loss: 1.1986  loss_cls: 0.3436  loss_mask: 0.1074  loss_dice: 0.7476
2025/06/24 12:57:15 - mmengine - INFO - Epoch(train)  [1][2650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:08  time: 0.3297  data_time: 0.0012  memory: 2284  grad_norm: 118.0076  loss: 1.3180  loss_cls: 0.2829  loss_mask: 0.0672  loss_dice: 0.9678
2025/06/24 12:57:18 - mmengine - INFO - Epoch(train)  [1][2660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:05  time: 0.3292  data_time: 0.0007  memory: 2378  grad_norm: 68.7399  loss: 1.2719  loss_cls: 0.3333  loss_mask: 0.0928  loss_dice: 0.8458
2025/06/24 12:57:21 - mmengine - INFO - Epoch(train)  [1][2670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:12:02  time: 0.3319  data_time: 0.0033  memory: 2445  grad_norm: 75.6356  loss: 1.5637  loss_cls: 0.4210  loss_mask: 0.1367  loss_dice: 1.0060
2025/06/24 12:57:25 - mmengine - INFO - Epoch(train)  [1][2680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:59  time: 0.3310  data_time: 0.0053  memory: 2378  grad_norm: 80.9999  loss: 1.4900  loss_cls: 0.3621  loss_mask: 0.1541  loss_dice: 0.9738
2025/06/24 12:57:28 - mmengine - INFO - Epoch(train)  [1][2690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:55  time: 0.3249  data_time: 0.0029  memory: 2243  grad_norm: 186.1694  loss: 0.9088  loss_cls: 0.1519  loss_mask: 0.1026  loss_dice: 0.6543
2025/06/24 12:57:31 - mmengine - INFO - Epoch(train)  [1][2700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:52  time: 0.3290  data_time: 0.0019  memory: 2134  grad_norm: 43.2457  loss: 0.7230  loss_cls: 0.1848  loss_mask: 0.0829  loss_dice: 0.4553
2025/06/24 12:57:35 - mmengine - INFO - Epoch(train)  [1][2710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:49  time: 0.3332  data_time: 0.0029  memory: 2189  grad_norm: 76.0256  loss: 1.2406  loss_cls: 0.3061  loss_mask: 0.1174  loss_dice: 0.8171
2025/06/24 12:57:38 - mmengine - INFO - Epoch(train)  [1][2720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:46  time: 0.3327  data_time: 0.0022  memory: 2356  grad_norm: 55.8457  loss: 1.1322  loss_cls: 0.2751  loss_mask: 0.0736  loss_dice: 0.7835
2025/06/24 12:57:41 - mmengine - INFO - Epoch(train)  [1][2730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:44  time: 0.3362  data_time: 0.0044  memory: 2569  grad_norm: 44.0625  loss: 1.1474  loss_cls: 0.3173  loss_mask: 0.1249  loss_dice: 0.7051
2025/06/24 12:57:45 - mmengine - INFO - Epoch(train)  [1][2740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:41  time: 0.3346  data_time: 0.0024  memory: 2276  grad_norm: 193.1358  loss: 1.4530  loss_cls: 0.3182  loss_mask: 0.0982  loss_dice: 1.0365
2025/06/24 12:57:48 - mmengine - INFO - Epoch(train)  [1][2750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:39  time: 0.3341  data_time: 0.0026  memory: 2474  grad_norm: 62.7439  loss: 1.0199  loss_cls: 0.2850  loss_mask: 0.0932  loss_dice: 0.6416
2025/06/24 12:57:51 - mmengine - INFO - Epoch(train)  [1][2760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:36  time: 0.3309  data_time: 0.0016  memory: 2430  grad_norm: 40.2973  loss: 1.4597  loss_cls: 0.2690  loss_mask: 0.1311  loss_dice: 1.0596
2025/06/24 12:57:55 - mmengine - INFO - Epoch(train)  [1][2770/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:32  time: 0.3299  data_time: 0.0035  memory: 2334  grad_norm: 83.7881  loss: 1.3034  loss_cls: 0.2998  loss_mask: 0.0878  loss_dice: 0.9158
2025/06/24 12:57:58 - mmengine - INFO - Epoch(train)  [1][2780/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:30  time: 0.3337  data_time: 0.0042  memory: 2452  grad_norm: 76.0180  loss: 0.9677  loss_cls: 0.2066  loss_mask: 0.1068  loss_dice: 0.6543
2025/06/24 12:58:01 - mmengine - INFO - Epoch(train)  [1][2790/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:26  time: 0.3297  data_time: 0.0021  memory: 2237  grad_norm: 38.3871  loss: 0.8742  loss_cls: 0.2006  loss_mask: 0.0829  loss_dice: 0.5908
2025/06/24 12:58:05 - mmengine - INFO - Epoch(train)  [1][2800/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:23  time: 0.3283  data_time: 0.0013  memory: 2232  grad_norm: 216.0410  loss: 1.4662  loss_cls: 0.3668  loss_mask: 0.1243  loss_dice: 0.9752
2025/06/24 12:58:08 - mmengine - INFO - Epoch(train)  [1][2810/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:20  time: 0.3325  data_time: 0.0032  memory: 2445  grad_norm: 86.0071  loss: 1.2606  loss_cls: 0.2552  loss_mask: 0.0875  loss_dice: 0.9179
2025/06/24 12:58:11 - mmengine - INFO - Epoch(train)  [1][2820/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:17  time: 0.3275  data_time: 0.0023  memory: 2080  grad_norm: 103.4886  loss: 1.3373  loss_cls: 0.2021  loss_mask: 0.4288  loss_dice: 0.7065
2025/06/24 12:58:14 - mmengine - INFO - Epoch(train)  [1][2830/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:14  time: 0.3322  data_time: 0.0020  memory: 2298  grad_norm: 195.2091  loss: 1.9485  loss_cls: 0.3481  loss_mask: 0.1215  loss_dice: 1.4789
2025/06/24 12:58:18 - mmengine - INFO - Epoch(train)  [1][2840/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:10  time: 0.3247  data_time: 0.0014  memory: 2147  grad_norm: 78.5272  loss: 0.8290  loss_cls: 0.1866  loss_mask: 0.0728  loss_dice: 0.5696
2025/06/24 12:58:21 - mmengine - INFO - Epoch(train)  [1][2850/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:07  time: 0.3284  data_time: 0.0018  memory: 2467  grad_norm: 61.3568  loss: 1.0446  loss_cls: 0.2597  loss_mask: 0.1425  loss_dice: 0.6424
2025/06/24 12:58:24 - mmengine - INFO - Epoch(train)  [1][2860/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:11:03  time: 0.3263  data_time: 0.0047  memory: 2201  grad_norm: 40.5715  loss: 1.3329  loss_cls: 0.2483  loss_mask: 0.1063  loss_dice: 0.9783
2025/06/24 12:58:27 - mmengine - INFO - Epoch(train)  [1][2870/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:59  time: 0.3250  data_time: 0.0015  memory: 2213  grad_norm: 47.2470  loss: 0.7436  loss_cls: 0.1971  loss_mask: 0.0676  loss_dice: 0.4789
2025/06/24 12:58:31 - mmengine - INFO - Epoch(train)  [1][2880/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:55  time: 0.3263  data_time: 0.0007  memory: 2342  grad_norm: 244.5515  loss: 1.0649  loss_cls: 0.3217  loss_mask: 0.1305  loss_dice: 0.6127
2025/06/24 12:58:34 - mmengine - INFO - Epoch(train)  [1][2890/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:52  time: 0.3300  data_time: 0.0018  memory: 2489  grad_norm: 60.7392  loss: 1.3642  loss_cls: 0.3596  loss_mask: 0.1284  loss_dice: 0.8761
2025/06/24 12:58:37 - mmengine - INFO - Epoch(train)  [1][2900/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:49  time: 0.3287  data_time: 0.0024  memory: 2269  grad_norm: 135.0421  loss: 1.0720  loss_cls: 0.2723  loss_mask: 0.1118  loss_dice: 0.6879
2025/06/24 12:58:41 - mmengine - INFO - Epoch(train)  [1][2910/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:46  time: 0.3318  data_time: 0.0034  memory: 2207  grad_norm: 71.6297  loss: 1.4364  loss_cls: 0.4149  loss_mask: 0.1104  loss_dice: 0.9112
2025/06/24 12:58:44 - mmengine - INFO - Epoch(train)  [1][2920/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:43  time: 0.3283  data_time: 0.0009  memory: 2525  grad_norm: 110.7081  loss: 1.1316  loss_cls: 0.2344  loss_mask: 0.0846  loss_dice: 0.8126
2025/06/24 12:58:47 - mmengine - INFO - Epoch(train)  [1][2930/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:39  time: 0.3267  data_time: 0.0034  memory: 2481  grad_norm: 130.8110  loss: 1.9882  loss_cls: 0.3117  loss_mask: 0.7217  loss_dice: 0.9549
2025/06/24 12:58:50 - mmengine - INFO - Epoch(train)  [1][2940/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:36  time: 0.3282  data_time: 0.0019  memory: 2467  grad_norm: 89.6164  loss: 1.1626  loss_cls: 0.3565  loss_mask: 0.1457  loss_dice: 0.6603
2025/06/24 12:58:54 - mmengine - INFO - Epoch(train)  [1][2950/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:32  time: 0.3282  data_time: 0.0023  memory: 2284  grad_norm: 44.3683  loss: 1.1356  loss_cls: 0.2875  loss_mask: 0.0788  loss_dice: 0.7692
2025/06/24 12:58:57 - mmengine - INFO - Epoch(train)  [1][2960/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:29  time: 0.3263  data_time: 0.0016  memory: 2423  grad_norm: 201.1007  loss: 1.2310  loss_cls: 0.3120  loss_mask: 0.1217  loss_dice: 0.7972
2025/06/24 12:59:00 - mmengine - INFO - Epoch(train)  [1][2970/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:26  time: 0.3326  data_time: 0.0025  memory: 2364  grad_norm: 36.2766  loss: 0.8284  loss_cls: 0.2041  loss_mask: 0.0761  loss_dice: 0.5482
2025/06/24 12:59:04 - mmengine - INFO - Epoch(train)  [1][2980/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:25  time: 0.3483  data_time: 0.0032  memory: 2481  grad_norm: 78.2129  loss: 1.5999  loss_cls: 0.3811  loss_mask: 0.1642  loss_dice: 1.0546
2025/06/24 12:59:07 - mmengine - INFO - Epoch(train)  [1][2990/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:22  time: 0.3358  data_time: 0.0017  memory: 2378  grad_norm: 156.7620  loss: 1.2702  loss_cls: 0.3303  loss_mask: 0.1119  loss_dice: 0.8279
2025/06/24 12:59:11 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250624_124223
2025/06/24 12:59:11 - mmengine - INFO - Epoch(train)  [1][3000/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:20  time: 0.3374  data_time: 0.0042  memory: 2327  grad_norm: 44.9783  loss: 1.5774  loss_cls: 0.3392  loss_mask: 0.1579  loss_dice: 1.0803
2025/06/24 12:59:14 - mmengine - INFO - Epoch(train)  [1][3010/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:17  time: 0.3347  data_time: 0.0033  memory: 2152  grad_norm: 29.6103  loss: 1.3992  loss_cls: 0.3725  loss_mask: 0.1491  loss_dice: 0.8776
2025/06/24 12:59:17 - mmengine - INFO - Epoch(train)  [1][3020/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:14  time: 0.3284  data_time: 0.0039  memory: 2497  grad_norm: 63.0514  loss: 1.4706  loss_cls: 0.2950  loss_mask: 0.2387  loss_dice: 0.9369
2025/06/24 12:59:21 - mmengine - INFO - Epoch(train)  [1][3030/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:12  time: 0.3395  data_time: 0.0023  memory: 2385  grad_norm: 55.1077  loss: 1.5804  loss_cls: 0.4343  loss_mask: 0.1018  loss_dice: 1.0443
2025/06/24 12:59:24 - mmengine - INFO - Epoch(train)  [1][3040/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:09  time: 0.3309  data_time: 0.0015  memory: 2305  grad_norm: 45.1366  loss: 1.0757  loss_cls: 0.3078  loss_mask: 0.0794  loss_dice: 0.6885
2025/06/24 12:59:27 - mmengine - INFO - Epoch(train)  [1][3050/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:05  time: 0.3242  data_time: 0.0024  memory: 2243  grad_norm: 49.4428  loss: 1.1343  loss_cls: 0.2269  loss_mask: 0.0862  loss_dice: 0.8212
2025/06/24 12:59:30 - mmengine - INFO - Epoch(train)  [1][3060/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:10:01  time: 0.3275  data_time: 0.0029  memory: 2334  grad_norm: 53.6760  loss: 1.2501  loss_cls: 0.2811  loss_mask: 0.1753  loss_dice: 0.7938
2025/06/24 12:59:34 - mmengine - INFO - Epoch(train)  [1][3070/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:57  time: 0.3237  data_time: 0.0045  memory: 2342  grad_norm: 95.8290  loss: 0.9693  loss_cls: 0.2580  loss_mask: 0.1399  loss_dice: 0.5714
2025/06/24 12:59:37 - mmengine - INFO - Epoch(train)  [1][3080/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:55  time: 0.3339  data_time: 0.0017  memory: 2371  grad_norm: 48.5083  loss: 1.5534  loss_cls: 0.3564  loss_mask: 0.1230  loss_dice: 1.0740
2025/06/24 12:59:40 - mmengine - INFO - Epoch(train)  [1][3090/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:52  time: 0.3318  data_time: 0.0048  memory: 2327  grad_norm: 53.2185  loss: 1.6251  loss_cls: 0.3660  loss_mask: 0.1091  loss_dice: 1.1500
2025/06/24 12:59:44 - mmengine - INFO - Epoch(train)  [1][3100/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:48  time: 0.3279  data_time: 0.0039  memory: 2284  grad_norm: 72.4341  loss: 1.1369  loss_cls: 0.2944  loss_mask: 0.1243  loss_dice: 0.7181
2025/06/24 12:59:47 - mmengine - INFO - Epoch(train)  [1][3110/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:45  time: 0.3308  data_time: 0.0015  memory: 2364  grad_norm: 80.0377  loss: 1.2140  loss_cls: 0.2626  loss_mask: 0.1041  loss_dice: 0.8473
2025/06/24 12:59:50 - mmengine - INFO - Epoch(train)  [1][3120/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:42  time: 0.3310  data_time: 0.0030  memory: 2225  grad_norm: 220.6549  loss: 1.4709  loss_cls: 0.3855  loss_mask: 0.1403  loss_dice: 0.9450
2025/06/24 12:59:54 - mmengine - INFO - Epoch(train)  [1][3130/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:39  time: 0.3315  data_time: 0.0025  memory: 2276  grad_norm: 51.5128  loss: 1.5189  loss_cls: 0.3140  loss_mask: 0.1353  loss_dice: 1.0696
2025/06/24 12:59:57 - mmengine - INFO - Epoch(train)  [1][3140/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:36  time: 0.3352  data_time: 0.0032  memory: 2467  grad_norm: 45.6737  loss: 0.9152  loss_cls: 0.1687  loss_mask: 0.1152  loss_dice: 0.6312
2025/06/24 13:00:00 - mmengine - INFO - Epoch(train)  [1][3150/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:33  time: 0.3279  data_time: 0.0023  memory: 2503  grad_norm: 182.7827  loss: 1.9920  loss_cls: 0.3139  loss_mask: 0.4768  loss_dice: 1.2014
2025/06/24 13:00:03 - mmengine - INFO - Epoch(train)  [1][3160/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:29  time: 0.3293  data_time: 0.0028  memory: 2407  grad_norm: 78.7829  loss: 1.8032  loss_cls: 0.4887  loss_mask: 0.1415  loss_dice: 1.1731
2025/06/24 13:00:07 - mmengine - INFO - Epoch(train)  [1][3170/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:26  time: 0.3300  data_time: 0.0017  memory: 2371  grad_norm: 68.1739  loss: 1.9801  loss_cls: 0.4533  loss_mask: 0.2245  loss_dice: 1.3023
2025/06/24 13:00:10 - mmengine - INFO - Epoch(train)  [1][3180/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:23  time: 0.3330  data_time: 0.0013  memory: 2371  grad_norm: 34.3654  loss: 1.0557  loss_cls: 0.2395  loss_mask: 0.0703  loss_dice: 0.7459
2025/06/24 13:00:13 - mmengine - INFO - Epoch(train)  [1][3190/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:20  time: 0.3278  data_time: 0.0011  memory: 2183  grad_norm: 22.2228  loss: 1.5660  loss_cls: 0.2164  loss_mask: 0.2018  loss_dice: 1.1478
2025/06/24 13:00:17 - mmengine - INFO - Epoch(train)  [1][3200/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:17  time: 0.3307  data_time: 0.0029  memory: 2327  grad_norm: 39.1849  loss: 1.0375  loss_cls: 0.2807  loss_mask: 0.0716  loss_dice: 0.6852
2025/06/24 13:00:20 - mmengine - INFO - Epoch(train)  [1][3210/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:13  time: 0.3296  data_time: 0.0028  memory: 2438  grad_norm: 159.3041  loss: 1.0274  loss_cls: 0.2700  loss_mask: 0.1267  loss_dice: 0.6307
2025/06/24 13:00:23 - mmengine - INFO - Epoch(train)  [1][3220/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:10  time: 0.3289  data_time: 0.0042  memory: 2255  grad_norm: 30.2370  loss: 1.3972  loss_cls: 0.2845  loss_mask: 0.1167  loss_dice: 0.9960
2025/06/24 13:00:27 - mmengine - INFO - Epoch(train)  [1][3230/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:07  time: 0.3332  data_time: 0.0035  memory: 2462  grad_norm: 52.2191  loss: 1.6097  loss_cls: 0.3984  loss_mask: 0.1235  loss_dice: 1.0878
2025/06/24 13:00:30 - mmengine - INFO - Epoch(train)  [1][3240/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:04  time: 0.3262  data_time: 0.0026  memory: 2356  grad_norm: 22.6909  loss: 0.8404  loss_cls: 0.1639  loss_mask: 0.0915  loss_dice: 0.5850
2025/06/24 13:00:33 - mmengine - INFO - Epoch(train)  [1][3250/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:09:00  time: 0.3244  data_time: 0.0026  memory: 2378  grad_norm: 40.5859  loss: 0.8934  loss_cls: 0.2310  loss_mask: 0.0586  loss_dice: 0.6038
2025/06/24 13:00:36 - mmengine - INFO - Epoch(train)  [1][3260/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:56  time: 0.3269  data_time: 0.0033  memory: 2104  grad_norm: 38.2930  loss: 1.6669  loss_cls: 0.5710  loss_mask: 0.1137  loss_dice: 0.9822
2025/06/24 13:00:40 - mmengine - INFO - Epoch(train)  [1][3270/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:53  time: 0.3314  data_time: 0.0029  memory: 2312  grad_norm: 70.6959  loss: 1.0168  loss_cls: 0.3164  loss_mask: 0.0626  loss_dice: 0.6378
2025/06/24 13:00:43 - mmengine - INFO - Epoch(train)  [1][3280/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:51  time: 0.3419  data_time: 0.0022  memory: 2284  grad_norm: 35.7519  loss: 1.2829  loss_cls: 0.3077  loss_mask: 0.0935  loss_dice: 0.8817
2025/06/24 13:00:46 - mmengine - INFO - Epoch(train)  [1][3290/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:49  time: 0.3344  data_time: 0.0025  memory: 2371  grad_norm: 64.6939  loss: 1.2794  loss_cls: 0.3361  loss_mask: 0.1349  loss_dice: 0.8084
2025/06/24 13:00:50 - mmengine - INFO - Epoch(train)  [1][3300/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:45  time: 0.3283  data_time: 0.0033  memory: 2312  grad_norm: 143.7197  loss: 1.2454  loss_cls: 0.2338  loss_mask: 0.1728  loss_dice: 0.8388
2025/06/24 13:00:53 - mmengine - INFO - Epoch(train)  [1][3310/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:42  time: 0.3306  data_time: 0.0029  memory: 2385  grad_norm: 52.6489  loss: 1.5562  loss_cls: 0.3205  loss_mask: 0.1119  loss_dice: 1.1238
2025/06/24 13:00:56 - mmengine - INFO - Epoch(train)  [1][3320/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:38  time: 0.3264  data_time: 0.0031  memory: 2213  grad_norm: 104.9730  loss: 1.6920  loss_cls: 0.3541  loss_mask: 0.1735  loss_dice: 1.1644
2025/06/24 13:01:00 - mmengine - INFO - Epoch(train)  [1][3330/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:35  time: 0.3258  data_time: 0.0008  memory: 2147  grad_norm: 43.3146  loss: 1.8138  loss_cls: 0.4014  loss_mask: 0.2804  loss_dice: 1.1320
2025/06/24 13:01:03 - mmengine - INFO - Epoch(train)  [1][3340/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:31  time: 0.3235  data_time: 0.0024  memory: 2232  grad_norm: 15.0626  loss: 0.6241  loss_cls: 0.1349  loss_mask: 0.0360  loss_dice: 0.4531
2025/06/24 13:01:06 - mmengine - INFO - Epoch(train)  [1][3350/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:27  time: 0.3265  data_time: 0.0033  memory: 2122  grad_norm: 37.6539  loss: 1.1920  loss_cls: 0.2084  loss_mask: 0.1517  loss_dice: 0.8319
2025/06/24 13:01:09 - mmengine - INFO - Epoch(train)  [1][3360/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:24  time: 0.3283  data_time: 0.0025  memory: 2416  grad_norm: 23.8552  loss: 1.0068  loss_cls: 0.2385  loss_mask: 0.0356  loss_dice: 0.7327
2025/06/24 13:01:13 - mmengine - INFO - Epoch(train)  [1][3370/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:20  time: 0.3279  data_time: 0.0028  memory: 2371  grad_norm: 90.0797  loss: 1.0797  loss_cls: 0.2642  loss_mask: 0.1380  loss_dice: 0.6775
2025/06/24 13:01:16 - mmengine - INFO - Epoch(train)  [1][3380/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:17  time: 0.3260  data_time: 0.0016  memory: 2400  grad_norm: 38.3027  loss: 1.1650  loss_cls: 0.3229  loss_mask: 0.1158  loss_dice: 0.7264
2025/06/24 13:01:19 - mmengine - INFO - Epoch(train)  [1][3390/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:13  time: 0.3245  data_time: 0.0033  memory: 2152  grad_norm: 60.8799  loss: 0.7174  loss_cls: 0.1214  loss_mask: 0.0901  loss_dice: 0.5058
2025/06/24 13:01:22 - mmengine - INFO - Epoch(train)  [1][3400/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:09  time: 0.3219  data_time: 0.0042  memory: 2093  grad_norm: 33.4937  loss: 0.6392  loss_cls: 0.1304  loss_mask: 0.0752  loss_dice: 0.4336
2025/06/24 13:01:26 - mmengine - INFO - Epoch(train)  [1][3410/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:06  time: 0.3292  data_time: 0.0026  memory: 2249  grad_norm: 61.3045  loss: 1.2937  loss_cls: 0.2529  loss_mask: 0.1590  loss_dice: 0.8819
2025/06/24 13:01:29 - mmengine - INFO - Epoch(train)  [1][3420/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:08:02  time: 0.3274  data_time: 0.0022  memory: 2128  grad_norm: 32.6076  loss: 0.8396  loss_cls: 0.2047  loss_mask: 0.0777  loss_dice: 0.5572
2025/06/24 13:01:32 - mmengine - INFO - Epoch(train)  [1][3430/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:59  time: 0.3292  data_time: 0.0028  memory: 2356  grad_norm: 27.5375  loss: 1.0193  loss_cls: 0.2103  loss_mask: 0.0806  loss_dice: 0.7284
2025/06/24 13:01:35 - mmengine - INFO - Epoch(train)  [1][3440/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:55  time: 0.3278  data_time: 0.0015  memory: 2269  grad_norm: 39.8018  loss: 0.8491  loss_cls: 0.1854  loss_mask: 0.0926  loss_dice: 0.5711
2025/06/24 13:01:39 - mmengine - INFO - Epoch(train)  [1][3450/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:53  time: 0.3335  data_time: 0.0039  memory: 2356  grad_norm: 53.2283  loss: 1.6419  loss_cls: 0.3390  loss_mask: 0.1530  loss_dice: 1.1500
2025/06/24 13:01:42 - mmengine - INFO - Epoch(train)  [1][3460/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:49  time: 0.3304  data_time: 0.0031  memory: 2237  grad_norm: 28.1762  loss: 1.1777  loss_cls: 0.2929  loss_mask: 0.0898  loss_dice: 0.7949
2025/06/24 13:01:45 - mmengine - INFO - Epoch(train)  [1][3470/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:46  time: 0.3266  data_time: 0.0023  memory: 2128  grad_norm: 118.9820  loss: 1.2095  loss_cls: 0.3001  loss_mask: 0.0703  loss_dice: 0.8391
2025/06/24 13:01:49 - mmengine - INFO - Epoch(train)  [1][3480/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:42  time: 0.3285  data_time: 0.0038  memory: 2407  grad_norm: 38.9002  loss: 1.9404  loss_cls: 0.4089  loss_mask: 0.1539  loss_dice: 1.3775
2025/06/24 13:01:52 - mmengine - INFO - Epoch(train)  [1][3490/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:39  time: 0.3321  data_time: 0.0033  memory: 2503  grad_norm: 47.7664  loss: 1.7303  loss_cls: 0.4101  loss_mask: 0.1144  loss_dice: 1.2058
2025/06/24 13:01:55 - mmengine - INFO - Epoch(train)  [1][3500/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:36  time: 0.3295  data_time: 0.0023  memory: 2474  grad_norm: 76.8131  loss: 1.5770  loss_cls: 0.3961  loss_mask: 0.1136  loss_dice: 1.0673
2025/06/24 13:01:59 - mmengine - INFO - Epoch(train)  [1][3510/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:33  time: 0.3293  data_time: 0.0032  memory: 2305  grad_norm: 65.2561  loss: 1.6819  loss_cls: 0.4359  loss_mask: 0.1609  loss_dice: 1.0851
2025/06/24 13:02:02 - mmengine - INFO - Epoch(train)  [1][3520/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:29  time: 0.3262  data_time: 0.0046  memory: 2334  grad_norm: 40.6358  loss: 1.4265  loss_cls: 0.3128  loss_mask: 0.1152  loss_dice: 0.9985
2025/06/24 13:02:05 - mmengine - INFO - Epoch(train)  [1][3530/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:26  time: 0.3288  data_time: 0.0007  memory: 2298  grad_norm: 152.1711  loss: 1.3992  loss_cls: 0.4150  loss_mask: 0.1103  loss_dice: 0.8739
2025/06/24 13:02:08 - mmengine - INFO - Epoch(train)  [1][3540/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:23  time: 0.3274  data_time: 0.0033  memory: 2481  grad_norm: 28.0664  loss: 1.2754  loss_cls: 0.3201  loss_mask: 0.1259  loss_dice: 0.8294
2025/06/24 13:02:12 - mmengine - INFO - Epoch(train)  [1][3550/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:19  time: 0.3268  data_time: 0.0024  memory: 2276  grad_norm: 73.7517  loss: 1.7941  loss_cls: 0.4315  loss_mask: 0.1555  loss_dice: 1.2072
2025/06/24 13:02:15 - mmengine - INFO - Epoch(train)  [1][3560/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:16  time: 0.3280  data_time: 0.0033  memory: 2430  grad_norm: 34.9149  loss: 1.9413  loss_cls: 0.4047  loss_mask: 0.1303  loss_dice: 1.4063
2025/06/24 13:02:18 - mmengine - INFO - Epoch(train)  [1][3570/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:12  time: 0.3281  data_time: 0.0031  memory: 2284  grad_norm: 61.7535  loss: 1.5798  loss_cls: 0.2358  loss_mask: 0.1868  loss_dice: 1.1572
2025/06/24 13:02:22 - mmengine - INFO - Epoch(train)  [1][3580/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:10  time: 0.3355  data_time: 0.0034  memory: 2467  grad_norm: 63.2280  loss: 1.7666  loss_cls: 0.3881  loss_mask: 0.1045  loss_dice: 1.2740
2025/06/24 13:02:25 - mmengine - INFO - Epoch(train)  [1][3590/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:06  time: 0.3268  data_time: 0.0028  memory: 2195  grad_norm: 308.2510  loss: 1.6353  loss_cls: 0.4417  loss_mask: 0.1631  loss_dice: 1.0305
2025/06/24 13:02:28 - mmengine - INFO - Epoch(train)  [1][3600/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:03  time: 0.3277  data_time: 0.0027  memory: 2416  grad_norm: 39.8597  loss: 1.6126  loss_cls: 0.4157  loss_mask: 0.1002  loss_dice: 1.0967
2025/06/24 13:02:31 - mmengine - INFO - Epoch(train)  [1][3610/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:07:00  time: 0.3338  data_time: 0.0022  memory: 2532  grad_norm: 294.6269  loss: 2.1322  loss_cls: 0.5280  loss_mask: 0.1826  loss_dice: 1.4216
2025/06/24 13:02:35 - mmengine - INFO - Epoch(train)  [1][3620/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:56  time: 0.3215  data_time: 0.0014  memory: 2385  grad_norm: 38.0947  loss: 1.0511  loss_cls: 0.1912  loss_mask: 0.2707  loss_dice: 0.5892
2025/06/24 13:02:38 - mmengine - INFO - Epoch(train)  [1][3630/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:53  time: 0.3315  data_time: 0.0027  memory: 2298  grad_norm: 106.5733  loss: 1.8770  loss_cls: 0.5628  loss_mask: 0.1303  loss_dice: 1.1838
2025/06/24 13:02:41 - mmengine - INFO - Epoch(train)  [1][3640/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:49  time: 0.3287  data_time: 0.0013  memory: 2086  grad_norm: 87.2262  loss: 1.2974  loss_cls: 0.2424  loss_mask: 0.1193  loss_dice: 0.9356
2025/06/24 13:02:45 - mmengine - INFO - Epoch(train)  [1][3650/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:46  time: 0.3264  data_time: 0.0029  memory: 2371  grad_norm: 35.2743  loss: 1.1292  loss_cls: 0.2403  loss_mask: 0.1353  loss_dice: 0.7536
2025/06/24 13:02:48 - mmengine - INFO - Epoch(train)  [1][3660/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:42  time: 0.3219  data_time: 0.0005  memory: 2171  grad_norm: 446.4648  loss: 1.0491  loss_cls: 0.3577  loss_mask: 0.0608  loss_dice: 0.6306
2025/06/24 13:02:51 - mmengine - INFO - Epoch(train)  [1][3670/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:38  time: 0.3263  data_time: 0.0037  memory: 2276  grad_norm: 112.5508  loss: 1.7676  loss_cls: 0.4723  loss_mask: 0.1895  loss_dice: 1.1058
2025/06/24 13:02:54 - mmengine - INFO - Epoch(train)  [1][3680/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:34  time: 0.3252  data_time: 0.0070  memory: 2291  grad_norm: 40.7067  loss: 1.6043  loss_cls: 0.3919  loss_mask: 0.1481  loss_dice: 1.0644
2025/06/24 13:02:58 - mmengine - INFO - Epoch(train)  [1][3690/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:31  time: 0.3287  data_time: 0.0039  memory: 2364  grad_norm: 53.1412  loss: 1.9999  loss_cls: 0.6008  loss_mask: 0.1699  loss_dice: 1.2293
2025/06/24 13:03:01 - mmengine - INFO - Epoch(train)  [1][3700/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:28  time: 0.3291  data_time: 0.0050  memory: 2489  grad_norm: 81.4975  loss: 1.6767  loss_cls: 0.3780  loss_mask: 0.1752  loss_dice: 1.1235
2025/06/24 13:03:04 - mmengine - INFO - Epoch(train)  [1][3710/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:24  time: 0.3226  data_time: 0.0040  memory: 2098  grad_norm: 82.3889  loss: 1.0363  loss_cls: 0.2923  loss_mask: 0.1066  loss_dice: 0.6374
2025/06/24 13:03:07 - mmengine - INFO - Epoch(train)  [1][3720/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:20  time: 0.3234  data_time: 0.0014  memory: 2195  grad_norm: 33.5667  loss: 1.5137  loss_cls: 0.2488  loss_mask: 0.1491  loss_dice: 1.1158
2025/06/24 13:03:11 - mmengine - INFO - Epoch(train)  [1][3730/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:17  time: 0.3312  data_time: 0.0027  memory: 2249  grad_norm: 62.5446  loss: 1.6993  loss_cls: 0.5850  loss_mask: 0.1038  loss_dice: 1.0105
2025/06/24 13:03:14 - mmengine - INFO - Epoch(train)  [1][3740/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:14  time: 0.3277  data_time: 0.0026  memory: 2276  grad_norm: 69.1788  loss: 1.4742  loss_cls: 0.3647  loss_mask: 0.2099  loss_dice: 0.8996
2025/06/24 13:03:17 - mmengine - INFO - Epoch(train)  [1][3750/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:10  time: 0.3225  data_time: 0.0021  memory: 2098  grad_norm: 170.3074  loss: 1.1949  loss_cls: 0.3284  loss_mask: 0.0858  loss_dice: 0.7807
2025/06/24 13:03:20 - mmengine - INFO - Epoch(train)  [1][3760/3769]  base_lr: 1.0000e-04 lr: 5.0000e-06  eta: 3:06:07  time: 0.3349  data_time: 0.0009  memory: 2407  grad_norm: 83.7493  loss: 1.7662  loss_cls: 0.5401  loss_mask: 0.1345  loss_dice: 1.0915
2025/06/24 13:03:23 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250624_124223
2025/06/24 13:03:24 - mmengine - INFO - Saving checkpoint at 1 epochs
2025/06/24 13:03:43 - mmengine - INFO - Epoch(val)  [1][ 10/209]    eta: 0:03:45  time: 1.1318  data_time: 0.5617  memory: 2237  
2025/06/24 13:03:50 - mmengine - INFO - Epoch(val)  [1][ 20/209]    eta: 0:02:46  time: 0.6336  data_time: 0.1183  memory: 1963  
2025/06/24 13:03:56 - mmengine - INFO - Epoch(val)  [1][ 30/209]    eta: 0:02:21  time: 0.6010  data_time: 0.0946  memory: 1963  
2025/06/24 13:04:02 - mmengine - INFO - Epoch(val)  [1][ 40/209]    eta: 0:02:05  time: 0.5978  data_time: 0.0992  memory: 1963  
2025/06/24 13:04:07 - mmengine - INFO - Epoch(val)  [1][ 50/209]    eta: 0:01:52  time: 0.5703  data_time: 0.0766  memory: 1963  
2025/06/24 13:04:14 - mmengine - INFO - Epoch(val)  [1][ 60/209]    eta: 0:01:45  time: 0.7198  data_time: 0.2018  memory: 1964  
2025/06/24 13:04:21 - mmengine - INFO - Epoch(val)  [1][ 70/209]    eta: 0:01:37  time: 0.6386  data_time: 0.1143  memory: 1964  
2025/06/24 13:04:27 - mmengine - INFO - Epoch(val)  [1][ 80/209]    eta: 0:01:29  time: 0.6558  data_time: 0.0918  memory: 1964  
2025/06/24 13:04:34 - mmengine - INFO - Epoch(val)  [1][ 90/209]    eta: 0:01:22  time: 0.6916  data_time: 0.1475  memory: 1964  
2025/06/24 13:04:41 - mmengine - INFO - Epoch(val)  [1][100/209]    eta: 0:01:15  time: 0.6630  data_time: 0.1216  memory: 1964  
2025/06/24 13:04:47 - mmengine - INFO - Epoch(val)  [1][110/209]    eta: 0:01:07  time: 0.6282  data_time: 0.1086  memory: 1964  
2025/06/24 13:04:53 - mmengine - INFO - Epoch(val)  [1][120/209]    eta: 0:01:00  time: 0.6143  data_time: 0.0861  memory: 1964  
2025/06/24 13:04:59 - mmengine - INFO - Epoch(val)  [1][130/209]    eta: 0:00:53  time: 0.5984  data_time: 0.0869  memory: 1964  
2025/06/24 13:05:05 - mmengine - INFO - Epoch(val)  [1][140/209]    eta: 0:00:45  time: 0.5852  data_time: 0.0744  memory: 1964  
2025/06/24 13:05:11 - mmengine - INFO - Epoch(val)  [1][150/209]    eta: 0:00:38  time: 0.5805  data_time: 0.0723  memory: 1964  
2025/06/24 13:05:18 - mmengine - INFO - Epoch(val)  [1][160/209]    eta: 0:00:32  time: 0.6484  data_time: 0.1159  memory: 1964  
2025/06/24 13:05:24 - mmengine - INFO - Epoch(val)  [1][170/209]    eta: 0:00:25  time: 0.6471  data_time: 0.1091  memory: 1964  
2025/06/24 13:05:31 - mmengine - INFO - Epoch(val)  [1][180/209]    eta: 0:00:19  time: 0.7185  data_time: 0.1793  memory: 1964  
2025/06/24 13:05:37 - mmengine - INFO - Epoch(val)  [1][190/209]    eta: 0:00:12  time: 0.5630  data_time: 0.0573  memory: 1964  
2025/06/24 13:05:42 - mmengine - INFO - Epoch(val)  [1][200/209]    eta: 0:00:05  time: 0.5594  data_time: 0.0599  memory: 1964  
2025/06/24 13:05:50 - mmengine - INFO - Evaluating segm...
2025/06/24 13:06:00 - mmengine - INFO - segm_mAP_copypaste: 0.354 0.568 0.387 0.151 0.486 0.643
