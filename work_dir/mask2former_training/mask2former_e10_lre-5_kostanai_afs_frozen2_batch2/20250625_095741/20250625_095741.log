2025/06/25 09:57:46 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: win32
    Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 143662607
    GPU 0: NVIDIA GeForce GTX 1660
    CUDA_HOME: C:\Users\Sagi\Miniconda3\envs\gcp-env\Library
    NVCC: Not Available
    MSVC: Оптимизирующий компилятор Microsoft (R) C/C++ версии 19.43.34810 для x64
    GCC: n/a
    PyTorch: 2.3.0
    PyTorch compiling details: PyTorch built with:
  - C++ Version: 201703
  - MSVC 192930151
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.8.1  (built against CUDA 12.0)
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.8.1, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.18.0
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 143662607
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/06/25 09:57:48 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=4, enable=True)
backend_args = None
batch_augments = [
    dict(
        img_pad_value=0,
        mask_pad_value=0,
        pad_mask=True,
        pad_seg=True,
        seg_pad_value=255,
        size=(
            512,
            512,
        ),
        type='BatchFixedSizePad'),
]
crop_size = (
    512,
    512,
)
data_preprocessor = dict(
    batch_augments=[
        dict(
            img_pad_value=0,
            mask_pad_value=0,
            pad_mask=True,
            pad_seg=True,
            seg_pad_value=255,
            size=(
                512,
                512,
            ),
            type='BatchFixedSizePad'),
    ],
    bgr_to_rgb=True,
    mask_pad_value=0,
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    pad_mask=True,
    pad_seg=True,
    pad_size_divisor=32,
    seg_pad_value=255,
    std=[
        58.395,
        57.12,
        57.375,
    ],
    type='DetDataPreprocessor')
data_root = 'data/kostanai'
dataset_type = 'WHUMixVectorDataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=True,
        interval=1,
        max_keep_ckpts=1,
        save_last=True,
        type='CheckpointHook'),
    logger=dict(interval=10, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(draw=True, interval=10, type='TanmlhVisualizationHook'))
default_scope = 'mmdet'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
img_norm_cfg = dict(
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    std=[
        58.395,
        57.12,
        57.375,
    ],
    to_rgb=True)
launcher = 'none'
load_from = 'work_dir\\mask2former_training\\mask2former_e10_lre-5_kostanai_afs_frozen2_batch2\\epoch_4.pth'
log_config = dict(hooks=[
    dict(type='TextLoggerHook'),
    dict(
        init_kwargs=dict(
            allow_val_change=True,
            group='mask2former_training',
            id='b2izs1o6',
            name='mask2former_e10_lre-5_kostanai_afs_frozen2_batch2',
            project='building-segmentation-gcp',
            resume='must'),
        interval=10,
        log_checkpoint=True,
        log_checkpoint_metadata=True,
        num_eval_images=10,
        type='MMDetWandbHook'),
])
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=10)
max_epochs = 10
model = dict(
    backbone=dict(
        depth=50,
        frozen_stages=2,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=False, type='BN'),
        norm_eval=True,
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mask_pad_value=0,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=True,
        pad_seg=True,
        pad_size_divisor=32,
        seg_pad_value=255,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    init_cfg=None,
    panoptic_fusion_head=dict(
        init_cfg=None,
        loss_panoptic=None,
        num_stuff_classes=0,
        num_things_classes=1,
        type='MaskFormerFusionHead'),
    panoptic_head=dict(
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=True),
        num_queries=300,
        num_stuff_classes=0,
        num_things_classes=1,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(
        filter_low_score=False,
        instance_on=True,
        iou_thr=0.8,
        max_per_image=200,
        panoptic_on=False,
        semantic_on=False),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='ClassificationCost', weight=2.0),
                dict(
                    type='CrossEntropyLossCost', use_sigmoid=True, weight=5.0),
                dict(eps=1.0, pred_act=True, type='DiceCost', weight=5.0),
            ],
            type='HungarianAssigner'),
        importance_sample_ratio=0.75,
        num_points=12544,
        oversample_ratio=3.0,
        sampler=dict(type='MaskPseudoSampler')),
    type='Mask2Former')
num_classes = 1
num_stuff_classes = 0
num_things_classes = 1
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=1e-05,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(decay_mult=1.0, lr_mult=0.01),
            level_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_feat=dict(decay_mult=0.0, lr_mult=1.0)),
        norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0, by_epoch=False, end=1000, start_factor=0.001,
        type='LinearLR'),
    dict(
        begin=0,
        by_epoch=True,
        end=10,
        gamma=0.1,
        milestones=[
            40,
        ],
        type='MultiStepLR'),
]
resume = True
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=2,
    dataset=dict(
        ann_file='test/test.json',
        backend_args=None,
        data_prefix=dict(img='test/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = [
    dict(
        ann_file='data/kostanai/test/test.json',
        backend_args=None,
        metric=[
            'segm',
        ],
        type='CocoMetric'),
]
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=False,
        with_mask=True,
        with_poly_json=False),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=10, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=4,
    dataset=dict(
        ann_file='train/train.json',
        backend_args=None,
        data_prefix=dict(img='train/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True,
                with_poly_json=False),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                direction=[
                    'horizontal',
                    'vertical',
                    'diagonal',
                ],
                prob=0.75,
                type='RandomFlip'),
            dict(prob=0.75, type='Rotate90'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        type='WHUMixVectorDataset'),
    num_workers=2,
    persistent_workers=False,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True,
        with_poly_json=False),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        direction=[
            'horizontal',
            'vertical',
            'diagonal',
        ],
        prob=0.75,
        type='RandomFlip'),
    dict(prob=0.75, type='Rotate90'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=2,
    dataset=dict(
        ann_file='val/val.json',
        backend_args=None,
        data_prefix=dict(img='val/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = [
    dict(
        ann_file='data/kostanai/val/val.json',
        backend_args=None,
        metric=[
            'segm',
        ],
        type='CocoMetric'),
]
vis_backends = [
    dict(
        init_kwargs=dict(
            allow_val_change=True,
            group='mask2former_training',
            id='b2izs1o6',
            name='mask2former_e10_lre-5_kostanai_afs_frozen2_batch2',
            project='building-segmentation-gcp',
            resume='must'),
        save_dir=
        'work_dir\\mask2former_training\\mask2former_e10_lre-5_kostanai_afs_frozen2_batch2\\wandb',
        type='WandbVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='TanmlhVisualizer',
    vis_backends=[
        dict(
            init_kwargs=dict(
                allow_val_change=True,
                group='mask2former_training',
                id='b2izs1o6',
                name='mask2former_e10_lre-5_kostanai_afs_frozen2_batch2',
                project='building-segmentation-gcp',
                resume='must'),
            save_dir=
            'work_dir\\mask2former_training\\mask2former_e10_lre-5_kostanai_afs_frozen2_batch2\\wandb',
            type='WandbVisBackend'),
    ])
work_dir = 'work_dir\\mask2former_training\\mask2former_e10_lre-5_kostanai_afs_frozen2_batch2'

2025/06/25 09:57:54 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/06/25 09:57:54 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/06/25 09:57:59 - mmengine - WARNING - backbone.conv1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.conv1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.conv2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.conv3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.downsample.0.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.1.conv1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.1.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.1.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.1.conv2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.1.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.1.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.1.conv3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.1.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.1.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.2.conv1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.2.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.2.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.2.conv2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.2.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.2.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.2.conv3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.2.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer1.2.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.conv1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.conv2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.conv3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.downsample.0.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.1.conv1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.1.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.1.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.1.conv2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.1.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.1.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.1.conv3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.1.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.1.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.2.conv1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.2.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.2.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.2.conv2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.2.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.2.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.2.conv3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.2.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.2.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.3.conv1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.3.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.3.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.3.conv2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.3.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.3.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.3.conv3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.3.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer2.3.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.0.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.0.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.0.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.0.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.0.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.0.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.1.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.1.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.1.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.1.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.1.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.1.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.2.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.2.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.2.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.2.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.2.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.2.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.3.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.3.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.3.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.3.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.3.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.3.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.4.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.4.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.4.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.4.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.4.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.4.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.5.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.5.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.5.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.5.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.5.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer3.5.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.0.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.0.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.0.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.0.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.0.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.0.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.1.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.1.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.1.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.1.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.1.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.1.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.2.bn1.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.2.bn1.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.2.bn2.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.2.bn2.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1.0000000000000001e-07
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.01
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.2.bn3.weight is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - WARNING - backbone.layer4.2.bn3.bias is skipped since its requires_grad=False
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.bias:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr=1e-05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr_mult=1.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:decay_mult=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr=1e-05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr_mult=1.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:decay_mult=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr=1e-05
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:weight_decay=0.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr_mult=1.0
2025/06/25 09:57:59 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:decay_mult=0.0
2025/06/25 09:57:59 - mmengine - INFO - LR is set based on batch size of 4 and the current batch size is 4. Scaling the original LR by 1.0.
2025/06/25 09:58:01 - mmengine - INFO - load model from: torchvision://resnet50
2025/06/25 09:58:01 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
2025/06/25 09:58:02 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

panoptic_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 512, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.query_embed.weight - torch.Size([300, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.query_feat.weight - torch.Size([300, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.cls_embed.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.cls_embed.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  
2025/06/25 09:58:02 - mmengine - INFO - Load checkpoint from work_dir\mask2former_training\mask2former_e10_lre-5_kostanai_afs_frozen2_batch2\epoch_4.pth
2025/06/25 09:58:02 - mmengine - INFO - resumed epoch: 4, iter: 3772
2025/06/25 09:58:02 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/06/25 09:58:02 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/06/25 09:58:02 - mmengine - INFO - Checkpoints will be saved to D:\Sagi\GCP\GCP\work_dir\mask2former_training\mask2former_e10_lre-5_kostanai_afs_frozen2_batch2.
2025/06/25 09:58:26 - mmengine - INFO - Epoch(train)  [5][ 10/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 3:42:44  time: 2.3663  data_time: 0.8728  memory: 6134  grad_norm: 29.5930  loss: 1.0793  loss_cls: 0.2138  loss_mask: 0.0848  loss_dice: 0.7807
2025/06/25 09:58:42 - mmengine - INFO - Epoch(train)  [5][ 20/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 3:04:21  time: 1.5577  data_time: 0.0008  memory: 6000  grad_norm: 17.8889  loss: 1.0251  loss_cls: 0.2222  loss_mask: 0.0859  loss_dice: 0.7170
2025/06/25 09:58:59 - mmengine - INFO - Epoch(train)  [5][ 30/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:57:23  time: 1.7496  data_time: 0.0034  memory: 6067  grad_norm: 34.7980  loss: 1.0812  loss_cls: 0.2413  loss_mask: 0.0703  loss_dice: 0.7696
2025/06/25 09:59:17 - mmengine - INFO - Epoch(train)  [5][ 40/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:54:43  time: 1.7909  data_time: 0.0026  memory: 5659  grad_norm: 14.4892  loss: 1.0395  loss_cls: 0.2271  loss_mask: 0.0845  loss_dice: 0.7279
2025/06/25 09:59:34 - mmengine - INFO - Epoch(train)  [5][ 50/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:51:35  time: 1.7152  data_time: 0.0046  memory: 5926  grad_norm: 28.3621  loss: 1.1024  loss_cls: 0.2879  loss_mask: 0.0738  loss_dice: 0.7406
2025/06/25 09:59:53 - mmengine - INFO - Epoch(train)  [5][ 60/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:52:22  time: 1.9051  data_time: 0.0059  memory: 6471  grad_norm: 8.4717  loss: 0.9652  loss_cls: 0.2206  loss_mask: 0.0786  loss_dice: 0.6661
2025/06/25 10:00:12 - mmengine - INFO - Epoch(train)  [5][ 70/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:52:13  time: 1.8595  data_time: 0.0016  memory: 5838  grad_norm: 20.3740  loss: 1.0708  loss_cls: 0.2217  loss_mask: 0.1059  loss_dice: 0.7432
2025/06/25 10:00:30 - mmengine - INFO - Epoch(train)  [5][ 80/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:51:48  time: 1.8398  data_time: 0.0051  memory: 6059  grad_norm: 29.3317  loss: 1.1391  loss_cls: 0.2578  loss_mask: 0.0921  loss_dice: 0.7892
2025/06/25 10:00:49 - mmengine - INFO - Epoch(train)  [5][ 90/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:51:22  time: 1.8358  data_time: 0.0022  memory: 5846  grad_norm: 20.3215  loss: 1.1523  loss_cls: 0.2370  loss_mask: 0.1040  loss_dice: 0.8114
2025/06/25 10:01:06 - mmengine - INFO - Epoch(train)  [5][100/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:50:05  time: 1.7413  data_time: 0.0003  memory: 5904  grad_norm: 28.0120  loss: 1.0569  loss_cls: 0.2287  loss_mask: 0.1006  loss_dice: 0.7275
2025/06/25 10:01:24 - mmengine - INFO - Epoch(train)  [5][110/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:49:07  time: 1.7589  data_time: 0.0048  memory: 5773  grad_norm: 25.1961  loss: 0.8916  loss_cls: 0.1946  loss_mask: 0.0790  loss_dice: 0.6180
2025/06/25 10:01:41 - mmengine - INFO - Epoch(train)  [5][120/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:48:23  time: 1.7719  data_time: 0.0062  memory: 6162  grad_norm: 19.8772  loss: 1.1386  loss_cls: 0.2497  loss_mask: 0.0859  loss_dice: 0.8030
2025/06/25 10:01:58 - mmengine - INFO - Epoch(train)  [5][130/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:47:15  time: 1.7079  data_time: 0.0054  memory: 6111  grad_norm: 9.8144  loss: 0.8738  loss_cls: 0.1896  loss_mask: 0.0744  loss_dice: 0.6097
2025/06/25 10:02:15 - mmengine - INFO - Epoch(train)  [5][140/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:46:13  time: 1.7046  data_time: 0.0049  memory: 5705  grad_norm: 20.6387  loss: 0.8597  loss_cls: 0.1714  loss_mask: 0.0679  loss_dice: 0.6205
2025/06/25 10:02:33 - mmengine - INFO - Epoch(train)  [5][150/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:45:39  time: 1.7624  data_time: 0.0036  memory: 6125  grad_norm: 14.2248  loss: 1.1279  loss_cls: 0.2430  loss_mask: 0.0870  loss_dice: 0.7979
2025/06/25 10:02:50 - mmengine - INFO - Epoch(train)  [5][160/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:44:55  time: 1.7311  data_time: 0.0030  memory: 6066  grad_norm: 27.5700  loss: 0.9808  loss_cls: 0.2443  loss_mask: 0.0834  loss_dice: 0.6530
2025/06/25 10:03:08 - mmengine - INFO - Epoch(train)  [5][170/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:44:23  time: 1.7552  data_time: 0.0028  memory: 6105  grad_norm: 20.2681  loss: 1.0376  loss_cls: 0.2766  loss_mask: 0.0764  loss_dice: 0.6847
2025/06/25 10:03:25 - mmengine - INFO - Epoch(train)  [5][180/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:43:42  time: 1.7213  data_time: 0.0053  memory: 5721  grad_norm: 20.4957  loss: 0.9512  loss_cls: 0.2119  loss_mask: 0.0651  loss_dice: 0.6742
2025/06/25 10:03:43 - mmengine - INFO - Epoch(train)  [5][190/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:43:13  time: 1.7546  data_time: 0.0037  memory: 6530  grad_norm: 26.9973  loss: 1.0014  loss_cls: 0.2231  loss_mask: 0.0777  loss_dice: 0.7006
2025/06/25 10:04:00 - mmengine - INFO - Epoch(train)  [5][200/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:42:39  time: 1.7344  data_time: 0.0046  memory: 5541  grad_norm: 20.9953  loss: 1.0119  loss_cls: 0.2400  loss_mask: 0.0925  loss_dice: 0.6795
2025/06/25 10:04:18 - mmengine - INFO - Epoch(train)  [5][210/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:42:32  time: 1.8297  data_time: 0.0042  memory: 5890  grad_norm: 15.9671  loss: 1.0463  loss_cls: 0.2100  loss_mask: 0.0814  loss_dice: 0.7549
2025/06/25 10:04:37 - mmengine - INFO - Epoch(train)  [5][220/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:42:25  time: 1.8335  data_time: 0.0018  memory: 5684  grad_norm: 20.0794  loss: 1.0368  loss_cls: 0.2360  loss_mask: 0.0929  loss_dice: 0.7078
2025/06/25 10:04:52 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250625_095741
2025/06/25 10:04:56 - mmengine - INFO - Epoch(train)  [5][230/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:42:29  time: 1.8834  data_time: 0.0058  memory: 6162  grad_norm: 17.0250  loss: 0.9914  loss_cls: 0.2327  loss_mask: 0.0895  loss_dice: 0.6691
2025/06/25 10:05:14 - mmengine - INFO - Epoch(train)  [5][240/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:42:16  time: 1.8176  data_time: 0.0071  memory: 5986  grad_norm: 26.7124  loss: 1.0039  loss_cls: 0.2339  loss_mask: 0.0790  loss_dice: 0.6911
2025/06/25 10:05:31 - mmengine - INFO - Epoch(train)  [5][250/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:41:42  time: 1.7224  data_time: 0.0043  memory: 5757  grad_norm: 41.3470  loss: 1.0310  loss_cls: 0.2427  loss_mask: 0.1004  loss_dice: 0.6879
2025/06/25 10:05:50 - mmengine - INFO - Epoch(train)  [5][260/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:41:37  time: 1.8600  data_time: 0.0045  memory: 5788  grad_norm: 18.4829  loss: 1.1807  loss_cls: 0.2675  loss_mask: 0.0860  loss_dice: 0.8272
2025/06/25 10:06:09 - mmengine - INFO - Epoch(train)  [5][270/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:41:47  time: 1.9340  data_time: 0.0022  memory: 5786  grad_norm: 134.1807  loss: 1.1815  loss_cls: 0.2487  loss_mask: 0.0909  loss_dice: 0.8419
2025/06/25 10:06:28 - mmengine - INFO - Epoch(train)  [5][280/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:41:43  time: 1.8777  data_time: 0.0016  memory: 5875  grad_norm: 12.0853  loss: 1.0343  loss_cls: 0.2292  loss_mask: 0.0653  loss_dice: 0.7397
2025/06/25 10:06:46 - mmengine - INFO - Epoch(train)  [5][290/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:41:38  time: 1.8704  data_time: 0.0043  memory: 5868  grad_norm: 16.4101  loss: 1.0217  loss_cls: 0.2652  loss_mask: 0.0771  loss_dice: 0.6793
2025/06/25 10:07:05 - mmengine - INFO - Epoch(train)  [5][300/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:41:24  time: 1.8325  data_time: 0.0047  memory: 5956  grad_norm: 20.2225  loss: 0.9743  loss_cls: 0.2078  loss_mask: 0.0741  loss_dice: 0.6924
2025/06/25 10:07:23 - mmengine - INFO - Epoch(train)  [5][310/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:41:05  time: 1.8018  data_time: 0.0041  memory: 5623  grad_norm: 28.7926  loss: 0.9594  loss_cls: 0.1971  loss_mask: 0.0895  loss_dice: 0.6728
2025/06/25 10:07:41 - mmengine - INFO - Epoch(train)  [5][320/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:40:49  time: 1.8179  data_time: 0.0045  memory: 5890  grad_norm: 22.4718  loss: 1.1041  loss_cls: 0.2680  loss_mask: 0.0769  loss_dice: 0.7592
2025/06/25 10:07:59 - mmengine - INFO - Epoch(train)  [5][330/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:40:35  time: 1.8336  data_time: 0.0019  memory: 5904  grad_norm: 17.0101  loss: 1.0346  loss_cls: 0.2689  loss_mask: 0.0726  loss_dice: 0.6931
2025/06/25 10:08:17 - mmengine - INFO - Epoch(train)  [5][340/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:40:15  time: 1.7945  data_time: 0.0031  memory: 5794  grad_norm: 20.5217  loss: 1.0428  loss_cls: 0.2105  loss_mask: 0.0795  loss_dice: 0.7528
2025/06/25 10:08:36 - mmengine - INFO - Epoch(train)  [5][350/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:40:04  time: 1.8597  data_time: 0.0030  memory: 6133  grad_norm: 31.2212  loss: 1.0844  loss_cls: 0.2599  loss_mask: 0.1069  loss_dice: 0.7177
2025/06/25 10:08:54 - mmengine - INFO - Epoch(train)  [5][360/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:39:49  time: 1.8271  data_time: 0.0036  memory: 6220  grad_norm: 26.3320  loss: 1.1020  loss_cls: 0.2463  loss_mask: 0.0765  loss_dice: 0.7792
2025/06/25 10:09:12 - mmengine - INFO - Epoch(train)  [5][370/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:39:24  time: 1.7638  data_time: 0.0033  memory: 5926  grad_norm: 29.7209  loss: 0.9156  loss_cls: 0.2052  loss_mask: 0.0609  loss_dice: 0.6495
2025/06/25 10:09:30 - mmengine - INFO - Epoch(train)  [5][380/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:39:05  time: 1.8005  data_time: 0.0016  memory: 5993  grad_norm: 12.7391  loss: 1.0234  loss_cls: 0.2268  loss_mask: 0.0742  loss_dice: 0.7223
2025/06/25 10:09:48 - mmengine - INFO - Epoch(train)  [5][390/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:38:49  time: 1.8214  data_time: 0.0035  memory: 5765  grad_norm: 26.7119  loss: 1.0943  loss_cls: 0.2264  loss_mask: 0.0790  loss_dice: 0.7889
2025/06/25 10:10:05 - mmengine - INFO - Epoch(train)  [5][400/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:38:21  time: 1.7351  data_time: 0.0036  memory: 5810  grad_norm: 59.0533  loss: 1.1903  loss_cls: 0.2611  loss_mask: 0.1206  loss_dice: 0.8086
2025/06/25 10:10:24 - mmengine - INFO - Epoch(train)  [5][410/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:38:16  time: 1.9085  data_time: 0.0041  memory: 5801  grad_norm: 12.2366  loss: 1.0168  loss_cls: 0.2003  loss_mask: 0.0816  loss_dice: 0.7349
2025/06/25 10:10:43 - mmengine - INFO - Epoch(train)  [5][420/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:38:03  time: 1.8559  data_time: 0.0032  memory: 5861  grad_norm: 25.3445  loss: 0.9859  loss_cls: 0.2143  loss_mask: 0.0701  loss_dice: 0.7015
2025/06/25 10:11:00 - mmengine - INFO - Epoch(train)  [5][430/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:37:32  time: 1.7021  data_time: 0.0037  memory: 5571  grad_norm: 18.8197  loss: 0.9583  loss_cls: 0.1847  loss_mask: 0.1082  loss_dice: 0.6654
2025/06/25 10:11:18 - mmengine - INFO - Epoch(train)  [5][440/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:37:10  time: 1.7755  data_time: 0.0030  memory: 5948  grad_norm: 18.1775  loss: 0.9690  loss_cls: 0.1869  loss_mask: 0.0836  loss_dice: 0.6986
2025/06/25 10:11:34 - mmengine - INFO - Epoch(train)  [5][450/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:36:36  time: 1.6716  data_time: 0.0032  memory: 5854  grad_norm: 26.4181  loss: 0.9719  loss_cls: 0.2407  loss_mask: 0.0979  loss_dice: 0.6333
2025/06/25 10:11:52 - mmengine - INFO - Epoch(train)  [5][460/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:36:12  time: 1.7496  data_time: 0.0018  memory: 5882  grad_norm: 20.3415  loss: 1.0515  loss_cls: 0.2248  loss_mask: 0.1140  loss_dice: 0.7127
2025/06/25 10:12:10 - mmengine - INFO - Epoch(train)  [5][470/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:36:01  time: 1.8627  data_time: 0.0020  memory: 5978  grad_norm: 60.3790  loss: 1.1191  loss_cls: 0.2617  loss_mask: 0.1144  loss_dice: 0.7430
2025/06/25 10:12:29 - mmengine - INFO - Epoch(train)  [5][480/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:35:45  time: 1.8235  data_time: 0.0046  memory: 6089  grad_norm: 29.4922  loss: 0.9697  loss_cls: 0.1946  loss_mask: 0.0664  loss_dice: 0.7087
2025/06/25 10:12:46 - mmengine - INFO - Epoch(train)  [5][490/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:35:20  time: 1.7400  data_time: 0.0004  memory: 5787  grad_norm: 46.3958  loss: 0.9142  loss_cls: 0.2052  loss_mask: 0.0734  loss_dice: 0.6357
2025/06/25 10:13:03 - mmengine - INFO - Epoch(train)  [5][500/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:34:53  time: 1.7159  data_time: 0.0025  memory: 6281  grad_norm: 18.9447  loss: 0.9687  loss_cls: 0.2118  loss_mask: 0.0659  loss_dice: 0.6910
2025/06/25 10:13:21 - mmengine - INFO - Epoch(train)  [5][510/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:34:28  time: 1.7336  data_time: 0.0035  memory: 5773  grad_norm: 16.9756  loss: 1.0521  loss_cls: 0.2082  loss_mask: 0.0808  loss_dice: 0.7631
2025/06/25 10:13:38 - mmengine - INFO - Epoch(train)  [5][520/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:34:06  time: 1.7618  data_time: 0.0052  memory: 6456  grad_norm: 49.3283  loss: 1.0661  loss_cls: 0.2440  loss_mask: 0.0757  loss_dice: 0.7464
2025/06/25 10:13:56 - mmengine - INFO - Epoch(train)  [5][530/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:33:51  time: 1.8256  data_time: 0.0026  memory: 6118  grad_norm: 20.3108  loss: 1.0502  loss_cls: 0.2457  loss_mask: 0.0859  loss_dice: 0.7186
2025/06/25 10:14:14 - mmengine - INFO - Epoch(train)  [5][540/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:33:31  time: 1.7868  data_time: 0.0027  memory: 6022  grad_norm: 24.7493  loss: 0.9973  loss_cls: 0.2042  loss_mask: 0.1047  loss_dice: 0.6884
2025/06/25 10:14:32 - mmengine - INFO - Epoch(train)  [5][550/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:33:14  time: 1.8092  data_time: 0.0028  memory: 5795  grad_norm: 48.5805  loss: 1.0678  loss_cls: 0.2690  loss_mask: 0.0769  loss_dice: 0.7219
2025/06/25 10:14:51 - mmengine - INFO - Epoch(train)  [5][560/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:32:58  time: 1.8178  data_time: 0.0029  memory: 5941  grad_norm: 18.1734  loss: 1.0596  loss_cls: 0.2451  loss_mask: 0.0781  loss_dice: 0.7364
2025/06/25 10:15:09 - mmengine - INFO - Epoch(train)  [5][570/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:32:43  time: 1.8414  data_time: 0.0018  memory: 6140  grad_norm: 36.2832  loss: 1.0879  loss_cls: 0.2509  loss_mask: 0.1050  loss_dice: 0.7320
2025/06/25 10:15:27 - mmengine - INFO - Epoch(train)  [5][580/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:32:23  time: 1.7698  data_time: 0.0049  memory: 5625  grad_norm: 19.3391  loss: 1.0057  loss_cls: 0.2363  loss_mask: 0.0723  loss_dice: 0.6971
2025/06/25 10:15:46 - mmengine - INFO - Epoch(train)  [5][590/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:32:15  time: 1.9155  data_time: 0.0025  memory: 5905  grad_norm: 18.0048  loss: 1.0349  loss_cls: 0.2548  loss_mask: 0.0808  loss_dice: 0.6993
2025/06/25 10:16:04 - mmengine - INFO - Epoch(train)  [5][600/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:31:54  time: 1.7780  data_time: 0.0031  memory: 6133  grad_norm: 12.9441  loss: 1.0256  loss_cls: 0.2297  loss_mask: 0.0699  loss_dice: 0.7261
2025/06/25 10:16:21 - mmengine - INFO - Epoch(train)  [5][610/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:31:33  time: 1.7554  data_time: 0.0031  memory: 5757  grad_norm: 23.2865  loss: 1.0701  loss_cls: 0.2156  loss_mask: 0.0917  loss_dice: 0.7628
2025/06/25 10:16:39 - mmengine - INFO - Epoch(train)  [5][620/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:31:17  time: 1.8265  data_time: 0.0033  memory: 5897  grad_norm: 25.9075  loss: 1.0776  loss_cls: 0.2474  loss_mask: 0.0971  loss_dice: 0.7330
2025/06/25 10:16:58 - mmengine - INFO - Epoch(train)  [5][630/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:31:03  time: 1.8597  data_time: 0.0013  memory: 5883  grad_norm: 12.3382  loss: 1.0470  loss_cls: 0.2055  loss_mask: 0.0943  loss_dice: 0.7471
2025/06/25 10:17:16 - mmengine - INFO - Epoch(train)  [5][640/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:30:45  time: 1.7958  data_time: 0.0017  memory: 5817  grad_norm: 33.8664  loss: 0.9899  loss_cls: 0.2236  loss_mask: 0.0721  loss_dice: 0.6942
2025/06/25 10:17:35 - mmengine - INFO - Epoch(train)  [5][650/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:30:34  time: 1.8943  data_time: 0.0021  memory: 5978  grad_norm: 40.0843  loss: 1.0742  loss_cls: 0.2208  loss_mask: 0.1198  loss_dice: 0.7336
2025/06/25 10:17:52 - mmengine - INFO - Epoch(train)  [5][660/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:30:10  time: 1.7250  data_time: 0.0021  memory: 5860  grad_norm: 21.1317  loss: 1.0563  loss_cls: 0.2339  loss_mask: 0.0758  loss_dice: 0.7466
2025/06/25 10:18:10 - mmengine - INFO - Epoch(train)  [5][670/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:29:52  time: 1.8054  data_time: 0.0018  memory: 5934  grad_norm: 66.8933  loss: 0.9681  loss_cls: 0.2216  loss_mask: 0.0600  loss_dice: 0.6864
2025/06/25 10:18:28 - mmengine - INFO - Epoch(train)  [5][680/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:29:33  time: 1.7883  data_time: 0.0025  memory: 6001  grad_norm: 95.1842  loss: 1.1026  loss_cls: 0.2684  loss_mask: 0.0723  loss_dice: 0.7619
2025/06/25 10:18:46 - mmengine - INFO - Epoch(train)  [5][690/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:29:15  time: 1.8057  data_time: 0.0020  memory: 5794  grad_norm: 23.9188  loss: 1.0233  loss_cls: 0.2440  loss_mask: 0.0715  loss_dice: 0.7078
2025/06/25 10:19:05 - mmengine - INFO - Epoch(train)  [5][700/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:29:01  time: 1.8674  data_time: 0.0051  memory: 6000  grad_norm: 54.4253  loss: 1.0115  loss_cls: 0.1927  loss_mask: 0.1091  loss_dice: 0.7096
2025/06/25 10:19:23 - mmengine - INFO - Epoch(train)  [5][710/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:28:42  time: 1.7843  data_time: 0.0019  memory: 5919  grad_norm: 62.1342  loss: 1.0297  loss_cls: 0.2300  loss_mask: 0.0749  loss_dice: 0.7248
2025/06/25 10:19:41 - mmengine - INFO - Epoch(train)  [5][720/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:28:25  time: 1.8097  data_time: 0.0014  memory: 6184  grad_norm: 31.3392  loss: 1.0575  loss_cls: 0.2278  loss_mask: 0.0915  loss_dice: 0.7382
2025/06/25 10:19:59 - mmengine - INFO - Epoch(train)  [5][730/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:28:06  time: 1.8027  data_time: 0.0029  memory: 6441  grad_norm: 26.8049  loss: 1.1073  loss_cls: 0.2538  loss_mask: 0.1300  loss_dice: 0.7235
2025/06/25 10:20:16 - mmengine - INFO - Epoch(train)  [5][740/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:27:46  time: 1.7624  data_time: 0.0017  memory: 6016  grad_norm: 36.2929  loss: 0.9465  loss_cls: 0.2043  loss_mask: 0.0774  loss_dice: 0.6648
2025/06/25 10:20:34 - mmengine - INFO - Epoch(train)  [5][750/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:27:24  time: 1.7484  data_time: 0.0028  memory: 5912  grad_norm: 12.1140  loss: 1.0145  loss_cls: 0.2285  loss_mask: 0.0680  loss_dice: 0.7179
2025/06/25 10:20:51 - mmengine - INFO - Epoch(train)  [5][760/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:27:03  time: 1.7481  data_time: 0.0021  memory: 5994  grad_norm: 24.6022  loss: 1.0828  loss_cls: 0.2644  loss_mask: 0.0708  loss_dice: 0.7476
2025/06/25 10:21:09 - mmengine - INFO - Epoch(train)  [5][770/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:26:42  time: 1.7641  data_time: 0.0037  memory: 5802  grad_norm: 10.8569  loss: 1.0252  loss_cls: 0.1871  loss_mask: 0.0977  loss_dice: 0.7405
2025/06/25 10:21:28 - mmengine - INFO - Epoch(train)  [5][780/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:26:27  time: 1.8500  data_time: 0.0035  memory: 5883  grad_norm: 14.8374  loss: 0.9846  loss_cls: 0.2513  loss_mask: 0.0739  loss_dice: 0.6594
2025/06/25 10:21:47 - mmengine - INFO - Epoch(train)  [5][790/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:26:15  time: 1.8973  data_time: 0.0049  memory: 6184  grad_norm: 32.5429  loss: 1.0590  loss_cls: 0.2457  loss_mask: 0.0955  loss_dice: 0.7177
2025/06/25 10:22:04 - mmengine - INFO - Epoch(train)  [5][800/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:25:55  time: 1.7687  data_time: 0.0030  memory: 5919  grad_norm: 12.3414  loss: 0.9400  loss_cls: 0.1911  loss_mask: 0.0760  loss_dice: 0.6729
2025/06/25 10:22:22 - mmengine - INFO - Epoch(train)  [5][810/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:25:34  time: 1.7539  data_time: 0.0010  memory: 5745  grad_norm: 21.1802  loss: 0.9661  loss_cls: 0.1939  loss_mask: 0.0691  loss_dice: 0.7032
2025/06/25 10:22:40 - mmengine - INFO - Epoch(train)  [5][820/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:25:15  time: 1.7881  data_time: 0.0031  memory: 5823  grad_norm: 56.0419  loss: 1.0596  loss_cls: 0.2570  loss_mask: 0.0741  loss_dice: 0.7286
2025/06/25 10:22:57 - mmengine - INFO - Epoch(train)  [5][830/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:24:52  time: 1.7180  data_time: 0.0013  memory: 6132  grad_norm: 30.7231  loss: 0.9722  loss_cls: 0.2112  loss_mask: 0.0868  loss_dice: 0.6742
2025/06/25 10:23:14 - mmengine - INFO - Epoch(train)  [5][840/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:24:29  time: 1.7036  data_time: 0.0069  memory: 5729  grad_norm: 20.1692  loss: 0.9406  loss_cls: 0.1973  loss_mask: 0.0634  loss_dice: 0.6799
2025/06/25 10:23:31 - mmengine - INFO - Epoch(train)  [5][850/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:24:08  time: 1.7500  data_time: 0.0043  memory: 5934  grad_norm: 33.2026  loss: 0.9292  loss_cls: 0.2071  loss_mask: 0.0737  loss_dice: 0.6484
2025/06/25 10:23:50 - mmengine - INFO - Epoch(train)  [5][860/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:23:51  time: 1.8110  data_time: 0.0025  memory: 6007  grad_norm: 8.2879  loss: 0.9381  loss_cls: 0.1922  loss_mask: 0.0796  loss_dice: 0.6663
2025/06/25 10:24:08 - mmengine - INFO - Epoch(train)  [5][870/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:23:37  time: 1.8776  data_time: 0.0044  memory: 5956  grad_norm: 74.6281  loss: 0.9848  loss_cls: 0.2071  loss_mask: 0.0952  loss_dice: 0.6825
2025/06/25 10:24:28 - mmengine - INFO - Epoch(train)  [5][880/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:23:27  time: 1.9489  data_time: 0.0061  memory: 6037  grad_norm: 16.8335  loss: 1.0387  loss_cls: 0.2415  loss_mask: 0.0753  loss_dice: 0.7219
2025/06/25 10:24:47 - mmengine - INFO - Epoch(train)  [5][890/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:23:15  time: 1.9114  data_time: 0.0026  memory: 5935  grad_norm: 29.2034  loss: 0.9028  loss_cls: 0.1819  loss_mask: 0.0729  loss_dice: 0.6480
2025/06/25 10:25:05 - mmengine - INFO - Epoch(train)  [5][900/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:22:55  time: 1.7681  data_time: 0.0015  memory: 5795  grad_norm: 19.4670  loss: 1.1292  loss_cls: 0.2449  loss_mask: 0.0872  loss_dice: 0.7970
2025/06/25 10:25:24 - mmengine - INFO - Epoch(train)  [5][910/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:22:43  time: 1.9124  data_time: 0.0022  memory: 6323  grad_norm: 13.6617  loss: 1.1327  loss_cls: 0.2789  loss_mask: 0.0734  loss_dice: 0.7803
2025/06/25 10:25:42 - mmengine - INFO - Epoch(train)  [5][920/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:22:24  time: 1.7834  data_time: 0.0075  memory: 5662  grad_norm: 37.6667  loss: 1.0187  loss_cls: 0.2165  loss_mask: 0.0717  loss_dice: 0.7305
2025/06/25 10:25:59 - mmengine - INFO - Epoch(train)  [5][930/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:22:03  time: 1.7500  data_time: 0.0016  memory: 5839  grad_norm: 59.0380  loss: 1.1576  loss_cls: 0.2682  loss_mask: 0.0963  loss_dice: 0.7931
2025/06/25 10:26:19 - mmengine - INFO - Epoch(train)  [5][940/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:21:53  time: 1.9527  data_time: 0.0028  memory: 5831  grad_norm: 17.0713  loss: 1.0684  loss_cls: 0.2378  loss_mask: 0.0948  loss_dice: 0.7358
2025/06/25 10:26:23 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250625_095741
2025/06/25 10:26:23 - mmengine - INFO - Saving checkpoint at 5 epochs
2025/06/25 10:26:52 - mmengine - INFO - Epoch(val)  [5][ 10/105]    eta: 0:03:01  time: 1.9078  data_time: 0.9619  memory: 5511  
2025/06/25 10:27:03 - mmengine - INFO - Epoch(val)  [5][ 20/105]    eta: 0:02:06  time: 1.0794  data_time: 0.1464  memory: 2334  
2025/06/25 10:27:14 - mmengine - INFO - Epoch(val)  [5][ 30/105]    eta: 0:01:42  time: 1.0948  data_time: 0.1418  memory: 2334  
2025/06/25 10:27:24 - mmengine - INFO - Epoch(val)  [5][ 40/105]    eta: 0:01:23  time: 1.0506  data_time: 0.0867  memory: 2334  
2025/06/25 10:27:35 - mmengine - INFO - Epoch(val)  [5][ 50/105]    eta: 0:01:07  time: 1.0386  data_time: 0.0855  memory: 2334  
2025/06/25 10:27:45 - mmengine - INFO - Epoch(val)  [5][ 60/105]    eta: 0:00:54  time: 1.0298  data_time: 0.0834  memory: 2334  
2025/06/25 10:27:55 - mmengine - INFO - Epoch(val)  [5][ 70/105]    eta: 0:00:41  time: 1.0539  data_time: 0.0735  memory: 2334  
2025/06/25 10:28:07 - mmengine - INFO - Epoch(val)  [5][ 80/105]    eta: 0:00:29  time: 1.1475  data_time: 0.1532  memory: 2334  
2025/06/25 10:28:17 - mmengine - INFO - Epoch(val)  [5][ 90/105]    eta: 0:00:17  time: 1.0337  data_time: 0.0734  memory: 2334  
2025/06/25 10:28:28 - mmengine - INFO - Epoch(val)  [5][100/105]    eta: 0:00:05  time: 1.0373  data_time: 0.0733  memory: 2334  
2025/06/25 10:28:34 - mmengine - INFO - Evaluating segm...
2025/06/25 10:28:43 - mmengine - INFO - segm_mAP_copypaste: 0.541 0.807 0.605 0.328 0.669 0.785
2025/06/25 10:28:43 - mmengine - INFO - Epoch(val) [5][105/105]    coco/segm_mAP: 0.5410  coco/segm_mAP_50: 0.8070  coco/segm_mAP_75: 0.6050  coco/segm_mAP_s: 0.3280  coco/segm_mAP_m: 0.6690  coco/segm_mAP_l: 0.7850  data_time: 0.1774  time: 1.1280
2025/06/25 10:29:09 - mmengine - INFO - Epoch(train)  [6][ 10/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:22:03  time: 2.5949  data_time: 0.7872  memory: 5875  grad_norm: 80.6803  loss: 1.0309  loss_cls: 0.2499  loss_mask: 0.0777  loss_dice: 0.7032
2025/06/25 10:29:26 - mmengine - INFO - Epoch(train)  [6][ 20/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:21:40  time: 1.7288  data_time: 0.0048  memory: 5987  grad_norm: 50.2018  loss: 1.0512  loss_cls: 0.2075  loss_mask: 0.0868  loss_dice: 0.7569
2025/06/25 10:29:44 - mmengine - INFO - Epoch(train)  [6][ 30/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:21:19  time: 1.7446  data_time: 0.0061  memory: 5765  grad_norm: 45.0914  loss: 1.0040  loss_cls: 0.2326  loss_mask: 0.0912  loss_dice: 0.6802
2025/06/25 10:30:01 - mmengine - INFO - Epoch(train)  [6][ 40/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:20:55  time: 1.6859  data_time: 0.0042  memory: 5765  grad_norm: 35.8589  loss: 1.0210  loss_cls: 0.2048  loss_mask: 0.1166  loss_dice: 0.6997
2025/06/25 10:30:20 - mmengine - INFO - Epoch(train)  [6][ 50/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:20:42  time: 1.9155  data_time: 0.0033  memory: 6192  grad_norm: 49.3226  loss: 1.0336  loss_cls: 0.2490  loss_mask: 0.0868  loss_dice: 0.6977
2025/06/25 10:30:39 - mmengine - INFO - Epoch(train)  [6][ 60/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:20:29  time: 1.9196  data_time: 0.0022  memory: 6279  grad_norm: 45.3293  loss: 1.0314  loss_cls: 0.2221  loss_mask: 0.0674  loss_dice: 0.7419
2025/06/25 10:30:57 - mmengine - INFO - Epoch(train)  [6][ 70/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:20:10  time: 1.7791  data_time: 0.0037  memory: 6051  grad_norm: 30.8442  loss: 1.0592  loss_cls: 0.2412  loss_mask: 0.0953  loss_dice: 0.7228
2025/06/25 10:31:14 - mmengine - INFO - Epoch(train)  [6][ 80/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:19:49  time: 1.7537  data_time: 0.0035  memory: 5948  grad_norm: 41.8247  loss: 0.9952  loss_cls: 0.2172  loss_mask: 0.0818  loss_dice: 0.6962
2025/06/25 10:31:31 - mmengine - INFO - Epoch(train)  [6][ 90/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:19:25  time: 1.6802  data_time: 0.0027  memory: 5905  grad_norm: 158.6466  loss: 0.9080  loss_cls: 0.2261  loss_mask: 0.0664  loss_dice: 0.6155
2025/06/25 10:31:49 - mmengine - INFO - Epoch(train)  [6][100/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:19:06  time: 1.7968  data_time: 0.0037  memory: 6001  grad_norm: 24.6815  loss: 1.0437  loss_cls: 0.2640  loss_mask: 0.0762  loss_dice: 0.7035
2025/06/25 10:32:07 - mmengine - INFO - Epoch(train)  [6][110/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:18:45  time: 1.7333  data_time: 0.0043  memory: 5875  grad_norm: 26.7105  loss: 0.9882  loss_cls: 0.2132  loss_mask: 0.0766  loss_dice: 0.6985
2025/06/25 10:32:25 - mmengine - INFO - Epoch(train)  [6][120/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:18:30  time: 1.8830  data_time: 0.0043  memory: 5861  grad_norm: 61.5303  loss: 1.0280  loss_cls: 0.2225  loss_mask: 0.0761  loss_dice: 0.7294
2025/06/25 10:32:44 - mmengine - INFO - Epoch(train)  [6][130/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:18:14  time: 1.8459  data_time: 0.0021  memory: 6169  grad_norm: 157.8443  loss: 1.0045  loss_cls: 0.2200  loss_mask: 0.0724  loss_dice: 0.7121
2025/06/25 10:33:01 - mmengine - INFO - Epoch(train)  [6][140/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:17:53  time: 1.7501  data_time: 0.0033  memory: 5926  grad_norm: 11.6466  loss: 1.0155  loss_cls: 0.2489  loss_mask: 0.0781  loss_dice: 0.6885
2025/06/25 10:33:20 - mmengine - INFO - Epoch(train)  [6][150/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:17:36  time: 1.8377  data_time: 0.0076  memory: 5948  grad_norm: 22.4986  loss: 1.1077  loss_cls: 0.2610  loss_mask: 0.1084  loss_dice: 0.7382
2025/06/25 10:33:41 - mmengine - INFO - Epoch(train)  [6][160/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:17:32  time: 2.1447  data_time: 0.0050  memory: 6221  grad_norm: 51.2171  loss: 0.9916  loss_cls: 0.2542  loss_mask: 0.0622  loss_dice: 0.6752
2025/06/25 10:34:00 - mmengine - INFO - Epoch(train)  [6][170/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:17:18  time: 1.9110  data_time: 0.0028  memory: 5641  grad_norm: 33.5735  loss: 1.0086  loss_cls: 0.2222  loss_mask: 0.1007  loss_dice: 0.6856
2025/06/25 10:34:18 - mmengine - INFO - Epoch(train)  [6][180/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:16:59  time: 1.7914  data_time: 0.0019  memory: 5904  grad_norm: 42.9313  loss: 1.0174  loss_cls: 0.2296  loss_mask: 0.0861  loss_dice: 0.7017
2025/06/25 10:34:36 - mmengine - INFO - Epoch(train)  [6][190/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:16:38  time: 1.7540  data_time: 0.0033  memory: 5736  grad_norm: 15.8522  loss: 0.9826  loss_cls: 0.2218  loss_mask: 0.0891  loss_dice: 0.6716
2025/06/25 10:34:54 - mmengine - INFO - Epoch(train)  [6][200/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:16:23  time: 1.8759  data_time: 0.0031  memory: 6191  grad_norm: 13.2021  loss: 0.9281  loss_cls: 0.2017  loss_mask: 0.0716  loss_dice: 0.6548
2025/06/25 10:35:13 - mmengine - INFO - Epoch(train)  [6][210/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:16:06  time: 1.8341  data_time: 0.0055  memory: 6286  grad_norm: 31.8449  loss: 1.0396  loss_cls: 0.2374  loss_mask: 0.0871  loss_dice: 0.7151
2025/06/25 10:35:31 - mmengine - INFO - Epoch(train)  [6][220/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:15:48  time: 1.8389  data_time: 0.0013  memory: 5671  grad_norm: 28.5023  loss: 1.0239  loss_cls: 0.2255  loss_mask: 0.0997  loss_dice: 0.6987
2025/06/25 10:35:50 - mmengine - INFO - Epoch(train)  [6][230/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:15:33  time: 1.8903  data_time: 0.0042  memory: 5904  grad_norm: 43.6323  loss: 1.0812  loss_cls: 0.2575  loss_mask: 0.0773  loss_dice: 0.7464
2025/06/25 10:36:09 - mmengine - INFO - Epoch(train)  [6][240/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:15:17  time: 1.8685  data_time: 0.0012  memory: 5750  grad_norm: 34.5268  loss: 1.0721  loss_cls: 0.2625  loss_mask: 0.0863  loss_dice: 0.7234
2025/06/25 10:36:26 - mmengine - INFO - Epoch(train)  [6][250/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:14:56  time: 1.7330  data_time: 0.0033  memory: 5734  grad_norm: 21.5337  loss: 0.9957  loss_cls: 0.2317  loss_mask: 0.0773  loss_dice: 0.6868
2025/06/25 10:36:44 - mmengine - INFO - Epoch(train)  [6][260/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:14:38  time: 1.8116  data_time: 0.0036  memory: 5762  grad_norm: 19.2070  loss: 0.9970  loss_cls: 0.2105  loss_mask: 0.0785  loss_dice: 0.7081
2025/06/25 10:37:01 - mmengine - INFO - Epoch(train)  [6][270/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:14:15  time: 1.6972  data_time: 0.0036  memory: 5890  grad_norm: 104.7400  loss: 1.0453  loss_cls: 0.2147  loss_mask: 0.0952  loss_dice: 0.7354
2025/06/25 10:37:19 - mmengine - INFO - Epoch(train)  [6][280/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:13:57  time: 1.8063  data_time: 0.0032  memory: 5919  grad_norm: 36.3210  loss: 1.0272  loss_cls: 0.2061  loss_mask: 0.0823  loss_dice: 0.7388
2025/06/25 10:37:28 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250625_095741
2025/06/25 10:37:36 - mmengine - INFO - Epoch(train)  [6][290/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:13:35  time: 1.7110  data_time: 0.0031  memory: 5956  grad_norm: 39.4146  loss: 0.9075  loss_cls: 0.1732  loss_mask: 0.0889  loss_dice: 0.6453
2025/06/25 10:37:54 - mmengine - INFO - Epoch(train)  [6][300/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:13:16  time: 1.7836  data_time: 0.0051  memory: 6118  grad_norm: 23.0378  loss: 1.1497  loss_cls: 0.2272  loss_mask: 0.0934  loss_dice: 0.8292
2025/06/25 10:38:12 - mmengine - INFO - Epoch(train)  [6][310/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:12:56  time: 1.7586  data_time: 0.0027  memory: 5846  grad_norm: 48.7842  loss: 1.0037  loss_cls: 0.2525  loss_mask: 0.0640  loss_dice: 0.6872
2025/06/25 10:38:29 - mmengine - INFO - Epoch(train)  [6][320/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:12:36  time: 1.7601  data_time: 0.0031  memory: 5801  grad_norm: 125.9972  loss: 1.0019  loss_cls: 0.2079  loss_mask: 0.0813  loss_dice: 0.7127
2025/06/25 10:38:47 - mmengine - INFO - Epoch(train)  [6][330/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:12:18  time: 1.8077  data_time: 0.0039  memory: 5699  grad_norm: 14.6156  loss: 0.9842  loss_cls: 0.2064  loss_mask: 0.0774  loss_dice: 0.7004
2025/06/25 10:39:05 - mmengine - INFO - Epoch(train)  [6][340/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:11:59  time: 1.7711  data_time: 0.0019  memory: 6030  grad_norm: 14.1242  loss: 0.9726  loss_cls: 0.2112  loss_mask: 0.0833  loss_dice: 0.6781
2025/06/25 10:39:24 - mmengine - INFO - Epoch(train)  [6][350/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:11:43  time: 1.8724  data_time: 0.0055  memory: 6257  grad_norm: 38.8188  loss: 0.9861  loss_cls: 0.1888  loss_mask: 0.0725  loss_dice: 0.7248
2025/06/25 10:39:42 - mmengine - INFO - Epoch(train)  [6][360/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:11:25  time: 1.8074  data_time: 0.0037  memory: 6441  grad_norm: 44.4898  loss: 1.0835  loss_cls: 0.2502  loss_mask: 0.0856  loss_dice: 0.7476
2025/06/25 10:40:00 - mmengine - INFO - Epoch(train)  [6][370/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:11:05  time: 1.7814  data_time: 0.0034  memory: 6088  grad_norm: 36.1253  loss: 0.9752  loss_cls: 0.2004  loss_mask: 0.0751  loss_dice: 0.6997
2025/06/25 10:40:18 - mmengine - INFO - Epoch(train)  [6][380/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:10:48  time: 1.8329  data_time: 0.0037  memory: 6125  grad_norm: 11.0679  loss: 1.0269  loss_cls: 0.2113  loss_mask: 0.0949  loss_dice: 0.7207
2025/06/25 10:40:36 - mmengine - INFO - Epoch(train)  [6][390/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:10:29  time: 1.7699  data_time: 0.0039  memory: 6066  grad_norm: 17.0642  loss: 1.0678  loss_cls: 0.2219  loss_mask: 0.0929  loss_dice: 0.7530
2025/06/25 10:40:53 - mmengine - INFO - Epoch(train)  [6][400/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:10:08  time: 1.7314  data_time: 0.0044  memory: 5823  grad_norm: 43.9464  loss: 0.9440  loss_cls: 0.2107  loss_mask: 0.0679  loss_dice: 0.6654
2025/06/25 10:41:11 - mmengine - INFO - Epoch(train)  [6][410/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:09:51  time: 1.8285  data_time: 0.0026  memory: 5583  grad_norm: 20.9049  loss: 1.0607  loss_cls: 0.2013  loss_mask: 0.0850  loss_dice: 0.7744
2025/06/25 10:41:29 - mmengine - INFO - Epoch(train)  [6][420/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:09:32  time: 1.7950  data_time: 0.0057  memory: 5729  grad_norm: 63.3266  loss: 1.0166  loss_cls: 0.2235  loss_mask: 0.0913  loss_dice: 0.7018
2025/06/25 10:41:47 - mmengine - INFO - Epoch(train)  [6][430/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:09:13  time: 1.7961  data_time: 0.0038  memory: 5685  grad_norm: 49.6937  loss: 1.0140  loss_cls: 0.1905  loss_mask: 0.0997  loss_dice: 0.7238
2025/06/25 10:42:05 - mmengine - INFO - Epoch(train)  [6][440/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:08:55  time: 1.8056  data_time: 0.0016  memory: 6155  grad_norm: 11.6960  loss: 1.0542  loss_cls: 0.2381  loss_mask: 0.0860  loss_dice: 0.7302
2025/06/25 10:42:23 - mmengine - INFO - Epoch(train)  [6][450/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:08:36  time: 1.7603  data_time: 0.0052  memory: 6008  grad_norm: 63.0016  loss: 0.9252  loss_cls: 0.1857  loss_mask: 0.0753  loss_dice: 0.6641
2025/06/25 10:42:40 - mmengine - INFO - Epoch(train)  [6][460/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:08:15  time: 1.7357  data_time: 0.0037  memory: 5758  grad_norm: 19.7097  loss: 1.0853  loss_cls: 0.2328  loss_mask: 0.1080  loss_dice: 0.7445
2025/06/25 10:42:58 - mmengine - INFO - Epoch(train)  [6][470/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:07:57  time: 1.8047  data_time: 0.0034  memory: 5780  grad_norm: 19.5375  loss: 0.9769  loss_cls: 0.1967  loss_mask: 0.0704  loss_dice: 0.7098
2025/06/25 10:43:16 - mmengine - INFO - Epoch(train)  [6][480/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:07:39  time: 1.8006  data_time: 0.0008  memory: 5882  grad_norm: 34.2901  loss: 1.1020  loss_cls: 0.2540  loss_mask: 0.0789  loss_dice: 0.7691
2025/06/25 10:43:34 - mmengine - INFO - Epoch(train)  [6][490/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:07:19  time: 1.7390  data_time: 0.0005  memory: 5897  grad_norm: 43.4734  loss: 1.0378  loss_cls: 0.2259  loss_mask: 0.0757  loss_dice: 0.7362
2025/06/25 10:43:51 - mmengine - INFO - Epoch(train)  [6][500/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:06:59  time: 1.7488  data_time: 0.0023  memory: 6220  grad_norm: 46.0428  loss: 0.9467  loss_cls: 0.1963  loss_mask: 0.1000  loss_dice: 0.6504
2025/06/25 10:44:09 - mmengine - INFO - Epoch(train)  [6][510/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:06:41  time: 1.8102  data_time: 0.0014  memory: 5788  grad_norm: 24.0812  loss: 0.9563  loss_cls: 0.2050  loss_mask: 0.0729  loss_dice: 0.6784
2025/06/25 10:44:27 - mmengine - INFO - Epoch(train)  [6][520/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:06:22  time: 1.7676  data_time: 0.0049  memory: 6353  grad_norm: 75.6176  loss: 0.9906  loss_cls: 0.2417  loss_mask: 0.0750  loss_dice: 0.6739
2025/06/25 10:44:45 - mmengine - INFO - Epoch(train)  [6][530/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:06:04  time: 1.8334  data_time: 0.0019  memory: 5758  grad_norm: 29.8319  loss: 1.0134  loss_cls: 0.2532  loss_mask: 0.0874  loss_dice: 0.6728
2025/06/25 10:45:03 - mmengine - INFO - Epoch(train)  [6][540/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:05:46  time: 1.7948  data_time: 0.0033  memory: 6007  grad_norm: 102.6994  loss: 1.0269  loss_cls: 0.2522  loss_mask: 0.0763  loss_dice: 0.6983
2025/06/25 10:45:21 - mmengine - INFO - Epoch(train)  [6][550/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:05:27  time: 1.7895  data_time: 0.0036  memory: 6273  grad_norm: 19.8138  loss: 1.0949  loss_cls: 0.2598  loss_mask: 0.0760  loss_dice: 0.7591
2025/06/25 10:45:40 - mmengine - INFO - Epoch(train)  [6][560/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:05:10  time: 1.8286  data_time: 0.0025  memory: 5965  grad_norm: 25.2297  loss: 1.1063  loss_cls: 0.2366  loss_mask: 0.1183  loss_dice: 0.7515
2025/06/25 10:45:57 - mmengine - INFO - Epoch(train)  [6][570/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:04:49  time: 1.7219  data_time: 0.0030  memory: 5978  grad_norm: 40.7143  loss: 0.9884  loss_cls: 0.1996  loss_mask: 0.0963  loss_dice: 0.6925
2025/06/25 10:46:14 - mmengine - INFO - Epoch(train)  [6][580/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:04:30  time: 1.7643  data_time: 0.0015  memory: 5758  grad_norm: 128.9871  loss: 0.9587  loss_cls: 0.2436  loss_mask: 0.1009  loss_dice: 0.6142
2025/06/25 10:46:33 - mmengine - INFO - Epoch(train)  [6][590/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:04:13  time: 1.8235  data_time: 0.0034  memory: 6162  grad_norm: 42.1390  loss: 1.1520  loss_cls: 0.2836  loss_mask: 0.0734  loss_dice: 0.7949
2025/06/25 10:46:51 - mmengine - INFO - Epoch(train)  [6][600/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:03:55  time: 1.8233  data_time: 0.0013  memory: 6104  grad_norm: 41.3353  loss: 0.9607  loss_cls: 0.2434  loss_mask: 0.0733  loss_dice: 0.6440
2025/06/25 10:47:08 - mmengine - INFO - Epoch(train)  [6][610/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:03:35  time: 1.7486  data_time: 0.0037  memory: 5978  grad_norm: 31.0301  loss: 0.9838  loss_cls: 0.1876  loss_mask: 0.0940  loss_dice: 0.7022
2025/06/25 10:47:27 - mmengine - INFO - Epoch(train)  [6][620/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:03:20  time: 1.8965  data_time: 0.0034  memory: 6031  grad_norm: 76.1619  loss: 1.0022  loss_cls: 0.1976  loss_mask: 0.0692  loss_dice: 0.7354
2025/06/25 10:47:46 - mmengine - INFO - Epoch(train)  [6][630/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:03:02  time: 1.8230  data_time: 0.0036  memory: 5890  grad_norm: 47.2473  loss: 1.0760  loss_cls: 0.2285  loss_mask: 0.0814  loss_dice: 0.7662
2025/06/25 10:48:05 - mmengine - INFO - Epoch(train)  [6][640/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:02:46  time: 1.9025  data_time: 0.0013  memory: 5772  grad_norm: 66.4282  loss: 1.1176  loss_cls: 0.2713  loss_mask: 0.0863  loss_dice: 0.7600
2025/06/25 10:48:23 - mmengine - INFO - Epoch(train)  [6][650/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:02:28  time: 1.8155  data_time: 0.0043  memory: 6588  grad_norm: 65.9232  loss: 0.9505  loss_cls: 0.2228  loss_mask: 0.0745  loss_dice: 0.6532
2025/06/25 10:48:40 - mmengine - INFO - Epoch(train)  [6][660/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:02:08  time: 1.7296  data_time: 0.0031  memory: 5677  grad_norm: 40.5040  loss: 0.9407  loss_cls: 0.1797  loss_mask: 0.0932  loss_dice: 0.6678
2025/06/25 10:48:58 - mmengine - INFO - Epoch(train)  [6][670/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:01:50  time: 1.8125  data_time: 0.0018  memory: 5868  grad_norm: 40.2259  loss: 0.8924  loss_cls: 0.1932  loss_mask: 0.0749  loss_dice: 0.6243
2025/06/25 10:49:17 - mmengine - INFO - Epoch(train)  [6][680/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:01:35  time: 1.9070  data_time: 0.0048  memory: 6272  grad_norm: 81.6289  loss: 1.1267  loss_cls: 0.2729  loss_mask: 0.0804  loss_dice: 0.7734
2025/06/25 10:49:37 - mmengine - INFO - Epoch(train)  [6][690/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:01:21  time: 1.9639  data_time: 0.0041  memory: 6007  grad_norm: 167.2047  loss: 1.0263  loss_cls: 0.2523  loss_mask: 0.0795  loss_dice: 0.6945
2025/06/25 10:49:55 - mmengine - INFO - Epoch(train)  [6][700/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:01:02  time: 1.7942  data_time: 0.0030  memory: 5772  grad_norm: 31.7239  loss: 1.1197  loss_cls: 0.2512  loss_mask: 0.0986  loss_dice: 0.7699
2025/06/25 10:50:13 - mmengine - INFO - Epoch(train)  [6][710/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:00:44  time: 1.7902  data_time: 0.0027  memory: 6243  grad_norm: 21.1953  loss: 0.9178  loss_cls: 0.2049  loss_mask: 0.0642  loss_dice: 0.6487
2025/06/25 10:50:31 - mmengine - INFO - Epoch(train)  [6][720/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:00:26  time: 1.8225  data_time: 0.0019  memory: 5978  grad_norm: 35.0893  loss: 1.1644  loss_cls: 0.2637  loss_mask: 0.0942  loss_dice: 0.8064
2025/06/25 10:50:49 - mmengine - INFO - Epoch(train)  [6][730/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:00:08  time: 1.8106  data_time: 0.0025  memory: 5534  grad_norm: 122.1919  loss: 0.8672  loss_cls: 0.1922  loss_mask: 0.0788  loss_dice: 0.5962
2025/06/25 10:51:08 - mmengine - INFO - Epoch(train)  [6][740/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:59:52  time: 1.9262  data_time: 0.0016  memory: 6170  grad_norm: 29.8302  loss: 1.1205  loss_cls: 0.2524  loss_mask: 0.1215  loss_dice: 0.7466
2025/06/25 10:51:30 - mmengine - INFO - Epoch(train)  [6][750/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:59:42  time: 2.1505  data_time: 0.0037  memory: 5986  grad_norm: 136.5089  loss: 1.0045  loss_cls: 0.2475  loss_mask: 0.0808  loss_dice: 0.6762
2025/06/25 10:51:48 - mmengine - INFO - Epoch(train)  [6][760/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:59:24  time: 1.7956  data_time: 0.0031  memory: 5743  grad_norm: 47.2076  loss: 0.9729  loss_cls: 0.1998  loss_mask: 0.1131  loss_dice: 0.6600
2025/06/25 10:52:05 - mmengine - INFO - Epoch(train)  [6][770/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:59:04  time: 1.7391  data_time: 0.0013  memory: 6279  grad_norm: 27.5930  loss: 1.0661  loss_cls: 0.2198  loss_mask: 0.0825  loss_dice: 0.7637
2025/06/25 10:52:23 - mmengine - INFO - Epoch(train)  [6][780/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:58:46  time: 1.8059  data_time: 0.0032  memory: 5795  grad_norm: 64.5337  loss: 1.0651  loss_cls: 0.2238  loss_mask: 0.0933  loss_dice: 0.7480
2025/06/25 10:52:42 - mmengine - INFO - Epoch(train)  [6][790/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:58:29  time: 1.8535  data_time: 0.0034  memory: 6486  grad_norm: 69.9263  loss: 1.1987  loss_cls: 0.3069  loss_mask: 0.0762  loss_dice: 0.8156
2025/06/25 10:53:01 - mmengine - INFO - Epoch(train)  [6][800/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:58:13  time: 1.9044  data_time: 0.0059  memory: 5709  grad_norm: 26.3890  loss: 0.9877  loss_cls: 0.1828  loss_mask: 0.0691  loss_dice: 0.7358
2025/06/25 10:53:19 - mmengine - INFO - Epoch(train)  [6][810/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:57:54  time: 1.7965  data_time: 0.0019  memory: 5804  grad_norm: 101.7883  loss: 0.9478  loss_cls: 0.1857  loss_mask: 0.0823  loss_dice: 0.6799
2025/06/25 10:53:37 - mmengine - INFO - Epoch(train)  [6][820/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:57:36  time: 1.8052  data_time: 0.0043  memory: 5868  grad_norm: 45.4864  loss: 0.9430  loss_cls: 0.2045  loss_mask: 0.0693  loss_dice: 0.6691
2025/06/25 10:53:54 - mmengine - INFO - Epoch(train)  [6][830/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:57:16  time: 1.7077  data_time: 0.0023  memory: 5948  grad_norm: 40.8804  loss: 1.0040  loss_cls: 0.2212  loss_mask: 0.0629  loss_dice: 0.7200
2025/06/25 10:54:11 - mmengine - INFO - Epoch(train)  [6][840/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:56:55  time: 1.6814  data_time: 0.0029  memory: 6059  grad_norm: 38.7115  loss: 1.1925  loss_cls: 0.2505  loss_mask: 0.2146  loss_dice: 0.7274
2025/06/25 10:54:28 - mmengine - INFO - Epoch(train)  [6][850/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:56:35  time: 1.7444  data_time: 0.0041  memory: 5735  grad_norm: 61.1700  loss: 0.9039  loss_cls: 0.2272  loss_mask: 0.0698  loss_dice: 0.6069
2025/06/25 10:54:46 - mmengine - INFO - Epoch(train)  [6][860/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:56:17  time: 1.8116  data_time: 0.0025  memory: 6073  grad_norm: 63.3023  loss: 1.0626  loss_cls: 0.2705  loss_mask: 0.0774  loss_dice: 0.7148
2025/06/25 10:55:04 - mmengine - INFO - Epoch(train)  [6][870/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:55:59  time: 1.7907  data_time: 0.0026  memory: 5890  grad_norm: 26.3479  loss: 1.0019  loss_cls: 0.1930  loss_mask: 0.0610  loss_dice: 0.7480
2025/06/25 10:55:22 - mmengine - INFO - Epoch(train)  [6][880/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:55:41  time: 1.8238  data_time: 0.0017  memory: 5949  grad_norm: 46.0167  loss: 1.0609  loss_cls: 0.2381  loss_mask: 0.0802  loss_dice: 0.7425
2025/06/25 10:55:41 - mmengine - INFO - Epoch(train)  [6][890/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:55:24  time: 1.8784  data_time: 0.0030  memory: 6456  grad_norm: 57.9287  loss: 0.9383  loss_cls: 0.2380  loss_mask: 0.0805  loss_dice: 0.6198
2025/06/25 10:55:59 - mmengine - INFO - Epoch(train)  [6][900/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:55:06  time: 1.7945  data_time: 0.0041  memory: 6015  grad_norm: 31.6038  loss: 1.1421  loss_cls: 0.2162  loss_mask: 0.0854  loss_dice: 0.8405
2025/06/25 10:56:17 - mmengine - INFO - Epoch(train)  [6][910/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:54:47  time: 1.7924  data_time: 0.0028  memory: 6147  grad_norm: 26.4249  loss: 1.0175  loss_cls: 0.2576  loss_mask: 0.0931  loss_dice: 0.6668
2025/06/25 10:56:35 - mmengine - INFO - Epoch(train)  [6][920/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:54:29  time: 1.8191  data_time: 0.0034  memory: 5809  grad_norm: 48.9665  loss: 0.9536  loss_cls: 0.1979  loss_mask: 0.0872  loss_dice: 0.6685
2025/06/25 10:56:53 - mmengine - INFO - Epoch(train)  [6][930/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:54:11  time: 1.8174  data_time: 0.0046  memory: 6184  grad_norm: 71.9685  loss: 1.0955  loss_cls: 0.2780  loss_mask: 0.0813  loss_dice: 0.7362
2025/06/25 10:57:11 - mmengine - INFO - Epoch(train)  [6][940/943]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:53:51  time: 1.7110  data_time: 0.0020  memory: 5800  grad_norm: 36.5001  loss: 1.0777  loss_cls: 0.2110  loss_mask: 0.1053  loss_dice: 0.7614
2025/06/25 10:57:15 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250625_095741
2025/06/25 10:57:15 - mmengine - INFO - Saving checkpoint at 6 epochs
2025/06/25 10:57:43 - mmengine - INFO - Epoch(val)  [6][ 10/105]    eta: 0:02:58  time: 1.8789  data_time: 0.9182  memory: 5985  
2025/06/25 10:57:54 - mmengine - INFO - Epoch(val)  [6][ 20/105]    eta: 0:02:07  time: 1.1130  data_time: 0.0809  memory: 2334  
2025/06/25 10:58:04 - mmengine - INFO - Epoch(val)  [6][ 30/105]    eta: 0:01:40  time: 1.0353  data_time: 0.1042  memory: 2334  
2025/06/25 10:58:14 - mmengine - INFO - Epoch(val)  [6][ 40/105]    eta: 0:01:21  time: 1.0119  data_time: 0.1092  memory: 2334  
2025/06/25 10:58:24 - mmengine - INFO - Epoch(val)  [6][ 50/105]    eta: 0:01:06  time: 0.9797  data_time: 0.0846  memory: 2334  
2025/06/25 10:58:34 - mmengine - INFO - Epoch(val)  [6][ 60/105]    eta: 0:00:52  time: 0.9971  data_time: 0.0845  memory: 2334  
2025/06/25 10:58:44 - mmengine - INFO - Epoch(val)  [6][ 70/105]    eta: 0:00:40  time: 0.9898  data_time: 0.0879  memory: 2334  
2025/06/25 10:58:54 - mmengine - INFO - Epoch(val)  [6][ 80/105]    eta: 0:00:28  time: 0.9803  data_time: 0.0702  memory: 2334  
2025/06/25 10:59:03 - mmengine - INFO - Epoch(val)  [6][ 90/105]    eta: 0:00:16  time: 0.9572  data_time: 0.0585  memory: 2334  
2025/06/25 10:59:14 - mmengine - INFO - Epoch(val)  [6][100/105]    eta: 0:00:05  time: 1.0795  data_time: 0.1641  memory: 2334  
2025/06/25 10:59:21 - mmengine - INFO - Evaluating segm...
2025/06/25 10:59:30 - mmengine - INFO - segm_mAP_copypaste: 0.540 0.806 0.602 0.325 0.668 0.786
2025/06/25 10:59:30 - mmengine - INFO - Epoch(val) [6][105/105]    coco/segm_mAP: 0.5400  coco/segm_mAP_50: 0.8060  coco/segm_mAP_75: 0.6020  coco/segm_mAP_s: 0.3250  coco/segm_mAP_m: 0.6680  coco/segm_mAP_l: 0.7860  data_time: 0.1723  time: 1.0879
