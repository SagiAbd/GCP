2025/06/26 10:39:31 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: win32
    Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1144607431
    GPU 0: NVIDIA GeForce GTX 1660
    CUDA_HOME: C:\Users\Sagi\Miniconda3\envs\gcp-env\Library
    NVCC: Not Available
    MSVC: Оптимизирующий компилятор Microsoft (R) C/C++ версии 19.43.34810 для x64
    GCC: n/a
    PyTorch: 2.3.0
    PyTorch compiling details: PyTorch built with:
  - C++ Version: 201703
  - MSVC 192930151
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.8.1  (built against CUDA 12.0)
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.8.1, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.18.0
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1144607431
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/06/26 10:39:33 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=4, enable=True)
backend_args = None
batch_augments = [
    dict(
        img_pad_value=0,
        mask_pad_value=0,
        pad_mask=True,
        pad_seg=True,
        seg_pad_value=255,
        size=(
            512,
            512,
        ),
        type='BatchFixedSizePad'),
]
crop_size = (
    512,
    512,
)
data_preprocessor = dict(
    batch_augments=[
        dict(
            img_pad_value=0,
            mask_pad_value=0,
            pad_mask=True,
            pad_seg=True,
            seg_pad_value=255,
            size=(
                512,
                512,
            ),
            type='BatchFixedSizePad'),
    ],
    bgr_to_rgb=True,
    mask_pad_value=0,
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    pad_mask=True,
    pad_seg=True,
    pad_size_divisor=32,
    seg_pad_value=255,
    std=[
        58.395,
        57.12,
        57.375,
    ],
    type='DetDataPreprocessor')
data_root = 'data/kostanai'
dataset_type = 'WHUMixVectorDataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=True,
        interval=1,
        max_keep_ckpts=3,
        save_last=True,
        type='CheckpointHook'),
    logger=dict(interval=10, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(draw=True, interval=10, type='TanmlhVisualizationHook'))
default_scope = 'mmdet'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
img_norm_cfg = dict(
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    std=[
        58.395,
        57.12,
        57.375,
    ],
    to_rgb=True)
launcher = 'none'
load_from = None
log_config = dict(hooks=[
    dict(type='TextLoggerHook'),
    dict(
        init_kwargs=dict(
            allow_val_change=True,
            group='mask2former_training',
            id='yymtrhr6',
            name='mask2former_e1_lre-5_kostanai_afs_quicktest',
            project='building-segmentation-gcp',
            resume='must'),
        interval=3,
        log_checkpoint=True,
        log_checkpoint_metadata=True,
        num_eval_images=10,
        type='MMDetWandbHook'),
])
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=10)
max_epochs = 10
model = dict(
    backbone=dict(
        depth=50,
        frozen_stages=2,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=False, type='BN'),
        norm_eval=True,
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mask_pad_value=0,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=True,
        pad_seg=True,
        pad_size_divisor=32,
        seg_pad_value=255,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    init_cfg=None,
    panoptic_fusion_head=dict(
        init_cfg=None,
        loss_panoptic=None,
        num_stuff_classes=0,
        num_things_classes=1,
        type='MaskFormerFusionHead'),
    panoptic_head=dict(
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=True),
        num_queries=300,
        num_stuff_classes=0,
        num_things_classes=1,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(
        filter_low_score=False,
        instance_on=True,
        iou_thr=0.8,
        max_per_image=200,
        panoptic_on=False,
        semantic_on=False),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='ClassificationCost', weight=2.0),
                dict(
                    type='CrossEntropyLossCost', use_sigmoid=True, weight=5.0),
                dict(eps=1.0, pred_act=True, type='DiceCost', weight=5.0),
            ],
            type='HungarianAssigner'),
        importance_sample_ratio=0.75,
        num_points=12544,
        oversample_ratio=3.0,
        sampler=dict(type='MaskPseudoSampler')),
    type='Mask2Former')
num_classes = 1
num_stuff_classes = 0
num_things_classes = 1
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=1e-05,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(decay_mult=1.0, lr_mult=0.01),
            level_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_feat=dict(decay_mult=0.0, lr_mult=1.0)),
        norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0, by_epoch=False, end=1000, start_factor=0.001,
        type='LinearLR'),
    dict(
        begin=0,
        by_epoch=True,
        end=10,
        gamma=0.1,
        milestones=[
            40,
        ],
        type='MultiStepLR'),
]
resume = True
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=2,
    dataset=dict(
        ann_file='test/test.json',
        backend_args=None,
        data_prefix=dict(img='test/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = [
    dict(
        ann_file='data/kostanai/test/test.json',
        backend_args=None,
        metric=[
            'segm',
        ],
        type='CocoMetric'),
]
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=False,
        with_mask=True,
        with_poly_json=False),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=10, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=4,
    dataset=dict(
        ann_file='train/train.json',
        backend_args=None,
        data_prefix=dict(img='train/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True,
                with_poly_json=False),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                direction=[
                    'horizontal',
                    'vertical',
                    'diagonal',
                ],
                prob=0.75,
                type='RandomFlip'),
            dict(prob=0.75, type='Rotate90'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        type='WHUMixVectorDataset'),
    num_workers=2,
    persistent_workers=False,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True,
        with_poly_json=False),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        direction=[
            'horizontal',
            'vertical',
            'diagonal',
        ],
        prob=0.75,
        type='RandomFlip'),
    dict(prob=0.75, type='Rotate90'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=2,
    dataset=dict(
        ann_file='val/val.json',
        backend_args=None,
        data_prefix=dict(img='val/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = [
    dict(
        ann_file='data/kostanai/val/val.json',
        backend_args=None,
        metric=[
            'segm',
        ],
        type='CocoMetric'),
]
vis_backends = [
    dict(
        init_kwargs=dict(
            allow_val_change=True,
            group='mask2former_training',
            id='yymtrhr6',
            name='mask2former_e1_lre-5_kostanai_afs_quicktest',
            project='building-segmentation-gcp',
            resume='must'),
        save_dir=
        'work_dir\\mask2former_training\\mask2former_e1_lre-5_kostanai_afs_quicktest\\wandb',
        type='WandbVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='TanmlhVisualizer',
    vis_backends=[
        dict(
            init_kwargs=dict(
                allow_val_change=True,
                group='mask2former_training',
                id='yymtrhr6',
                name='mask2former_e1_lre-5_kostanai_afs_quicktest',
                project='building-segmentation-gcp',
                resume='must'),
            save_dir=
            'work_dir\\mask2former_training\\mask2former_e1_lre-5_kostanai_afs_quicktest\\wandb',
            type='WandbVisBackend'),
    ])
work_dir = 'work_dir\\mask2former_training\\mask2former_e1_lre-5_kostanai_afs_quicktest'

2025/06/26 10:39:43 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/06/26 10:39:43 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/06/26 10:39:48 - mmengine - WARNING - backbone.conv1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.conv1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.conv2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.conv3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.downsample.0.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.1.conv1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.1.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.1.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.1.conv2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.1.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.1.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.1.conv3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.1.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.1.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.2.conv1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.2.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.2.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.2.conv2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.2.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.2.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.2.conv3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.2.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer1.2.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.conv1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.conv2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.conv3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.downsample.0.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.1.conv1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.1.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.1.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.1.conv2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.1.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.1.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.1.conv3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.1.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.1.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.2.conv1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.2.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.2.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.2.conv2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.2.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.2.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.2.conv3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.2.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.2.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.3.conv1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.3.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.3.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.3.conv2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.3.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.3.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.3.conv3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.3.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer2.3.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.0.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.0.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.0.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.0.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.0.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.0.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.1.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.1.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.1.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.1.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.1.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.1.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.2.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.2.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.2.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.2.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.2.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.2.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.3.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.3.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.01
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.3.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - WARNING - backbone.layer3.3.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:48 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.3.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.3.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.4.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.4.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.4.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.4.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.4.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.4.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.5.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.5.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.5.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.5.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.5.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer3.5.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.0.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.0.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.0.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.0.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.0.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.0.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.1.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.1.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.1.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.1.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.1.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.1.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.2.bn1.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.2.bn1.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.2.bn2.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.2.bn2.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1.0000000000000001e-07
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.01
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.2.bn3.weight is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - WARNING - backbone.layer4.2.bn3.bias is skipped since its requires_grad=False
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.bias:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr=1e-05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr_mult=1.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:decay_mult=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr=1e-05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr_mult=1.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:decay_mult=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr=1e-05
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:weight_decay=0.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr_mult=1.0
2025/06/26 10:39:49 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:decay_mult=0.0
2025/06/26 10:39:49 - mmengine - INFO - LR is set based on batch size of 4 and the current batch size is 4. Scaling the original LR by 1.0.
2025/06/26 10:39:52 - mmengine - INFO - load model from: torchvision://resnet50
2025/06/26 10:39:52 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
2025/06/26 10:39:52 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

panoptic_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 512, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

panoptic_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.query_embed.weight - torch.Size([300, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.query_feat.weight - torch.Size([300, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.cls_embed.weight - torch.Size([2, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.cls_embed.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Mask2Former  

panoptic_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Mask2Former  
2025/06/26 10:39:53 - mmengine - INFO - Auto resumed from the latest checkpoint D:\Sagi\GCP\GCP\work_dir\mask2former_training\mask2former_e1_lre-5_kostanai_afs_quicktest\epoch_2.pth.
2025/06/26 10:39:59 - mmengine - INFO - Load checkpoint from D:\Sagi\GCP\GCP\work_dir\mask2former_training\mask2former_e1_lre-5_kostanai_afs_quicktest\epoch_2.pth
2025/06/26 10:39:59 - mmengine - INFO - resumed epoch: 2, iter: 942
2025/06/26 10:39:59 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/06/26 10:39:59 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/06/26 10:39:59 - mmengine - INFO - Checkpoints will be saved to D:\Sagi\GCP\GCP\work_dir\mask2former_training\mask2former_e1_lre-5_kostanai_afs_quicktest.
2025/06/26 10:40:25 - mmengine - INFO - Epoch(train)  [3][ 10/471]  base_lr: 9.5200e-06 lr: 9.5200e-08  eta: 2:47:52  time: 2.6804  data_time: 0.8564  memory: 5812  grad_norm: 35.6437  loss: 1.1611  loss_cls: 0.2437  loss_mask: 0.0626  loss_dice: 0.8548
2025/06/26 10:40:44 - mmengine - INFO - Epoch(train)  [3][ 20/471]  base_lr: 9.6200e-06 lr: 9.6200e-08  eta: 2:21:34  time: 1.8526  data_time: 0.0033  memory: 6137  grad_norm: 28.1828  loss: 1.2718  loss_cls: 0.3033  loss_mask: 0.0875  loss_dice: 0.8810
2025/06/26 10:41:02 - mmengine - INFO - Epoch(train)  [3][ 30/471]  base_lr: 9.7200e-06 lr: 9.7200e-08  eta: 2:12:26  time: 1.8449  data_time: 0.0031  memory: 5819  grad_norm: 46.5901  loss: 1.3008  loss_cls: 0.3082  loss_mask: 0.1084  loss_dice: 0.8842
2025/06/26 10:41:21 - mmengine - INFO - Epoch(train)  [3][ 40/471]  base_lr: 9.8200e-06 lr: 9.8200e-08  eta: 2:07:13  time: 1.8123  data_time: 0.0055  memory: 6137  grad_norm: 21.7742  loss: 1.2595  loss_cls: 0.2934  loss_mask: 0.0969  loss_dice: 0.8692
2025/06/26 10:41:39 - mmengine - INFO - Epoch(train)  [3][ 50/471]  base_lr: 9.9200e-06 lr: 9.9200e-08  eta: 2:04:13  time: 1.8330  data_time: 0.0026  memory: 6114  grad_norm: 41.0903  loss: 1.3372  loss_cls: 0.2741  loss_mask: 0.0930  loss_dice: 0.9702
2025/06/26 10:41:53 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250626_103923
2025/06/26 10:41:57 - mmengine - INFO - Epoch(train)  [3][ 60/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 2:01:37  time: 1.7849  data_time: 0.0039  memory: 6123  grad_norm: 14.7804  loss: 1.1464  loss_cls: 0.2529  loss_mask: 0.0950  loss_dice: 0.7985
2025/06/26 10:42:15 - mmengine - INFO - Epoch(train)  [3][ 70/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:59:45  time: 1.7935  data_time: 0.0026  memory: 6245  grad_norm: 45.2152  loss: 1.0770  loss_cls: 0.2115  loss_mask: 0.0722  loss_dice: 0.7933
2025/06/26 10:42:32 - mmengine - INFO - Epoch(train)  [3][ 80/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:57:56  time: 1.7497  data_time: 0.0024  memory: 5835  grad_norm: 37.7766  loss: 1.1459  loss_cls: 0.2630  loss_mask: 0.0870  loss_dice: 0.7959
2025/06/26 10:42:51 - mmengine - INFO - Epoch(train)  [3][ 90/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:57:08  time: 1.8473  data_time: 0.0021  memory: 6194  grad_norm: 14.8794  loss: 1.1599  loss_cls: 0.2712  loss_mask: 0.0844  loss_dice: 0.8043
2025/06/26 10:43:09 - mmengine - INFO - Epoch(train)  [3][100/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:56:16  time: 1.8216  data_time: 0.0061  memory: 6562  grad_norm: 24.4662  loss: 1.2247  loss_cls: 0.2655  loss_mask: 0.0943  loss_dice: 0.8649
2025/06/26 10:43:27 - mmengine - INFO - Epoch(train)  [3][110/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:55:31  time: 1.8251  data_time: 0.0047  memory: 6040  grad_norm: 22.7132  loss: 1.3220  loss_cls: 0.3068  loss_mask: 0.1037  loss_dice: 0.9115
2025/06/26 10:43:45 - mmengine - INFO - Epoch(train)  [3][120/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:54:46  time: 1.8071  data_time: 0.0015  memory: 6091  grad_norm: 33.1984  loss: 1.1486  loss_cls: 0.2620  loss_mask: 0.0671  loss_dice: 0.8196
2025/06/26 10:44:03 - mmengine - INFO - Epoch(train)  [3][130/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:54:06  time: 1.8130  data_time: 0.0039  memory: 6076  grad_norm: 41.0146  loss: 1.2753  loss_cls: 0.2741  loss_mask: 0.0827  loss_dice: 0.9184
2025/06/26 10:44:21 - mmengine - INFO - Epoch(train)  [3][140/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:53:30  time: 1.8169  data_time: 0.0021  memory: 5579  grad_norm: 30.5675  loss: 1.2449  loss_cls: 0.2866  loss_mask: 0.0810  loss_dice: 0.8774
2025/06/26 10:44:39 - mmengine - INFO - Epoch(train)  [3][150/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:52:46  time: 1.7695  data_time: 0.0027  memory: 6003  grad_norm: 48.1894  loss: 1.3409  loss_cls: 0.2710  loss_mask: 0.1644  loss_dice: 0.9056
2025/06/26 10:44:57 - mmengine - INFO - Epoch(train)  [3][160/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:52:16  time: 1.8226  data_time: 0.0051  memory: 5797  grad_norm: 26.7053  loss: 1.1610  loss_cls: 0.2739  loss_mask: 0.0810  loss_dice: 0.8061
2025/06/26 10:45:16 - mmengine - INFO - Epoch(train)  [3][170/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:51:47  time: 1.8195  data_time: 0.0015  memory: 6223  grad_norm: 21.5402  loss: 1.1792  loss_cls: 0.2698  loss_mask: 0.0792  loss_dice: 0.8302
2025/06/26 10:45:34 - mmengine - INFO - Epoch(train)  [3][180/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:51:22  time: 1.8301  data_time: 0.0023  memory: 6083  grad_norm: 11.1116  loss: 1.1796  loss_cls: 0.2674  loss_mask: 0.0716  loss_dice: 0.8406
2025/06/26 10:45:52 - mmengine - INFO - Epoch(train)  [3][190/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:50:45  time: 1.7641  data_time: 0.0013  memory: 5723  grad_norm: 31.7123  loss: 1.2303  loss_cls: 0.2564  loss_mask: 0.1029  loss_dice: 0.8710
2025/06/26 10:46:10 - mmengine - INFO - Epoch(train)  [3][200/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:50:27  time: 1.8633  data_time: 0.0051  memory: 6091  grad_norm: 21.9871  loss: 1.2847  loss_cls: 0.3268  loss_mask: 0.0978  loss_dice: 0.8600
2025/06/26 10:46:29 - mmengine - INFO - Epoch(train)  [3][210/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:50:06  time: 1.8429  data_time: 0.0031  memory: 6076  grad_norm: 17.0431  loss: 1.4342  loss_cls: 0.3006  loss_mask: 0.1125  loss_dice: 1.0210
2025/06/26 10:46:47 - mmengine - INFO - Epoch(train)  [3][220/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:49:41  time: 1.8164  data_time: 0.0086  memory: 5775  grad_norm: 67.3562  loss: 1.1776  loss_cls: 0.2387  loss_mask: 0.0962  loss_dice: 0.8427
2025/06/26 10:47:05 - mmengine - INFO - Epoch(train)  [3][230/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:49:23  time: 1.8607  data_time: 0.0021  memory: 6503  grad_norm: 50.4585  loss: 1.2285  loss_cls: 0.2896  loss_mask: 0.0725  loss_dice: 0.8663
2025/06/26 10:47:24 - mmengine - INFO - Epoch(train)  [3][240/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:49:00  time: 1.8207  data_time: 0.0037  memory: 5786  grad_norm: 294.5575  loss: 1.0977  loss_cls: 0.2136  loss_mask: 0.0841  loss_dice: 0.8001
2025/06/26 10:47:42 - mmengine - INFO - Epoch(train)  [3][250/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:48:35  time: 1.8065  data_time: 0.0034  memory: 5841  grad_norm: 25.8839  loss: 1.2213  loss_cls: 0.2473  loss_mask: 0.1024  loss_dice: 0.8716
2025/06/26 10:48:00 - mmengine - INFO - Epoch(train)  [3][260/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:48:14  time: 1.8368  data_time: 0.0029  memory: 6657  grad_norm: 49.9768  loss: 1.1688  loss_cls: 0.2548  loss_mask: 0.1117  loss_dice: 0.8022
2025/06/26 10:48:17 - mmengine - INFO - Epoch(train)  [3][270/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:47:40  time: 1.7351  data_time: 0.0030  memory: 6100  grad_norm: 22.5152  loss: 1.2457  loss_cls: 0.2636  loss_mask: 0.1171  loss_dice: 0.8650
2025/06/26 10:48:36 - mmengine - INFO - Epoch(train)  [3][280/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:47:20  time: 1.8337  data_time: 0.0048  memory: 5973  grad_norm: 24.5090  loss: 1.3645  loss_cls: 0.2951  loss_mask: 0.0936  loss_dice: 0.9758
2025/06/26 10:48:54 - mmengine - INFO - Epoch(train)  [3][290/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:46:59  time: 1.8221  data_time: 0.0029  memory: 6076  grad_norm: 33.0569  loss: 1.1700  loss_cls: 0.2441  loss_mask: 0.0894  loss_dice: 0.8365
2025/06/26 10:49:12 - mmengine - INFO - Epoch(train)  [3][300/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:46:39  time: 1.8306  data_time: 0.0033  memory: 6436  grad_norm: 22.7924  loss: 1.3067  loss_cls: 0.2925  loss_mask: 0.0901  loss_dice: 0.9242
2025/06/26 10:49:30 - mmengine - INFO - Epoch(train)  [3][310/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:46:16  time: 1.8068  data_time: 0.0040  memory: 5864  grad_norm: 19.3765  loss: 1.2434  loss_cls: 0.2903  loss_mask: 0.1019  loss_dice: 0.8512
2025/06/26 10:49:49 - mmengine - INFO - Epoch(train)  [3][320/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:45:57  time: 1.8377  data_time: 0.0049  memory: 6459  grad_norm: 15.8436  loss: 1.0872  loss_cls: 0.2424  loss_mask: 0.0874  loss_dice: 0.7574
2025/06/26 10:50:07 - mmengine - INFO - Epoch(train)  [3][330/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:45:33  time: 1.7932  data_time: 0.0016  memory: 5937  grad_norm: 19.0187  loss: 1.2771  loss_cls: 0.2754  loss_mask: 0.1145  loss_dice: 0.8873
2025/06/26 10:50:25 - mmengine - INFO - Epoch(train)  [3][340/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:45:12  time: 1.8180  data_time: 0.0032  memory: 6025  grad_norm: 36.2261  loss: 1.1493  loss_cls: 0.2761  loss_mask: 0.0775  loss_dice: 0.7957
2025/06/26 10:50:45 - mmengine - INFO - Epoch(train)  [3][350/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:45:07  time: 1.9738  data_time: 0.0040  memory: 6070  grad_norm: 10.6902  loss: 1.1996  loss_cls: 0.2687  loss_mask: 0.0752  loss_dice: 0.8557
2025/06/26 10:51:03 - mmengine - INFO - Epoch(train)  [3][360/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:44:47  time: 1.8307  data_time: 0.0032  memory: 5650  grad_norm: 51.0101  loss: 1.1927  loss_cls: 0.2425  loss_mask: 0.1621  loss_dice: 0.7881
2025/06/26 10:51:22 - mmengine - INFO - Epoch(train)  [3][370/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:44:36  time: 1.9249  data_time: 0.0046  memory: 5848  grad_norm: 24.5369  loss: 1.0586  loss_cls: 0.2114  loss_mask: 0.0832  loss_dice: 0.7640
2025/06/26 10:51:41 - mmengine - INFO - Epoch(train)  [3][380/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:44:24  time: 1.9230  data_time: 0.0017  memory: 5958  grad_norm: 98.9583  loss: 1.1257  loss_cls: 0.2286  loss_mask: 0.0762  loss_dice: 0.8209
2025/06/26 10:52:00 - mmengine - INFO - Epoch(train)  [3][390/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:44:04  time: 1.8350  data_time: 0.0042  memory: 6209  grad_norm: 51.0467  loss: 1.2438  loss_cls: 0.2827  loss_mask: 0.0766  loss_dice: 0.8845
2025/06/26 10:52:19 - mmengine - INFO - Epoch(train)  [3][400/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:43:56  time: 1.9699  data_time: 0.0036  memory: 6289  grad_norm: 111.0008  loss: 1.1539  loss_cls: 0.2353  loss_mask: 0.0813  loss_dice: 0.8373
2025/06/26 10:52:40 - mmengine - INFO - Epoch(train)  [3][410/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:43:52  time: 2.0249  data_time: 0.0057  memory: 6123  grad_norm: 12.2430  loss: 1.1739  loss_cls: 0.2683  loss_mask: 0.0747  loss_dice: 0.8308
2025/06/26 10:53:00 - mmengine - INFO - Epoch(train)  [3][420/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:43:45  time: 2.0001  data_time: 0.0023  memory: 6635  grad_norm: 20.1414  loss: 1.2386  loss_cls: 0.2628  loss_mask: 0.0983  loss_dice: 0.8775
2025/06/26 10:53:17 - mmengine - INFO - Epoch(train)  [3][430/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:43:14  time: 1.7086  data_time: 0.0016  memory: 5848  grad_norm: 90.9963  loss: 1.1840  loss_cls: 0.2030  loss_mask: 0.1164  loss_dice: 0.8646
2025/06/26 10:53:34 - mmengine - INFO - Epoch(train)  [3][440/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:42:49  time: 1.7627  data_time: 0.0028  memory: 5761  grad_norm: 16.9636  loss: 1.0722  loss_cls: 0.2242  loss_mask: 0.0820  loss_dice: 0.7660
2025/06/26 10:53:54 - mmengine - INFO - Epoch(train)  [3][450/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:42:36  time: 1.9243  data_time: 0.0028  memory: 5863  grad_norm: 47.0373  loss: 1.1984  loss_cls: 0.2971  loss_mask: 0.0776  loss_dice: 0.8237
2025/06/26 10:54:14 - mmengine - INFO - Epoch(train)  [3][460/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:42:27  time: 2.0007  data_time: 0.0028  memory: 6724  grad_norm: 39.2806  loss: 1.1682  loss_cls: 0.3147  loss_mask: 0.0778  loss_dice: 0.7757
2025/06/26 10:54:34 - mmengine - INFO - Epoch(train)  [3][470/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:42:23  time: 2.0627  data_time: 0.0043  memory: 6105  grad_norm: 20.6962  loss: 1.2726  loss_cls: 0.2890  loss_mask: 0.0697  loss_dice: 0.9139
2025/06/26 10:54:36 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250626_103923
2025/06/26 10:54:37 - mmengine - INFO - Saving checkpoint at 3 epochs
2025/06/26 10:55:09 - mmengine - INFO - Epoch(val)  [3][10/53]    eta: 0:01:33  time: 2.1670  data_time: 1.0765  memory: 5450  
2025/06/26 10:55:20 - mmengine - INFO - Epoch(val)  [3][20/53]    eta: 0:00:54  time: 1.1211  data_time: 0.1136  memory: 2334  
2025/06/26 10:55:32 - mmengine - INFO - Epoch(val)  [3][30/53]    eta: 0:00:34  time: 1.1913  data_time: 0.0945  memory: 2334  
2025/06/26 10:55:44 - mmengine - INFO - Epoch(val)  [3][40/53]    eta: 0:00:18  time: 1.2097  data_time: 0.1714  memory: 2334  
2025/06/26 10:55:56 - mmengine - INFO - Epoch(val)  [3][50/53]    eta: 0:00:04  time: 1.1967  data_time: 0.1225  memory: 2334  
2025/06/26 10:56:00 - mmengine - INFO - Evaluating segm...
2025/06/26 10:56:06 - mmengine - INFO - segm_mAP_copypaste: 0.516 0.765 0.564 0.296 0.669 0.813
2025/06/26 10:56:06 - mmengine - INFO - Epoch(val) [3][53/53]    coco/segm_mAP: 0.5160  coco/segm_mAP_50: 0.7650  coco/segm_mAP_75: 0.5640  coco/segm_mAP_s: 0.2960  coco/segm_mAP_m: 0.6690  coco/segm_mAP_l: 0.8130  data_time: 0.2924  time: 1.3267
2025/06/26 10:56:35 - mmengine - INFO - Epoch(train)  [4][ 10/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:43:17  time: 2.9170  data_time: 0.8751  memory: 6032  grad_norm: 18.1030  loss: 1.1708  loss_cls: 0.2624  loss_mask: 0.0820  loss_dice: 0.8263
2025/06/26 10:56:56 - mmengine - INFO - Epoch(train)  [4][ 20/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:43:11  time: 2.0776  data_time: 0.0039  memory: 5841  grad_norm: 48.1765  loss: 1.2231  loss_cls: 0.2510  loss_mask: 0.0920  loss_dice: 0.8800
2025/06/26 10:57:17 - mmengine - INFO - Epoch(train)  [4][ 30/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:43:05  time: 2.0903  data_time: 0.0029  memory: 6061  grad_norm: 19.9307  loss: 1.0956  loss_cls: 0.2145  loss_mask: 0.0813  loss_dice: 0.7998
2025/06/26 10:57:38 - mmengine - INFO - Epoch(train)  [4][ 40/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:42:59  time: 2.1034  data_time: 0.0028  memory: 6333  grad_norm: 15.5129  loss: 1.2810  loss_cls: 0.3335  loss_mask: 0.0746  loss_dice: 0.8728
2025/06/26 10:57:59 - mmengine - INFO - Epoch(train)  [4][ 50/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:42:53  time: 2.0999  data_time: 0.0021  memory: 6201  grad_norm: 38.8180  loss: 1.1678  loss_cls: 0.2462  loss_mask: 0.0963  loss_dice: 0.8253
2025/06/26 10:58:19 - mmengine - INFO - Epoch(train)  [4][ 60/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:42:40  time: 2.0076  data_time: 0.0032  memory: 5966  grad_norm: 11.3412  loss: 1.1854  loss_cls: 0.2317  loss_mask: 0.0857  loss_dice: 0.8680
2025/06/26 10:58:39 - mmengine - INFO - Epoch(train)  [4][ 70/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:42:30  time: 2.0422  data_time: 0.0049  memory: 5900  grad_norm: 15.6139  loss: 1.2256  loss_cls: 0.3050  loss_mask: 0.0768  loss_dice: 0.8438
2025/06/26 10:58:59 - mmengine - INFO - Epoch(train)  [4][ 80/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:42:17  time: 2.0136  data_time: 0.0028  memory: 5908  grad_norm: 10.4702  loss: 1.1310  loss_cls: 0.2603  loss_mask: 0.0782  loss_dice: 0.7924
2025/06/26 10:59:18 - mmengine - INFO - Epoch(train)  [4][ 90/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:41:57  time: 1.9025  data_time: 0.0036  memory: 5538  grad_norm: 44.6472  loss: 1.0260  loss_cls: 0.2304  loss_mask: 0.1004  loss_dice: 0.6952
2025/06/26 10:59:38 - mmengine - INFO - Epoch(train)  [4][100/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:41:40  time: 1.9355  data_time: 0.0040  memory: 6137  grad_norm: 38.9383  loss: 1.1218  loss_cls: 0.2082  loss_mask: 0.0979  loss_dice: 0.8158
2025/06/26 10:59:59 - mmengine - INFO - Epoch(train)  [4][110/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:41:34  time: 2.1391  data_time: 0.0035  memory: 6473  grad_norm: 16.1507  loss: 1.2848  loss_cls: 0.3211  loss_mask: 0.0678  loss_dice: 0.8959
2025/06/26 11:00:20 - mmengine - INFO - Epoch(train)  [4][120/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:41:24  time: 2.0852  data_time: 0.0022  memory: 6114  grad_norm: 28.6244  loss: 1.2750  loss_cls: 0.2565  loss_mask: 0.0956  loss_dice: 0.9229
2025/06/26 11:00:39 - mmengine - INFO - Epoch(train)  [4][130/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:41:06  time: 1.9510  data_time: 0.0036  memory: 5782  grad_norm: 26.1397  loss: 1.2199  loss_cls: 0.2679  loss_mask: 0.0766  loss_dice: 0.8753
2025/06/26 11:00:59 - mmengine - INFO - Epoch(train)  [4][140/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:40:50  time: 1.9593  data_time: 0.0031  memory: 6746  grad_norm: 56.0273  loss: 1.1660  loss_cls: 0.2435  loss_mask: 0.0776  loss_dice: 0.8450
2025/06/26 11:01:18 - mmengine - INFO - Epoch(train)  [4][150/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:40:32  time: 1.9482  data_time: 0.0033  memory: 6392  grad_norm: 21.6934  loss: 1.1488  loss_cls: 0.2635  loss_mask: 0.0745  loss_dice: 0.8108
2025/06/26 11:01:38 - mmengine - INFO - Epoch(train)  [4][160/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:40:12  time: 1.9086  data_time: 0.0027  memory: 5980  grad_norm: 32.9139  loss: 1.4013  loss_cls: 0.3428  loss_mask: 0.1160  loss_dice: 0.9424
2025/06/26 11:01:57 - mmengine - INFO - Epoch(train)  [4][170/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:39:53  time: 1.9159  data_time: 0.0031  memory: 5848  grad_norm: 24.7891  loss: 1.2755  loss_cls: 0.2951  loss_mask: 0.0917  loss_dice: 0.8887
2025/06/26 11:02:17 - mmengine - INFO - Epoch(train)  [4][180/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:39:41  time: 2.0572  data_time: 0.0028  memory: 6797  grad_norm: 20.9340  loss: 1.4089  loss_cls: 0.3458  loss_mask: 0.0958  loss_dice: 0.9672
2025/06/26 11:02:36 - mmengine - INFO - Epoch(train)  [4][190/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:39:17  time: 1.8224  data_time: 0.0037  memory: 6268  grad_norm: 77.8197  loss: 1.2937  loss_cls: 0.3086  loss_mask: 0.0892  loss_dice: 0.8960
2025/06/26 11:02:55 - mmengine - INFO - Epoch(train)  [4][200/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:38:59  time: 1.9337  data_time: 0.0054  memory: 6451  grad_norm: 44.6685  loss: 1.2050  loss_cls: 0.2764  loss_mask: 0.0792  loss_dice: 0.8495
2025/06/26 11:03:14 - mmengine - INFO - Epoch(train)  [4][210/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:38:39  time: 1.9129  data_time: 0.0017  memory: 6223  grad_norm: 14.3607  loss: 1.2701  loss_cls: 0.2724  loss_mask: 0.1047  loss_dice: 0.8930
2025/06/26 11:03:32 - mmengine - INFO - Epoch(train)  [4][220/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:38:13  time: 1.7572  data_time: 0.0027  memory: 6304  grad_norm: 340.6512  loss: 1.1887  loss_cls: 0.2445  loss_mask: 0.1040  loss_dice: 0.8402
2025/06/26 11:03:49 - mmengine - INFO - Epoch(train)  [4][230/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:37:47  time: 1.7603  data_time: 0.0039  memory: 6114  grad_norm: 14.0904  loss: 1.1963  loss_cls: 0.2824  loss_mask: 0.0814  loss_dice: 0.8325
2025/06/26 11:04:07 - mmengine - INFO - Epoch(train)  [4][240/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:37:21  time: 1.7450  data_time: 0.0028  memory: 5848  grad_norm: 19.7496  loss: 1.1788  loss_cls: 0.2878  loss_mask: 0.1228  loss_dice: 0.7682
2025/06/26 11:04:25 - mmengine - INFO - Epoch(train)  [4][250/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:36:57  time: 1.8053  data_time: 0.0025  memory: 6436  grad_norm: 19.1282  loss: 1.3642  loss_cls: 0.2951  loss_mask: 0.0807  loss_dice: 0.9884
2025/06/26 11:04:43 - mmengine - INFO - Epoch(train)  [4][260/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:36:36  time: 1.8663  data_time: 0.0039  memory: 6032  grad_norm: 70.9837  loss: 1.1843  loss_cls: 0.3099  loss_mask: 0.0694  loss_dice: 0.8050
2025/06/26 11:05:01 - mmengine - INFO - Epoch(train)  [4][270/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:36:12  time: 1.7730  data_time: 0.0051  memory: 5878  grad_norm: 26.7892  loss: 1.0230  loss_cls: 0.2316  loss_mask: 0.0777  loss_dice: 0.7137
2025/06/26 11:05:19 - mmengine - INFO - Epoch(train)  [4][280/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:35:48  time: 1.7948  data_time: 0.0030  memory: 6341  grad_norm: 17.3235  loss: 1.2038  loss_cls: 0.2661  loss_mask: 0.0673  loss_dice: 0.8703
2025/06/26 11:05:38 - mmengine - INFO - Epoch(train)  [4][290/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:35:28  time: 1.8816  data_time: 0.0029  memory: 5870  grad_norm: 16.3040  loss: 1.1261  loss_cls: 0.2654  loss_mask: 0.0880  loss_dice: 0.7727
2025/06/26 11:05:58 - mmengine - INFO - Epoch(train)  [4][300/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:35:13  time: 2.0165  data_time: 0.0021  memory: 5951  grad_norm: 14.3720  loss: 1.1392  loss_cls: 0.2661  loss_mask: 0.0822  loss_dice: 0.7909
2025/06/26 11:06:16 - mmengine - INFO - Epoch(train)  [4][310/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:34:51  time: 1.8122  data_time: 0.0027  memory: 6054  grad_norm: 31.5457  loss: 1.1459  loss_cls: 0.2272  loss_mask: 0.0860  loss_dice: 0.8328
2025/06/26 11:06:34 - mmengine - INFO - Epoch(train)  [4][320/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:34:29  time: 1.8325  data_time: 0.0034  memory: 6693  grad_norm: 32.9435  loss: 1.1024  loss_cls: 0.2560  loss_mask: 0.0776  loss_dice: 0.7688
2025/06/26 11:06:54 - mmengine - INFO - Epoch(train)  [4][330/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:34:12  time: 1.9720  data_time: 0.0018  memory: 6252  grad_norm: 17.1483  loss: 1.1375  loss_cls: 0.2719  loss_mask: 0.0855  loss_dice: 0.7801
2025/06/26 11:07:12 - mmengine - INFO - Epoch(train)  [4][340/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:33:48  time: 1.7689  data_time: 0.0043  memory: 5914  grad_norm: 42.7564  loss: 1.1216  loss_cls: 0.2575  loss_mask: 0.0855  loss_dice: 0.7786
2025/06/26 11:07:30 - mmengine - INFO - Epoch(train)  [4][350/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:33:27  time: 1.8405  data_time: 0.0061  memory: 6129  grad_norm: 20.4493  loss: 1.0953  loss_cls: 0.2158  loss_mask: 0.0887  loss_dice: 0.7908
2025/06/26 11:07:49 - mmengine - INFO - Epoch(train)  [4][360/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:33:07  time: 1.8840  data_time: 0.0017  memory: 6223  grad_norm: 30.6533  loss: 1.2356  loss_cls: 0.2918  loss_mask: 0.0835  loss_dice: 0.8602
2025/06/26 11:08:07 - mmengine - INFO - Epoch(train)  [4][370/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:32:45  time: 1.8240  data_time: 0.0037  memory: 6392  grad_norm: 15.6857  loss: 1.2279  loss_cls: 0.2164  loss_mask: 0.0932  loss_dice: 0.9182
2025/06/26 11:08:25 - mmengine - INFO - Epoch(train)  [4][380/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:32:22  time: 1.7734  data_time: 0.0056  memory: 5863  grad_norm: 23.6125  loss: 1.1710  loss_cls: 0.2561  loss_mask: 0.1171  loss_dice: 0.7978
2025/06/26 11:08:43 - mmengine - INFO - Epoch(train)  [4][390/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:31:59  time: 1.7896  data_time: 0.0046  memory: 6061  grad_norm: 10.8857  loss: 1.1187  loss_cls: 0.2507  loss_mask: 0.0807  loss_dice: 0.7873
2025/06/26 11:09:00 - mmengine - INFO - Epoch(train)  [4][400/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:31:35  time: 1.7488  data_time: 0.0034  memory: 5885  grad_norm: 33.4996  loss: 1.2536  loss_cls: 0.2661  loss_mask: 0.0938  loss_dice: 0.8936
2025/06/26 11:09:19 - mmengine - INFO - Epoch(train)  [4][410/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:31:15  time: 1.8605  data_time: 0.0027  memory: 6077  grad_norm: 81.0758  loss: 1.0538  loss_cls: 0.2229  loss_mask: 0.0636  loss_dice: 0.7673
2025/06/26 11:09:37 - mmengine - INFO - Epoch(train)  [4][420/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:30:52  time: 1.7689  data_time: 0.0023  memory: 5711  grad_norm: 69.2373  loss: 1.0430  loss_cls: 0.1779  loss_mask: 0.0988  loss_dice: 0.7663
2025/06/26 11:09:54 - mmengine - INFO - Epoch(train)  [4][430/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:30:29  time: 1.7637  data_time: 0.0043  memory: 6012  grad_norm: 30.8724  loss: 1.2407  loss_cls: 0.2823  loss_mask: 0.1039  loss_dice: 0.8544
2025/06/26 11:10:13 - mmengine - INFO - Epoch(train)  [4][440/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:30:08  time: 1.8185  data_time: 0.0022  memory: 5951  grad_norm: 21.3702  loss: 1.1923  loss_cls: 0.2943  loss_mask: 0.0755  loss_dice: 0.8226
2025/06/26 11:10:30 - mmengine - INFO - Epoch(train)  [4][450/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:29:45  time: 1.7620  data_time: 0.0034  memory: 5938  grad_norm: 48.1885  loss: 1.2069  loss_cls: 0.2619  loss_mask: 0.0880  loss_dice: 0.8570
2025/06/26 11:10:49 - mmengine - INFO - Epoch(train)  [4][460/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:29:24  time: 1.8373  data_time: 0.0035  memory: 6172  grad_norm: 23.9888  loss: 1.1039  loss_cls: 0.2718  loss_mask: 0.0669  loss_dice: 0.7652
2025/06/26 11:11:07 - mmengine - INFO - Epoch(train)  [4][470/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:29:03  time: 1.8199  data_time: 0.0040  memory: 5812  grad_norm: 17.4593  loss: 1.1119  loss_cls: 0.2478  loss_mask: 0.0807  loss_dice: 0.7835
2025/06/26 11:11:09 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250626_103923
2025/06/26 11:11:09 - mmengine - INFO - Saving checkpoint at 4 epochs
2025/06/26 11:11:40 - mmengine - INFO - Epoch(val)  [4][10/53]    eta: 0:01:20  time: 1.8742  data_time: 0.9518  memory: 5716  
2025/06/26 11:11:50 - mmengine - INFO - Epoch(val)  [4][20/53]    eta: 0:00:47  time: 0.9843  data_time: 0.0870  memory: 2334  
2025/06/26 11:11:59 - mmengine - INFO - Epoch(val)  [4][30/53]    eta: 0:00:29  time: 0.9768  data_time: 0.0688  memory: 2334  
2025/06/26 11:12:10 - mmengine - INFO - Epoch(val)  [4][40/53]    eta: 0:00:15  time: 1.0685  data_time: 0.1625  memory: 2334  
2025/06/26 11:12:21 - mmengine - INFO - Epoch(val)  [4][50/53]    eta: 0:00:03  time: 1.0382  data_time: 0.1339  memory: 2334  
2025/06/26 11:12:24 - mmengine - INFO - Evaluating segm...
2025/06/26 11:12:29 - mmengine - INFO - segm_mAP_copypaste: 0.517 0.770 0.571 0.300 0.669 0.810
2025/06/26 11:12:29 - mmengine - INFO - Epoch(val) [4][53/53]    coco/segm_mAP: 0.5170  coco/segm_mAP_50: 0.7700  coco/segm_mAP_75: 0.5710  coco/segm_mAP_s: 0.3000  coco/segm_mAP_m: 0.6690  coco/segm_mAP_l: 0.8100  data_time: 0.2602  time: 1.1503
2025/06/26 11:12:55 - mmengine - INFO - Epoch(train)  [5][ 10/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:29:03  time: 2.6011  data_time: 0.7984  memory: 6099  grad_norm: 18.8811  loss: 1.1735  loss_cls: 0.2753  loss_mask: 0.0800  loss_dice: 0.8183
2025/06/26 11:13:14 - mmengine - INFO - Epoch(train)  [5][ 20/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:28:45  time: 1.9155  data_time: 0.0033  memory: 5826  grad_norm: 18.9222  loss: 1.1973  loss_cls: 0.2276  loss_mask: 0.0845  loss_dice: 0.8853
2025/06/26 11:13:33 - mmengine - INFO - Epoch(train)  [5][ 30/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:28:24  time: 1.8491  data_time: 0.0044  memory: 6429  grad_norm: 15.6589  loss: 1.1749  loss_cls: 0.2568  loss_mask: 0.0913  loss_dice: 0.8268
2025/06/26 11:13:51 - mmengine - INFO - Epoch(train)  [5][ 40/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:28:02  time: 1.7896  data_time: 0.0008  memory: 5951  grad_norm: 36.6506  loss: 1.2343  loss_cls: 0.2768  loss_mask: 0.0850  loss_dice: 0.8725
2025/06/26 11:14:08 - mmengine - INFO - Epoch(train)  [5][ 50/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:27:38  time: 1.7152  data_time: 0.0037  memory: 5682  grad_norm: 17.5303  loss: 1.3108  loss_cls: 0.2920  loss_mask: 0.0922  loss_dice: 0.9265
2025/06/26 11:14:26 - mmengine - INFO - Epoch(train)  [5][ 60/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:27:18  time: 1.8550  data_time: 0.0031  memory: 6076  grad_norm: 11.1311  loss: 1.3454  loss_cls: 0.3267  loss_mask: 0.0839  loss_dice: 0.9347
2025/06/26 11:14:44 - mmengine - INFO - Epoch(train)  [5][ 70/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:26:57  time: 1.7990  data_time: 0.0083  memory: 5936  grad_norm: 83.2234  loss: 1.2798  loss_cls: 0.3027  loss_mask: 0.0964  loss_dice: 0.8806
2025/06/26 11:15:02 - mmengine - INFO - Epoch(train)  [5][ 80/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:26:36  time: 1.8106  data_time: 0.0038  memory: 6260  grad_norm: 18.1704  loss: 1.2087  loss_cls: 0.2528  loss_mask: 0.1116  loss_dice: 0.8443
2025/06/26 11:15:20 - mmengine - INFO - Epoch(train)  [5][ 90/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:26:13  time: 1.7579  data_time: 0.0045  memory: 5782  grad_norm: 11.8158  loss: 1.1611  loss_cls: 0.2565  loss_mask: 0.0730  loss_dice: 0.8317
2025/06/26 11:15:39 - mmengine - INFO - Epoch(train)  [5][100/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:25:54  time: 1.8698  data_time: 0.0043  memory: 6091  grad_norm: 35.8593  loss: 0.9981  loss_cls: 0.1876  loss_mask: 0.0735  loss_dice: 0.7371
2025/06/26 11:15:58 - mmengine - INFO - Epoch(train)  [5][110/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:25:36  time: 1.9285  data_time: 0.0020  memory: 6172  grad_norm: 25.0539  loss: 1.1769  loss_cls: 0.2654  loss_mask: 0.0727  loss_dice: 0.8388
2025/06/26 11:16:09 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250626_103923
2025/06/26 11:16:16 - mmengine - INFO - Epoch(train)  [5][120/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:25:14  time: 1.7651  data_time: 0.0032  memory: 6144  grad_norm: 27.9821  loss: 1.1051  loss_cls: 0.2407  loss_mask: 0.0698  loss_dice: 0.7945
2025/06/26 11:16:34 - mmengine - INFO - Epoch(train)  [5][130/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:24:52  time: 1.7879  data_time: 0.0020  memory: 6304  grad_norm: 22.7532  loss: 1.1334  loss_cls: 0.2790  loss_mask: 0.0870  loss_dice: 0.7674
2025/06/26 11:16:51 - mmengine - INFO - Epoch(train)  [5][140/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:24:29  time: 1.7245  data_time: 0.0016  memory: 5914  grad_norm: 20.7083  loss: 1.0979  loss_cls: 0.2171  loss_mask: 0.0785  loss_dice: 0.8022
2025/06/26 11:17:08 - mmengine - INFO - Epoch(train)  [5][150/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:24:07  time: 1.7451  data_time: 0.0019  memory: 5856  grad_norm: 13.4423  loss: 1.1172  loss_cls: 0.2459  loss_mask: 0.0798  loss_dice: 0.7915
2025/06/26 11:17:26 - mmengine - INFO - Epoch(train)  [5][160/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:23:46  time: 1.7842  data_time: 0.0018  memory: 5959  grad_norm: 17.1756  loss: 1.2325  loss_cls: 0.2681  loss_mask: 0.1166  loss_dice: 0.8477
2025/06/26 11:17:45 - mmengine - INFO - Epoch(train)  [5][170/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:23:26  time: 1.8467  data_time: 0.0027  memory: 6201  grad_norm: 30.0366  loss: 1.1551  loss_cls: 0.2725  loss_mask: 0.0763  loss_dice: 0.8063
2025/06/26 11:18:03 - mmengine - INFO - Epoch(train)  [5][180/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:23:05  time: 1.8041  data_time: 0.0039  memory: 5716  grad_norm: 17.7482  loss: 1.0227  loss_cls: 0.2292  loss_mask: 0.0717  loss_dice: 0.7218
2025/06/26 11:18:21 - mmengine - INFO - Epoch(train)  [5][190/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:22:44  time: 1.7990  data_time: 0.0039  memory: 6282  grad_norm: 10.3087  loss: 1.1309  loss_cls: 0.2594  loss_mask: 0.0803  loss_dice: 0.7912
2025/06/26 11:18:39 - mmengine - INFO - Epoch(train)  [5][200/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:22:24  time: 1.8313  data_time: 0.0024  memory: 5995  grad_norm: 16.7893  loss: 1.1888  loss_cls: 0.2651  loss_mask: 0.0770  loss_dice: 0.8467
2025/06/26 11:18:57 - mmengine - INFO - Epoch(train)  [5][210/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:22:03  time: 1.7661  data_time: 0.0012  memory: 5848  grad_norm: 105.0943  loss: 1.2199  loss_cls: 0.2790  loss_mask: 0.0870  loss_dice: 0.8539
2025/06/26 11:19:15 - mmengine - INFO - Epoch(train)  [5][220/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:21:42  time: 1.7983  data_time: 0.0025  memory: 6172  grad_norm: 58.8047  loss: 1.0784  loss_cls: 0.2583  loss_mask: 0.0760  loss_dice: 0.7440
2025/06/26 11:19:35 - mmengine - INFO - Epoch(train)  [5][230/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:21:27  time: 2.0527  data_time: 0.0035  memory: 6003  grad_norm: 20.6997  loss: 1.2860  loss_cls: 0.2929  loss_mask: 0.0889  loss_dice: 0.9041
2025/06/26 11:19:55 - mmengine - INFO - Epoch(train)  [5][240/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:21:09  time: 1.9439  data_time: 0.0046  memory: 6238  grad_norm: 76.2561  loss: 1.2213  loss_cls: 0.2748  loss_mask: 0.0813  loss_dice: 0.8652
2025/06/26 11:20:12 - mmengine - INFO - Epoch(train)  [5][250/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:20:49  time: 1.7941  data_time: 0.0028  memory: 6230  grad_norm: 22.7278  loss: 1.3583  loss_cls: 0.3243  loss_mask: 0.0929  loss_dice: 0.9410
2025/06/26 11:20:30 - mmengine - INFO - Epoch(train)  [5][260/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:20:27  time: 1.7631  data_time: 0.0020  memory: 6399  grad_norm: 134.6574  loss: 1.1300  loss_cls: 0.2258  loss_mask: 0.1127  loss_dice: 0.7915
2025/06/26 11:20:48 - mmengine - INFO - Epoch(train)  [5][270/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:20:07  time: 1.7982  data_time: 0.0022  memory: 6348  grad_norm: 18.0057  loss: 1.2210  loss_cls: 0.2826  loss_mask: 0.0774  loss_dice: 0.8610
2025/06/26 11:21:06 - mmengine - INFO - Epoch(train)  [5][280/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:19:47  time: 1.8193  data_time: 0.0036  memory: 5996  grad_norm: 19.0274  loss: 1.1642  loss_cls: 0.2256  loss_mask: 0.1046  loss_dice: 0.8341
2025/06/26 11:21:24 - mmengine - INFO - Epoch(train)  [5][290/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:19:26  time: 1.7679  data_time: 0.0037  memory: 6326  grad_norm: 14.8373  loss: 1.1625  loss_cls: 0.2735  loss_mask: 0.0757  loss_dice: 0.8133
2025/06/26 11:21:42 - mmengine - INFO - Epoch(train)  [5][300/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:19:05  time: 1.7800  data_time: 0.0026  memory: 6054  grad_norm: 22.8027  loss: 1.3318  loss_cls: 0.2512  loss_mask: 0.1319  loss_dice: 0.9487
2025/06/26 11:21:59 - mmengine - INFO - Epoch(train)  [5][310/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:18:43  time: 1.7340  data_time: 0.0047  memory: 5937  grad_norm: 12.2625  loss: 1.1911  loss_cls: 0.2504  loss_mask: 0.0771  loss_dice: 0.8637
2025/06/26 11:22:17 - mmengine - INFO - Epoch(train)  [5][320/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:18:23  time: 1.8150  data_time: 0.0004  memory: 6312  grad_norm: 15.6014  loss: 1.1531  loss_cls: 0.2796  loss_mask: 0.0831  loss_dice: 0.7904
2025/06/26 11:22:36 - mmengine - INFO - Epoch(train)  [5][330/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:18:04  time: 1.8567  data_time: 0.0038  memory: 6532  grad_norm: 16.0816  loss: 1.2214  loss_cls: 0.2649  loss_mask: 0.0789  loss_dice: 0.8776
2025/06/26 11:22:53 - mmengine - INFO - Epoch(train)  [5][340/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:17:43  time: 1.7489  data_time: 0.0045  memory: 5703  grad_norm: 23.4080  loss: 1.0882  loss_cls: 0.2096  loss_mask: 0.0968  loss_dice: 0.7818
2025/06/26 11:23:13 - mmengine - INFO - Epoch(train)  [5][350/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:17:27  time: 2.0194  data_time: 0.0021  memory: 6105  grad_norm: 232.2168  loss: 1.2042  loss_cls: 0.2409  loss_mask: 0.1064  loss_dice: 0.8569
2025/06/26 11:23:32 - mmengine - INFO - Epoch(train)  [5][360/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:17:08  time: 1.8922  data_time: 0.0027  memory: 6026  grad_norm: 25.2414  loss: 1.1875  loss_cls: 0.2458  loss_mask: 0.1039  loss_dice: 0.8377
2025/06/26 11:23:50 - mmengine - INFO - Epoch(train)  [5][370/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:16:48  time: 1.8101  data_time: 0.0045  memory: 5804  grad_norm: 62.0741  loss: 1.2129  loss_cls: 0.2745  loss_mask: 0.0813  loss_dice: 0.8572
2025/06/26 11:24:09 - mmengine - INFO - Epoch(train)  [5][380/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:16:29  time: 1.8515  data_time: 0.0022  memory: 6165  grad_norm: 16.8456  loss: 1.3471  loss_cls: 0.3606  loss_mask: 0.0770  loss_dice: 0.9095
2025/06/26 11:24:27 - mmengine - INFO - Epoch(train)  [5][390/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:16:09  time: 1.8017  data_time: 0.0018  memory: 5878  grad_norm: 74.8310  loss: 1.1863  loss_cls: 0.2489  loss_mask: 0.0763  loss_dice: 0.8611
2025/06/26 11:24:45 - mmengine - INFO - Epoch(train)  [5][400/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:15:48  time: 1.7708  data_time: 0.0026  memory: 5650  grad_norm: 128.1226  loss: 1.1425  loss_cls: 0.2348  loss_mask: 0.0950  loss_dice: 0.8127
2025/06/26 11:25:03 - mmengine - INFO - Epoch(train)  [5][410/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:15:28  time: 1.8160  data_time: 0.0035  memory: 5945  grad_norm: 37.8434  loss: 1.1751  loss_cls: 0.2689  loss_mask: 0.0807  loss_dice: 0.8255
2025/06/26 11:25:21 - mmengine - INFO - Epoch(train)  [5][420/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:15:09  time: 1.8368  data_time: 0.0013  memory: 6627  grad_norm: 17.3894  loss: 1.2300  loss_cls: 0.2608  loss_mask: 0.0873  loss_dice: 0.8818
2025/06/26 11:25:39 - mmengine - INFO - Epoch(train)  [5][430/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:14:48  time: 1.7613  data_time: 0.0026  memory: 6164  grad_norm: 18.1635  loss: 1.1142  loss_cls: 0.2369  loss_mask: 0.0758  loss_dice: 0.8016
2025/06/26 11:25:57 - mmengine - INFO - Epoch(train)  [5][440/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:14:28  time: 1.7862  data_time: 0.0049  memory: 5863  grad_norm: 14.9595  loss: 1.1338  loss_cls: 0.2302  loss_mask: 0.0868  loss_dice: 0.8167
2025/06/26 11:26:16 - mmengine - INFO - Epoch(train)  [5][450/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:14:10  time: 1.9025  data_time: 0.0024  memory: 5929  grad_norm: 17.8503  loss: 1.3139  loss_cls: 0.2711  loss_mask: 0.1367  loss_dice: 0.9060
2025/06/26 11:26:35 - mmengine - INFO - Epoch(train)  [5][460/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:13:52  time: 1.9058  data_time: 0.0022  memory: 6260  grad_norm: 47.7254  loss: 1.1070  loss_cls: 0.2414  loss_mask: 0.0787  loss_dice: 0.7869
2025/06/26 11:26:53 - mmengine - INFO - Epoch(train)  [5][470/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:13:32  time: 1.8493  data_time: 0.0036  memory: 5974  grad_norm: 12.8788  loss: 1.0917  loss_cls: 0.2239  loss_mask: 0.0844  loss_dice: 0.7834
2025/06/26 11:26:55 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250626_103923
2025/06/26 11:26:56 - mmengine - INFO - Saving checkpoint at 5 epochs
2025/06/26 11:27:25 - mmengine - INFO - Epoch(val)  [5][10/53]    eta: 0:01:24  time: 1.9577  data_time: 0.9958  memory: 5296  
2025/06/26 11:27:36 - mmengine - INFO - Epoch(val)  [5][20/53]    eta: 0:00:50  time: 1.1087  data_time: 0.1215  memory: 2334  
2025/06/26 11:27:47 - mmengine - INFO - Epoch(val)  [5][30/53]    eta: 0:00:31  time: 1.0545  data_time: 0.0726  memory: 2334  
2025/06/26 11:27:58 - mmengine - INFO - Epoch(val)  [5][40/53]    eta: 0:00:16  time: 1.0626  data_time: 0.1523  memory: 2334  
2025/06/26 11:28:08 - mmengine - INFO - Epoch(val)  [5][50/53]    eta: 0:00:03  time: 1.0189  data_time: 0.0890  memory: 2334  
2025/06/26 11:28:11 - mmengine - INFO - Evaluating segm...
2025/06/26 11:28:16 - mmengine - INFO - segm_mAP_copypaste: 0.521 0.773 0.575 0.303 0.672 0.815
2025/06/26 11:28:16 - mmengine - INFO - Epoch(val) [5][53/53]    coco/segm_mAP: 0.5210  coco/segm_mAP_50: 0.7730  coco/segm_mAP_75: 0.5750  coco/segm_mAP_s: 0.3030  coco/segm_mAP_m: 0.6720  coco/segm_mAP_l: 0.8150  data_time: 0.2651  time: 1.1971
2025/06/26 11:28:44 - mmengine - INFO - Epoch(train)  [6][ 10/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:13:27  time: 2.7809  data_time: 0.7940  memory: 6466  grad_norm: 41.8940  loss: 1.2328  loss_cls: 0.2913  loss_mask: 0.0751  loss_dice: 0.8664
2025/06/26 11:29:06 - mmengine - INFO - Epoch(train)  [6][ 20/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:13:12  time: 2.1544  data_time: 0.0040  memory: 6253  grad_norm: 68.9339  loss: 1.2601  loss_cls: 0.2535  loss_mask: 0.0801  loss_dice: 0.9264
2025/06/26 11:29:26 - mmengine - INFO - Epoch(train)  [6][ 30/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:12:55  time: 1.9915  data_time: 0.0016  memory: 6635  grad_norm: 38.8304  loss: 1.1286  loss_cls: 0.2213  loss_mask: 0.0728  loss_dice: 0.8345
2025/06/26 11:29:43 - mmengine - INFO - Epoch(train)  [6][ 40/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:12:35  time: 1.7665  data_time: 0.0036  memory: 5878  grad_norm: 57.1849  loss: 1.1666  loss_cls: 0.2407  loss_mask: 0.0808  loss_dice: 0.8451
2025/06/26 11:30:02 - mmengine - INFO - Epoch(train)  [6][ 50/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:12:15  time: 1.8353  data_time: 0.0063  memory: 6385  grad_norm: 20.2142  loss: 1.1929  loss_cls: 0.2630  loss_mask: 0.0652  loss_dice: 0.8647
2025/06/26 11:30:19 - mmengine - INFO - Epoch(train)  [6][ 60/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:11:54  time: 1.7666  data_time: 0.0040  memory: 5981  grad_norm: 32.0221  loss: 1.1465  loss_cls: 0.2398  loss_mask: 0.0858  loss_dice: 0.8208
2025/06/26 11:30:38 - mmengine - INFO - Epoch(train)  [6][ 70/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:11:36  time: 1.8951  data_time: 0.0031  memory: 6069  grad_norm: 19.6427  loss: 1.2226  loss_cls: 0.2649  loss_mask: 0.0885  loss_dice: 0.8692
2025/06/26 11:30:56 - mmengine - INFO - Epoch(train)  [6][ 80/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:11:15  time: 1.7556  data_time: 0.0056  memory: 6121  grad_norm: 31.9916  loss: 1.2560  loss_cls: 0.2517  loss_mask: 0.0878  loss_dice: 0.9165
2025/06/26 11:31:13 - mmengine - INFO - Epoch(train)  [6][ 90/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:10:54  time: 1.7449  data_time: 0.0032  memory: 6158  grad_norm: 28.6299  loss: 1.1045  loss_cls: 0.2313  loss_mask: 0.0790  loss_dice: 0.7942
2025/06/26 11:31:31 - mmengine - INFO - Epoch(train)  [6][100/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:10:34  time: 1.7609  data_time: 0.0028  memory: 6130  grad_norm: 645.0411  loss: 1.2519  loss_cls: 0.2817  loss_mask: 0.1534  loss_dice: 0.8168
2025/06/26 11:31:49 - mmengine - INFO - Epoch(train)  [6][110/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:10:14  time: 1.7732  data_time: 0.0026  memory: 6289  grad_norm: 18.2903  loss: 1.1023  loss_cls: 0.2379  loss_mask: 0.0793  loss_dice: 0.7852
2025/06/26 11:32:07 - mmengine - INFO - Epoch(train)  [6][120/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:09:54  time: 1.8543  data_time: 0.0019  memory: 5819  grad_norm: 16.5797  loss: 1.1379  loss_cls: 0.2574  loss_mask: 0.0712  loss_dice: 0.8093
2025/06/26 11:32:25 - mmengine - INFO - Epoch(train)  [6][130/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:09:34  time: 1.7671  data_time: 0.0027  memory: 5701  grad_norm: 8.2493  loss: 1.1669  loss_cls: 0.2211  loss_mask: 0.0834  loss_dice: 0.8624
2025/06/26 11:32:44 - mmengine - INFO - Epoch(train)  [6][140/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:09:16  time: 1.9019  data_time: 0.0034  memory: 5790  grad_norm: 29.7972  loss: 1.3517  loss_cls: 0.2895  loss_mask: 0.1313  loss_dice: 0.9310
2025/06/26 11:33:04 - mmengine - INFO - Epoch(train)  [6][150/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:08:58  time: 1.9615  data_time: 0.0024  memory: 6047  grad_norm: 11.3079  loss: 1.1035  loss_cls: 0.2405  loss_mask: 0.0647  loss_dice: 0.7983
2025/06/26 11:33:23 - mmengine - INFO - Epoch(train)  [6][160/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:08:40  time: 1.9225  data_time: 0.0030  memory: 6473  grad_norm: 8.0817  loss: 1.1451  loss_cls: 0.2324  loss_mask: 0.0894  loss_dice: 0.8233
2025/06/26 11:33:41 - mmengine - INFO - Epoch(train)  [6][170/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:08:20  time: 1.7937  data_time: 0.0012  memory: 5870  grad_norm: 42.2429  loss: 1.0248  loss_cls: 0.2340  loss_mask: 0.0748  loss_dice: 0.7160
2025/06/26 11:33:59 - mmengine - INFO - Epoch(train)  [6][180/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:08:01  time: 1.8233  data_time: 0.0036  memory: 5761  grad_norm: 26.6397  loss: 1.0664  loss_cls: 0.2080  loss_mask: 0.0763  loss_dice: 0.7821
2025/06/26 11:34:17 - mmengine - INFO - Epoch(train)  [6][190/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:07:41  time: 1.8468  data_time: 0.0020  memory: 6341  grad_norm: 16.1789  loss: 1.2105  loss_cls: 0.2028  loss_mask: 0.0743  loss_dice: 0.9334
2025/06/26 11:34:35 - mmengine - INFO - Epoch(train)  [6][200/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:07:22  time: 1.7958  data_time: 0.0047  memory: 5914  grad_norm: 12.7017  loss: 1.0617  loss_cls: 0.2134  loss_mask: 0.0998  loss_dice: 0.7485
2025/06/26 11:34:54 - mmengine - INFO - Epoch(train)  [6][210/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:07:03  time: 1.8598  data_time: 0.0043  memory: 6025  grad_norm: 23.7401  loss: 1.2524  loss_cls: 0.2747  loss_mask: 0.0736  loss_dice: 0.9041
2025/06/26 11:35:13 - mmengine - INFO - Epoch(train)  [6][220/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:06:44  time: 1.8692  data_time: 0.0044  memory: 6165  grad_norm: 30.4923  loss: 1.2071  loss_cls: 0.3180  loss_mask: 0.0760  loss_dice: 0.8131
2025/06/26 11:35:31 - mmengine - INFO - Epoch(train)  [6][230/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:06:25  time: 1.8448  data_time: 0.0009  memory: 6054  grad_norm: 37.0046  loss: 1.2580  loss_cls: 0.2957  loss_mask: 0.0759  loss_dice: 0.8865
2025/06/26 11:35:51 - mmengine - INFO - Epoch(train)  [6][240/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:06:07  time: 1.9742  data_time: 0.0058  memory: 6385  grad_norm: 60.7299  loss: 1.1849  loss_cls: 0.2480  loss_mask: 0.0776  loss_dice: 0.8593
2025/06/26 11:36:09 - mmengine - INFO - Epoch(train)  [6][250/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:05:47  time: 1.7763  data_time: 0.0032  memory: 5838  grad_norm: 52.3582  loss: 1.1063  loss_cls: 0.2286  loss_mask: 0.0796  loss_dice: 0.7980
2025/06/26 11:36:26 - mmengine - INFO - Epoch(train)  [6][260/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:05:27  time: 1.7767  data_time: 0.0036  memory: 5826  grad_norm: 24.3287  loss: 1.2683  loss_cls: 0.3007  loss_mask: 0.0843  loss_dice: 0.8833
2025/06/26 11:36:45 - mmengine - INFO - Epoch(train)  [6][270/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:05:08  time: 1.8193  data_time: 0.0035  memory: 6165  grad_norm: 38.4869  loss: 1.2485  loss_cls: 0.2810  loss_mask: 0.0792  loss_dice: 0.8883
2025/06/26 11:37:02 - mmengine - INFO - Epoch(train)  [6][280/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:04:48  time: 1.7615  data_time: 0.0025  memory: 6201  grad_norm: 36.6534  loss: 1.2042  loss_cls: 0.2561  loss_mask: 0.1050  loss_dice: 0.8432
2025/06/26 11:37:20 - mmengine - INFO - Epoch(train)  [6][290/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:04:27  time: 1.7584  data_time: 0.0036  memory: 6114  grad_norm: 21.7868  loss: 1.0876  loss_cls: 0.2478  loss_mask: 0.0660  loss_dice: 0.7737
2025/06/26 11:37:38 - mmengine - INFO - Epoch(train)  [6][300/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:04:08  time: 1.7793  data_time: 0.0023  memory: 5980  grad_norm: 28.7202  loss: 1.2415  loss_cls: 0.2803  loss_mask: 0.0810  loss_dice: 0.8802
2025/06/26 11:37:56 - mmengine - INFO - Epoch(train)  [6][310/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:03:48  time: 1.8309  data_time: 0.0017  memory: 6261  grad_norm: 18.5109  loss: 1.1965  loss_cls: 0.2389  loss_mask: 0.0814  loss_dice: 0.8762
2025/06/26 11:38:14 - mmengine - INFO - Epoch(train)  [6][320/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:03:28  time: 1.7654  data_time: 0.0041  memory: 5797  grad_norm: 23.1160  loss: 1.0873  loss_cls: 0.2629  loss_mask: 0.0868  loss_dice: 0.7376
2025/06/26 11:38:32 - mmengine - INFO - Epoch(train)  [6][330/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:03:09  time: 1.8358  data_time: 0.0043  memory: 6524  grad_norm: 16.4158  loss: 1.1095  loss_cls: 0.2510  loss_mask: 0.0771  loss_dice: 0.7814
2025/06/26 11:38:49 - mmengine - INFO - Epoch(train)  [6][340/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:02:49  time: 1.7344  data_time: 0.0032  memory: 5871  grad_norm: 8.2438  loss: 0.9910  loss_cls: 0.2111  loss_mask: 0.0742  loss_dice: 0.7057
2025/06/26 11:39:07 - mmengine - INFO - Epoch(train)  [6][350/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:02:29  time: 1.7465  data_time: 0.0028  memory: 5856  grad_norm: 13.3363  loss: 1.0876  loss_cls: 0.2321  loss_mask: 0.0826  loss_dice: 0.7729
2025/06/26 11:39:24 - mmengine - INFO - Epoch(train)  [6][360/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:02:08  time: 1.7238  data_time: 0.0020  memory: 5782  grad_norm: 53.1521  loss: 1.1502  loss_cls: 0.2336  loss_mask: 0.0805  loss_dice: 0.8361
2025/06/26 11:39:43 - mmengine - INFO - Epoch(train)  [6][370/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:01:50  time: 1.9114  data_time: 0.0058  memory: 5789  grad_norm: 35.4798  loss: 1.1218  loss_cls: 0.2490  loss_mask: 0.1296  loss_dice: 0.7431
2025/06/26 11:40:01 - mmengine - INFO - Epoch(train)  [6][380/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:01:31  time: 1.8235  data_time: 0.0031  memory: 5753  grad_norm: 60.0720  loss: 1.1045  loss_cls: 0.2123  loss_mask: 0.0772  loss_dice: 0.8150
2025/06/26 11:40:19 - mmengine - INFO - Epoch(train)  [6][390/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:01:12  time: 1.8110  data_time: 0.0025  memory: 6194  grad_norm: 15.8438  loss: 1.1998  loss_cls: 0.2349  loss_mask: 0.1079  loss_dice: 0.8570
2025/06/26 11:40:37 - mmengine - INFO - Epoch(train)  [6][400/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:00:52  time: 1.7972  data_time: 0.0014  memory: 6054  grad_norm: 20.4087  loss: 1.2351  loss_cls: 0.2756  loss_mask: 0.0912  loss_dice: 0.8683
2025/06/26 11:40:55 - mmengine - INFO - Epoch(train)  [6][410/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:00:33  time: 1.7739  data_time: 0.0037  memory: 5588  grad_norm: 17.7251  loss: 1.0412  loss_cls: 0.2159  loss_mask: 0.1180  loss_dice: 0.7073
2025/06/26 11:41:14 - mmengine - INFO - Epoch(train)  [6][420/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 1:00:14  time: 1.9127  data_time: 0.0024  memory: 5981  grad_norm: 19.3448  loss: 1.1890  loss_cls: 0.2548  loss_mask: 0.0886  loss_dice: 0.8456
2025/06/26 11:41:33 - mmengine - INFO - Epoch(train)  [6][430/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:59:56  time: 1.9046  data_time: 0.0038  memory: 5745  grad_norm: 14.5555  loss: 1.2521  loss_cls: 0.2776  loss_mask: 0.0909  loss_dice: 0.8836
2025/06/26 11:41:51 - mmengine - INFO - Epoch(train)  [6][440/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:59:36  time: 1.7776  data_time: 0.0025  memory: 5958  grad_norm: 12.2249  loss: 0.9516  loss_cls: 0.2065  loss_mask: 0.0638  loss_dice: 0.6813
2025/06/26 11:42:10 - mmengine - INFO - Epoch(train)  [6][450/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:59:18  time: 1.8478  data_time: 0.0047  memory: 5727  grad_norm: 17.1901  loss: 0.9878  loss_cls: 0.1959  loss_mask: 0.0621  loss_dice: 0.7298
2025/06/26 11:42:27 - mmengine - INFO - Epoch(train)  [6][460/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:58:58  time: 1.7357  data_time: 0.0029  memory: 5789  grad_norm: 17.3168  loss: 1.1432  loss_cls: 0.2210  loss_mask: 0.0898  loss_dice: 0.8324
2025/06/26 11:42:45 - mmengine - INFO - Epoch(train)  [6][470/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:58:38  time: 1.8151  data_time: 0.0018  memory: 5901  grad_norm: 34.1786  loss: 1.2242  loss_cls: 0.2955  loss_mask: 0.0840  loss_dice: 0.8447
2025/06/26 11:42:47 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250626_103923
2025/06/26 11:42:47 - mmengine - INFO - Saving checkpoint at 6 epochs
2025/06/26 11:43:17 - mmengine - INFO - Epoch(val)  [6][10/53]    eta: 0:01:25  time: 1.9870  data_time: 0.9922  memory: 5560  
2025/06/26 11:43:28 - mmengine - INFO - Epoch(val)  [6][20/53]    eta: 0:00:50  time: 1.0641  data_time: 0.0974  memory: 2334  
2025/06/26 11:43:38 - mmengine - INFO - Epoch(val)  [6][30/53]    eta: 0:00:31  time: 1.0106  data_time: 0.1070  memory: 2334  
2025/06/26 11:43:48 - mmengine - INFO - Epoch(val)  [6][40/53]    eta: 0:00:16  time: 1.0367  data_time: 0.0835  memory: 2334  
2025/06/26 11:43:59 - mmengine - INFO - Epoch(val)  [6][50/53]    eta: 0:00:03  time: 1.1117  data_time: 0.1862  memory: 2334  
2025/06/26 11:44:03 - mmengine - INFO - Evaluating segm...
2025/06/26 11:44:08 - mmengine - INFO - segm_mAP_copypaste: 0.525 0.772 0.581 0.304 0.677 0.821
2025/06/26 11:44:08 - mmengine - INFO - Epoch(val) [6][53/53]    coco/segm_mAP: 0.5250  coco/segm_mAP_50: 0.7720  coco/segm_mAP_75: 0.5810  coco/segm_mAP_s: 0.3040  coco/segm_mAP_m: 0.6770  coco/segm_mAP_l: 0.8210  data_time: 0.2721  time: 1.1998
2025/06/26 11:44:36 - mmengine - INFO - Epoch(train)  [7][ 10/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:58:27  time: 2.8053  data_time: 0.8187  memory: 6061  grad_norm: 8.1602  loss: 1.1320  loss_cls: 0.2467  loss_mask: 0.0733  loss_dice: 0.8121
2025/06/26 11:44:56 - mmengine - INFO - Epoch(train)  [7][ 20/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:58:09  time: 1.9246  data_time: 0.0022  memory: 6115  grad_norm: 16.6058  loss: 1.1720  loss_cls: 0.2372  loss_mask: 0.0969  loss_dice: 0.8379
2025/06/26 11:45:14 - mmengine - INFO - Epoch(train)  [7][ 30/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:57:50  time: 1.8481  data_time: 0.0041  memory: 6414  grad_norm: 27.8246  loss: 1.2819  loss_cls: 0.2543  loss_mask: 0.0691  loss_dice: 0.9584
2025/06/26 11:45:32 - mmengine - INFO - Epoch(train)  [7][ 40/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:57:31  time: 1.8411  data_time: 0.0019  memory: 5782  grad_norm: 14.7314  loss: 1.2272  loss_cls: 0.2637  loss_mask: 0.0751  loss_dice: 0.8884
2025/06/26 11:45:53 - mmengine - INFO - Epoch(train)  [7][ 50/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:57:14  time: 2.0226  data_time: 0.0028  memory: 6144  grad_norm: 19.0747  loss: 1.1271  loss_cls: 0.2567  loss_mask: 0.0839  loss_dice: 0.7865
2025/06/26 11:46:14 - mmengine - INFO - Epoch(train)  [7][ 60/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:56:57  time: 2.0857  data_time: 0.0027  memory: 6282  grad_norm: 19.9370  loss: 1.2894  loss_cls: 0.2456  loss_mask: 0.1129  loss_dice: 0.9309
2025/06/26 11:46:34 - mmengine - INFO - Epoch(train)  [7][ 70/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:56:40  time: 2.0641  data_time: 0.0025  memory: 5907  grad_norm: 67.1280  loss: 1.2972  loss_cls: 0.2768  loss_mask: 0.0950  loss_dice: 0.9254
2025/06/26 11:46:58 - mmengine - INFO - Epoch(train)  [7][ 80/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:56:25  time: 2.3387  data_time: 0.0054  memory: 6326  grad_norm: 21.6451  loss: 1.2102  loss_cls: 0.3022  loss_mask: 0.0851  loss_dice: 0.8228
2025/06/26 11:47:17 - mmengine - INFO - Epoch(train)  [7][ 90/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:56:07  time: 1.9146  data_time: 0.0026  memory: 6392  grad_norm: 34.7012  loss: 1.2816  loss_cls: 0.3006  loss_mask: 0.0992  loss_dice: 0.8818
2025/06/26 11:47:35 - mmengine - INFO - Epoch(train)  [7][100/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:55:48  time: 1.8634  data_time: 0.0021  memory: 5682  grad_norm: 32.5350  loss: 1.1409  loss_cls: 0.2161  loss_mask: 0.0995  loss_dice: 0.8253
2025/06/26 11:47:54 - mmengine - INFO - Epoch(train)  [7][110/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:55:29  time: 1.8668  data_time: 0.0036  memory: 5789  grad_norm: 15.4451  loss: 1.1368  loss_cls: 0.2650  loss_mask: 0.0731  loss_dice: 0.7987
2025/06/26 11:48:13 - mmengine - INFO - Epoch(train)  [7][120/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:55:11  time: 1.9067  data_time: 0.0035  memory: 5944  grad_norm: 21.1181  loss: 1.1068  loss_cls: 0.2410  loss_mask: 0.0824  loss_dice: 0.7834
2025/06/26 11:48:31 - mmengine - INFO - Epoch(train)  [7][130/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:54:51  time: 1.7701  data_time: 0.0022  memory: 5580  grad_norm: 92.8081  loss: 1.1613  loss_cls: 0.2271  loss_mask: 0.0971  loss_dice: 0.8371
2025/06/26 11:48:49 - mmengine - INFO - Epoch(train)  [7][140/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:54:32  time: 1.7838  data_time: 0.0031  memory: 5944  grad_norm: 24.4821  loss: 1.1966  loss_cls: 0.2511  loss_mask: 0.0899  loss_dice: 0.8556
2025/06/26 11:49:06 - mmengine - INFO - Epoch(train)  [7][150/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:54:12  time: 1.7767  data_time: 0.0021  memory: 6017  grad_norm: 37.1174  loss: 1.1780  loss_cls: 0.2698  loss_mask: 0.0826  loss_dice: 0.8256
2025/06/26 11:49:24 - mmengine - INFO - Epoch(train)  [7][160/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:53:52  time: 1.7891  data_time: 0.0043  memory: 6371  grad_norm: 19.2239  loss: 1.0012  loss_cls: 0.2251  loss_mask: 0.0624  loss_dice: 0.7137
2025/06/26 11:49:42 - mmengine - INFO - Epoch(train)  [7][170/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:53:33  time: 1.7717  data_time: 0.0015  memory: 6223  grad_norm: 9.5533  loss: 1.2107  loss_cls: 0.2778  loss_mask: 0.0719  loss_dice: 0.8610
2025/06/26 11:49:50 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250626_103923
2025/06/26 11:50:01 - mmengine - INFO - Epoch(train)  [7][180/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:53:14  time: 1.8618  data_time: 0.0056  memory: 6165  grad_norm: 22.3453  loss: 1.0380  loss_cls: 0.1875  loss_mask: 0.0959  loss_dice: 0.7547
2025/06/26 11:50:20 - mmengine - INFO - Epoch(train)  [7][190/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:52:56  time: 1.9323  data_time: 0.0042  memory: 6151  grad_norm: 10.3797  loss: 1.0882  loss_cls: 0.2324  loss_mask: 0.0635  loss_dice: 0.7923
2025/06/26 11:50:41 - mmengine - INFO - Epoch(train)  [7][200/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:52:39  time: 2.0852  data_time: 0.0074  memory: 5944  grad_norm: 64.4717  loss: 1.1120  loss_cls: 0.2921  loss_mask: 0.0668  loss_dice: 0.7530
2025/06/26 11:51:01 - mmengine - INFO - Epoch(train)  [7][210/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:52:21  time: 1.9872  data_time: 0.0035  memory: 5981  grad_norm: 61.1109  loss: 1.2851  loss_cls: 0.3074  loss_mask: 0.0825  loss_dice: 0.8952
2025/06/26 11:51:21 - mmengine - INFO - Epoch(train)  [7][220/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:52:03  time: 1.9988  data_time: 0.0044  memory: 5988  grad_norm: 30.4800  loss: 1.2846  loss_cls: 0.2693  loss_mask: 0.0764  loss_dice: 0.9389
2025/06/26 11:51:41 - mmengine - INFO - Epoch(train)  [7][230/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:51:46  time: 2.0756  data_time: 0.0063  memory: 6355  grad_norm: 32.9710  loss: 1.0962  loss_cls: 0.2437  loss_mask: 0.0705  loss_dice: 0.7820
2025/06/26 11:52:02 - mmengine - INFO - Epoch(train)  [7][240/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:51:29  time: 2.0894  data_time: 0.0040  memory: 5762  grad_norm: 23.5265  loss: 1.1450  loss_cls: 0.2422  loss_mask: 0.0805  loss_dice: 0.8223
2025/06/26 11:52:24 - mmengine - INFO - Epoch(train)  [7][250/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:51:12  time: 2.1171  data_time: 0.0051  memory: 6083  grad_norm: 17.8932  loss: 1.0629  loss_cls: 0.2193  loss_mask: 0.0813  loss_dice: 0.7623
2025/06/26 11:52:46 - mmengine - INFO - Epoch(train)  [7][260/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:50:56  time: 2.2845  data_time: 0.0050  memory: 5980  grad_norm: 16.9854  loss: 1.2184  loss_cls: 0.2577  loss_mask: 0.1232  loss_dice: 0.8375
2025/06/26 11:53:05 - mmengine - INFO - Epoch(train)  [7][270/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:50:37  time: 1.8364  data_time: 0.0029  memory: 5966  grad_norm: 15.6292  loss: 1.2597  loss_cls: 0.2650  loss_mask: 0.0877  loss_dice: 0.9070
2025/06/26 11:53:23 - mmengine - INFO - Epoch(train)  [7][280/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:50:17  time: 1.7963  data_time: 0.0067  memory: 6017  grad_norm: 31.9685  loss: 1.1776  loss_cls: 0.2619  loss_mask: 0.0733  loss_dice: 0.8425
2025/06/26 11:53:41 - mmengine - INFO - Epoch(train)  [7][290/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:49:58  time: 1.7953  data_time: 0.0048  memory: 6047  grad_norm: 35.4030  loss: 1.1798  loss_cls: 0.2842  loss_mask: 0.0698  loss_dice: 0.8259
2025/06/26 11:53:58 - mmengine - INFO - Epoch(train)  [7][300/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:49:38  time: 1.7703  data_time: 0.0023  memory: 5914  grad_norm: 21.8819  loss: 1.1608  loss_cls: 0.2555  loss_mask: 0.0889  loss_dice: 0.8164
2025/06/26 11:54:16 - mmengine - INFO - Epoch(train)  [7][310/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:49:19  time: 1.7840  data_time: 0.0025  memory: 6532  grad_norm: 10.7226  loss: 1.0906  loss_cls: 0.2465  loss_mask: 0.0863  loss_dice: 0.7577
2025/06/26 11:54:35 - mmengine - INFO - Epoch(train)  [7][320/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:49:00  time: 1.8966  data_time: 0.0032  memory: 5761  grad_norm: 19.4056  loss: 1.0618  loss_cls: 0.2613  loss_mask: 0.0660  loss_dice: 0.7345
2025/06/26 11:54:53 - mmengine - INFO - Epoch(train)  [7][330/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:48:41  time: 1.8169  data_time: 0.0024  memory: 5745  grad_norm: 20.8093  loss: 1.1791  loss_cls: 0.2172  loss_mask: 0.0828  loss_dice: 0.8790
2025/06/26 11:55:12 - mmengine - INFO - Epoch(train)  [7][340/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:48:22  time: 1.9047  data_time: 0.0043  memory: 6826  grad_norm: 30.2414  loss: 0.9801  loss_cls: 0.2389  loss_mask: 0.0748  loss_dice: 0.6665
2025/06/26 11:55:31 - mmengine - INFO - Epoch(train)  [7][350/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:48:03  time: 1.8695  data_time: 0.0034  memory: 5878  grad_norm: 8.6033  loss: 1.1507  loss_cls: 0.2152  loss_mask: 0.0609  loss_dice: 0.8747
2025/06/26 11:55:49 - mmengine - INFO - Epoch(train)  [7][360/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:47:44  time: 1.7877  data_time: 0.0024  memory: 5841  grad_norm: 54.6648  loss: 1.1293  loss_cls: 0.1993  loss_mask: 0.0868  loss_dice: 0.8432
2025/06/26 11:56:09 - mmengine - INFO - Epoch(train)  [7][370/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:47:26  time: 2.0086  data_time: 0.0015  memory: 5908  grad_norm: 11.4657  loss: 1.1849  loss_cls: 0.2461  loss_mask: 0.0966  loss_dice: 0.8422
2025/06/26 11:56:27 - mmengine - INFO - Epoch(train)  [7][380/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:47:07  time: 1.8135  data_time: 0.0014  memory: 5988  grad_norm: 31.3354  loss: 1.4152  loss_cls: 0.2698  loss_mask: 0.0957  loss_dice: 1.0496
2025/06/26 11:56:45 - mmengine - INFO - Epoch(train)  [7][390/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:46:47  time: 1.7829  data_time: 0.0031  memory: 6143  grad_norm: 14.3096  loss: 1.0442  loss_cls: 0.2185  loss_mask: 0.0787  loss_dice: 0.7470
2025/06/26 11:57:03 - mmengine - INFO - Epoch(train)  [7][400/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:46:28  time: 1.8013  data_time: 0.0026  memory: 6123  grad_norm: 18.8285  loss: 1.1323  loss_cls: 0.2452  loss_mask: 0.0740  loss_dice: 0.8130
2025/06/26 11:57:21 - mmengine - INFO - Epoch(train)  [7][410/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:46:09  time: 1.7853  data_time: 0.0044  memory: 6011  grad_norm: 32.9654  loss: 1.3039  loss_cls: 0.3008  loss_mask: 0.0843  loss_dice: 0.9188
2025/06/26 11:57:39 - mmengine - INFO - Epoch(train)  [7][420/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:45:49  time: 1.8290  data_time: 0.0030  memory: 6083  grad_norm: 62.4026  loss: 1.1079  loss_cls: 0.2323  loss_mask: 0.0940  loss_dice: 0.7816
2025/06/26 11:57:57 - mmengine - INFO - Epoch(train)  [7][430/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:45:30  time: 1.8069  data_time: 0.0045  memory: 5914  grad_norm: 67.0759  loss: 0.9645  loss_cls: 0.1959  loss_mask: 0.0675  loss_dice: 0.7011
2025/06/26 11:58:15 - mmengine - INFO - Epoch(train)  [7][440/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:45:11  time: 1.7793  data_time: 0.0029  memory: 6173  grad_norm: 9.2320  loss: 1.0303  loss_cls: 0.2335  loss_mask: 0.0672  loss_dice: 0.7296
2025/06/26 11:58:33 - mmengine - INFO - Epoch(train)  [7][450/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:44:52  time: 1.8296  data_time: 0.0058  memory: 6083  grad_norm: 134.2824  loss: 1.1308  loss_cls: 0.2791  loss_mask: 0.0926  loss_dice: 0.7591
2025/06/26 11:58:52 - mmengine - INFO - Epoch(train)  [7][460/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:44:33  time: 1.8203  data_time: 0.0038  memory: 6377  grad_norm: 14.3232  loss: 1.2669  loss_cls: 0.3237  loss_mask: 0.0924  loss_dice: 0.8507
2025/06/26 11:59:09 - mmengine - INFO - Epoch(train)  [7][470/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:44:13  time: 1.7813  data_time: 0.0032  memory: 5958  grad_norm: 13.8316  loss: 1.1305  loss_cls: 0.2513  loss_mask: 0.0672  loss_dice: 0.8120
2025/06/26 11:59:11 - mmengine - INFO - Exp name: mask2former_r50_kazgisa-kostanai_20250626_103923
2025/06/26 11:59:12 - mmengine - INFO - Saving checkpoint at 7 epochs
2025/06/26 11:59:39 - mmengine - INFO - Epoch(val)  [7][10/53]    eta: 0:01:21  time: 1.8977  data_time: 0.9162  memory: 5543  
2025/06/26 11:59:51 - mmengine - INFO - Epoch(val)  [7][20/53]    eta: 0:00:50  time: 1.1373  data_time: 0.1796  memory: 2334  
2025/06/26 12:00:01 - mmengine - INFO - Epoch(val)  [7][30/53]    eta: 0:00:31  time: 1.0225  data_time: 0.1069  memory: 2334  
2025/06/26 12:00:11 - mmengine - INFO - Epoch(val)  [7][40/53]    eta: 0:00:16  time: 1.0125  data_time: 0.0827  memory: 2334  
2025/06/26 12:00:23 - mmengine - INFO - Epoch(val)  [7][50/53]    eta: 0:00:03  time: 1.1425  data_time: 0.1222  memory: 2334  
2025/06/26 12:00:26 - mmengine - INFO - Evaluating segm...
2025/06/26 12:00:31 - mmengine - INFO - segm_mAP_copypaste: 0.523 0.775 0.580 0.302 0.675 0.808
2025/06/26 12:00:31 - mmengine - INFO - Epoch(val) [7][53/53]    coco/segm_mAP: 0.5230  coco/segm_mAP_50: 0.7750  coco/segm_mAP_75: 0.5800  coco/segm_mAP_s: 0.3020  coco/segm_mAP_m: 0.6750  coco/segm_mAP_l: 0.8080  data_time: 0.2608  time: 1.2008
2025/06/26 12:00:59 - mmengine - INFO - Epoch(train)  [8][ 10/471]  base_lr: 1.0000e-05 lr: 1.0000e-07  eta: 0:43:58  time: 2.7516  data_time: 0.8409  memory: 6092  grad_norm: 16.7033  loss: 1.3022  loss_cls: 0.3031  loss_mask: 0.0936  loss_dice: 0.9055
