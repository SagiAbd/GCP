2025/06/30 13:08:56 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: win32
    Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 649822913
    GPU 0: NVIDIA GeForce GTX 1660
    CUDA_HOME: C:\Users\Sagi\Miniconda3\envs\gcp-env\Library
    NVCC: Not Available
    MSVC: Оптимизирующий компилятор Microsoft (R) C/C++ версии 19.43.34810 для x64
    GCC: n/a
    PyTorch: 2.2.2+cu118
    PyTorch compiling details: PyTorch built with:
  - C++ Version: 201703
  - MSVC 192930151
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.2 (Git Hash 2dc95a2ad0841e29db8b22fbccaf3e5da7992b01)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.7
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=C:/actions-runner/_work/pytorch/pytorch/builder/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.2.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.17.2+cu118
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 649822913
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/06/30 13:08:56 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=4, enable=False)
backend_args = None
batch_augments = [
    dict(
        img_pad_value=0,
        mask_pad_value=0,
        pad_mask=True,
        pad_seg=True,
        seg_pad_value=255,
        size=(
            512,
            512,
        ),
        type='BatchFixedSizePad'),
]
crop_size = (
    512,
    512,
)
data_preprocessor = dict(
    batch_augments=[
        dict(
            img_pad_value=0,
            mask_pad_value=0,
            pad_mask=True,
            pad_seg=True,
            seg_pad_value=255,
            size=(
                512,
                512,
            ),
            type='BatchFixedSizePad'),
    ],
    bgr_to_rgb=True,
    mask_pad_value=0,
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    pad_mask=True,
    pad_seg=True,
    pad_size_divisor=32,
    seg_pad_value=255,
    std=[
        58.395,
        57.12,
        57.375,
    ],
    type='DetDataPreprocessor')
data_root = 'data/kostanai'
dataset_type = 'WHUMixVectorDataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=True,
        interval=1,
        max_keep_ckpts=3,
        save_last=True,
        type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(
        draw=True, interval=3, score_thr=0.6, type='TanmlhVisualizationHook'))
default_scope = 'mmdet'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
img_norm_cfg = dict(
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    std=[
        58.395,
        57.12,
        57.375,
    ],
    to_rgb=True)
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=10)
max_epochs = 8
model = dict(
    backbone=dict(
        depth=50,
        frozen_stages=-1,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=False, type='BN'),
        norm_eval=True,
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mask_pad_value=0,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=True,
        pad_seg=True,
        pad_size_divisor=32,
        seg_pad_value=255,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    frozen_parameters=[
        'backbone',
        'panoptic_head.pixel_decoder',
        'panoptic_head.transformer_decoder',
        'panoptic_head.decoder_input_projs',
        'panoptic_head.query_embed',
        'panoptic_head.query_feat',
        'panoptic_head.level_embed',
        'panoptic_head.cls_embed',
        'panoptic_head.mask_embed',
    ],
    init_cfg=None,
    panoptic_fusion_head=dict(
        init_cfg=None,
        loss_panoptic=None,
        num_stuff_classes=0,
        num_things_classes=1,
        type='PolyFormerFusionHeadV2'),
    panoptic_head=dict(
        dp_polygonize_head=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=3,
            return_intermediate=True),
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=True),
        loss_poly_ang=dict(
            loss_weight=1.0, reduction='mean', type='SmoothL1Loss'),
        loss_poly_reg=dict(
            loss_weight=1.0, reduction='mean', type='SmoothL1Loss'),
        num_queries=300,
        num_stuff_classes=0,
        num_things_classes=1,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='MSDeformAttnPixelDecoder'),
        poly_cfg=dict(
            align_iou_thre=0.5,
            apply_angle_loss=True,
            apply_prim_pred=True,
            lam=4,
            loss_weight_dp=0.01,
            map_features=True,
            mask_cls_thre=0.0,
            max_align_dis=15,
            max_match_dis=10,
            max_offsets=5,
            max_step_size=128,
            num_cls_channels=2,
            num_inter_points=64,
            num_min_bins=32,
            point_as_prim=True,
            poly_decode_type='dp',
            polygonize_mode='cv2_single_mask',
            polygonized_scale=4.0,
            pred_angle=False,
            prim_cls_thre=0.1,
            proj_gt=False,
            reg_targets_type='vertice',
            return_poly_json=False,
            sample_points=True,
            step_size=4,
            stride_size=64,
            use_coords_in_poly_feat=True,
            use_decoded_feat_in_poly_feat=True,
            use_gt_jsons=False,
            use_ind_offset=True,
            use_point_feat_in_poly_feat=True,
            use_ref_rings=False),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=9,
            return_intermediate=True),
        type='PolygonizerHead'),
    test_cfg=dict(
        filter_low_score=True,
        instance_on=True,
        iou_thr=0.8,
        max_per_image=300,
        panoptic_on=False,
        score_thr=0.6,
        semantic_on=False),
    train_cfg=dict(
        add_target_to_data_samples=True,
        assigner=dict(
            match_costs=[
                dict(type='ClassificationCost', weight=2.0),
                dict(
                    type='CrossEntropyLossCost', use_sigmoid=True, weight=5.0),
                dict(eps=1.0, pred_act=True, type='DiceCost', weight=5.0),
            ],
            type='HungarianAssigner'),
        importance_sample_ratio=0.75,
        num_points=12544,
        oversample_ratio=3.0,
        prim_assigner=dict(
            match_costs=[
                dict(type='PointL1Cost', weight=0.1),
            ],
            type='HungarianAssigner'),
        sampler=dict(type='MaskPseudoSampler')),
    type='PolyFormerV2')
num_classes = 1
num_stuff_classes = 0
num_things_classes = 1
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(decay_mult=1.0, lr_mult=0.1),
            level_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_feat=dict(decay_mult=0.0, lr_mult=1.0)),
        norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=8,
        gamma=0.1,
        milestones=[
            9,
        ],
        type='MultiStepLR'),
]
resume = True
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='test/test.json',
        backend_args=None,
        data_prefix=dict(img='test/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=1,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = [
    dict(
        ann_file='data/kostanai/test/test.json',
        backend_args=None,
        calculate_iou_ciou=True,
        calculate_mta=True,
        mask_type='polygon',
        metric=[
            'segm',
        ],
        score_thre=0.5,
        type='CocoMetric'),
]
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=False,
        with_mask=True,
        with_poly_json=False),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=8, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=2,
    dataset=dict(
        ann_file='train/train.json',
        backend_args=None,
        data_prefix=dict(img='train/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True,
                with_poly_json=False),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                direction=[
                    'horizontal',
                    'vertical',
                    'diagonal',
                ],
                prob=0.75,
                type='RandomFlip'),
            dict(prob=0.75, type='Rotate90'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        type='WHUMixVectorDataset'),
    num_workers=2,
    persistent_workers=False,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True,
        with_poly_json=False),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(
        direction=[
            'horizontal',
            'vertical',
            'diagonal',
        ],
        prob=0.75,
        type='RandomFlip'),
    dict(prob=0.75, type='Rotate90'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=2,
    dataset=dict(
        ann_file='val/val.json',
        backend_args=None,
        data_prefix=dict(img='val/images'),
        data_root='data/kostanai',
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=False,
                with_mask=True,
                with_poly_json=False),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='WHUMixVectorDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=False,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = [
    dict(
        ann_file='data/kostanai/val/val.json',
        backend_args=None,
        calculate_iou_ciou=True,
        calculate_mta=True,
        mask_type='polygon',
        metric=[
            'segm',
        ],
        score_thre=0.5,
        type='CocoMetric'),
]
vis_backends = [
    dict(
        init_kwargs=dict(
            allow_val_change=True,
            group='gcp_training',
            id='uyfqo770',
            name='gcp_e5_lre-4_kostanai_afs_v2',
            project='building-segmentation-gcp',
            resume='must'),
        save_dir='work_dir\\gcp_training\\gcp_e5_lre-4_kostanai_afs_v2\\wandb',
        type='WandbVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='TanmlhVisualizer',
    vis_backends=[
        dict(
            init_kwargs=dict(
                allow_val_change=True,
                group='gcp_training',
                id='uyfqo770',
                name='gcp_e5_lre-4_kostanai_afs_v2',
                project='building-segmentation-gcp',
                resume='must'),
            save_dir=
            'work_dir\\gcp_training\\gcp_e5_lre-4_kostanai_afs_v2\\wandb',
            type='WandbVisBackend'),
    ])
work_dir = 'work_dir\\gcp_training\\gcp_e5_lre-4_kostanai_afs_v2'

2025/06/30 13:09:03 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/06/30 13:09:03 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) TanmlhVisualizationHook            
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.0.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.0.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.0.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.0.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.0.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.0.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.1.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.1.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.1.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.1.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.1.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.1.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.2.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.2.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.2.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.2.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.2.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer1.2.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.0.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.0.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.0.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.0.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.0.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.0.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.1.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.1.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.1.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.1.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.1.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.1.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.2.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.2.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.2.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.2.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.2.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.2.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.3.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.3.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.3.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.3.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.3.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer2.3.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.0.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.0.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.0.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.0.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.0.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.0.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.1.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.1.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.1.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.1.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.1.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.1.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.2.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.2.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.2.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.2.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.2.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.2.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.3.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.3.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.3.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.3.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.3.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.3.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.4.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.4.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.4.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.4.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.4.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.4.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.5.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.5.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.5.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.5.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.5.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer3.5.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.0.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.0.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.0.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.0.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.0.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.0.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.0.downsample.1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.0.downsample.1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.1.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.1.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.1.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.1.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.1.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.1.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.2.bn1.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.2.bn1.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.2.bn2.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.2.bn2.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1e-05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.1
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.2.bn3.weight is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - WARNING - backbone.layer4.2.bn3.bias is skipped since its requires_grad=False
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr=0.0001
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr_mult=1.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:decay_mult=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr=0.0001
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr_mult=1.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:decay_mult=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr=0.0001
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr_mult=1.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:decay_mult=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.mask_feat_proj.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.mask_feat_proj.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.mask_feat_proj.4.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.mask_feat_proj.4.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.0.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.0.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.0.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.0.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.0.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.0.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.1.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.1.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.1.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.1.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.1.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.1.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.2.norms.0.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.2.norms.0.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.2.norms.1.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.2.norms.1.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.2.norms.2.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.layers.2.norms.2.bias:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.post_norm.weight:weight_decay=0.0
2025/06/30 13:09:06 - mmengine - INFO - paramwise_options -- panoptic_head.dp_polygonize_head.decoder.post_norm.bias:weight_decay=0.0
2025/06/30 13:09:07 - mmengine - INFO - Frozen parameters: ['backbone', 'panoptic_head.pixel_decoder', 'panoptic_head.transformer_decoder', 'panoptic_head.decoder_input_projs', 'panoptic_head.query_embed', 'panoptic_head.query_feat', 'panoptic_head.level_embed', 'panoptic_head.cls_embed', 'panoptic_head.mask_embed']
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.0.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.0.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.1.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.1.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.3.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.3.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.4.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.4.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.6.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.mask_feat_proj.6.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.self_attn.attn.in_proj_weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.self_attn.attn.in_proj_bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.self_attn.attn.out_proj.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.self_attn.attn.out_proj.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.cross_attn.attn.in_proj_weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.cross_attn.attn.in_proj_bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.cross_attn.attn.out_proj.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.cross_attn.attn.out_proj.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.ffn.layers.0.0.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.ffn.layers.0.0.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.ffn.layers.1.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.ffn.layers.1.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.norms.0.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.norms.0.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.norms.1.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.norms.1.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.norms.2.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.0.norms.2.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.self_attn.attn.in_proj_weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.self_attn.attn.in_proj_bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.self_attn.attn.out_proj.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.self_attn.attn.out_proj.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.cross_attn.attn.in_proj_weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.cross_attn.attn.in_proj_bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.cross_attn.attn.out_proj.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.cross_attn.attn.out_proj.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.ffn.layers.0.0.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.ffn.layers.0.0.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.ffn.layers.1.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.ffn.layers.1.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.norms.0.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.norms.0.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.norms.1.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.norms.1.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.norms.2.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.1.norms.2.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.self_attn.attn.in_proj_weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.self_attn.attn.in_proj_bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.self_attn.attn.out_proj.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.self_attn.attn.out_proj.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.cross_attn.attn.in_proj_weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.cross_attn.attn.in_proj_bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.cross_attn.attn.out_proj.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.cross_attn.attn.out_proj.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.ffn.layers.0.0.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.ffn.layers.0.0.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.ffn.layers.1.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.ffn.layers.1.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.norms.0.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.norms.0.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.norms.1.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.norms.1.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.norms.2.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.layers.2.norms.2.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.post_norm.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.decoder.post_norm.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.poly_reg_head.0.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.poly_reg_head.0.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.poly_reg_head.2.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.poly_reg_head.2.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.poly_reg_head.4.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.poly_reg_head.4.bias
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.poly_embed.weight
2025/06/30 13:09:07 - mmengine - INFO - Training parameters: panoptic_head.dp_polygonize_head.poly_embed.bias
2025/06/30 13:09:07 - mmengine - INFO - Auto resumed from the latest checkpoint D:\Sagi\GCP\GCP\work_dir\gcp_training\gcp_e5_lre-4_kostanai_afs_v2\epoch_2.pth.
2025/06/30 13:09:11 - mmengine - INFO - Load checkpoint from D:\Sagi\GCP\GCP\work_dir\gcp_training\gcp_e5_lre-4_kostanai_afs_v2\epoch_2.pth
2025/06/30 13:09:11 - mmengine - INFO - resumed epoch: 2, iter: 1884
2025/06/30 13:09:11 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/06/30 13:09:11 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/06/30 13:09:11 - mmengine - INFO - Checkpoints will be saved to D:\Sagi\GCP\GCP\work_dir\gcp_training\gcp_e5_lre-4_kostanai_afs_v2.
2025/06/30 13:10:48 - mmengine - INFO - Epoch(train) [3][ 50/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:02:17  time: 2.0780  data_time: 0.0023  memory: 6737  grad_norm: 2.8614  loss: 18.2819  loss_cls: 0.2962  loss_mask: 0.0823  loss_dice: 0.8677  loss_poly_reg: 1.2509  loss_poly_ang: 0.8663  loss_dp: 0.3994  d0.loss_cls: 1.0833  d0.loss_mask: 0.1198  d0.loss_dice: 1.0529  d1.loss_cls: 0.4595  d1.loss_mask: 0.1203  d1.loss_dice: 1.0052  d2.loss_cls: 0.4128  d2.loss_mask: 0.1003  d2.loss_dice: 0.9526  d3.loss_cls: 0.3865  d3.loss_mask: 0.1078  d3.loss_dice: 0.9230  d4.loss_cls: 0.3666  d4.loss_mask: 0.0849  d4.loss_dice: 0.9115  d5.loss_cls: 0.2903  d5.loss_mask: 0.0902  d5.loss_dice: 0.9531  d6.loss_cls: 0.2946  d6.loss_mask: 0.0871  d6.loss_dice: 0.9086  d7.loss_cls: 0.3013  d7.loss_mask: 0.0851  d7.loss_dice: 0.9018  d8.loss_cls: 0.3101  d8.loss_mask: 0.0839  d8.loss_dice: 0.8800  d9.loss_cls: 0.2962  d9.loss_mask: 0.0823  d9.loss_dice: 0.8677
2025/06/30 13:12:17 - mmengine - INFO - Epoch(train) [3][100/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:52:19  time: 1.9363  data_time: 0.0028  memory: 7811  grad_norm: 2.4088  loss: 17.3947  loss_cls: 0.2262  loss_mask: 0.0690  loss_dice: 0.8548  loss_poly_reg: 1.1206  loss_poly_ang: 0.9325  loss_dp: 0.3807  d0.loss_cls: 1.1610  d0.loss_mask: 0.0998  d0.loss_dice: 1.0452  d1.loss_cls: 0.4236  d1.loss_mask: 0.0772  d1.loss_dice: 0.9777  d2.loss_cls: 0.3679  d2.loss_mask: 0.0749  d2.loss_dice: 0.9634  d3.loss_cls: 0.3650  d3.loss_mask: 0.0718  d3.loss_dice: 0.9110  d4.loss_cls: 0.3076  d4.loss_mask: 0.0748  d4.loss_dice: 0.8822  d5.loss_cls: 0.2883  d5.loss_mask: 0.0715  d5.loss_dice: 0.9126  d6.loss_cls: 0.2595  d6.loss_mask: 0.0692  d6.loss_dice: 0.8745  d7.loss_cls: 0.2668  d7.loss_mask: 0.0699  d7.loss_dice: 0.8407  d8.loss_cls: 0.2489  d8.loss_mask: 0.0697  d8.loss_dice: 0.8862  d9.loss_cls: 0.2262  d9.loss_mask: 0.0690  d9.loss_dice: 0.8548
2025/06/30 13:12:49 - mmengine - INFO - Exp name: gcp_r50_kazgisa-kostanai_20250630_130850
2025/06/30 13:14:16 - mmengine - INFO - Epoch(train) [3][150/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:06:53  time: 2.4629  data_time: 0.0012  memory: 8343  grad_norm: 2.6386  loss: 17.1295  loss_cls: 0.2279  loss_mask: 0.0669  loss_dice: 0.8500  loss_poly_reg: 1.1078  loss_poly_ang: 0.8592  loss_dp: 0.3753  d0.loss_cls: 1.1503  d0.loss_mask: 0.0947  d0.loss_dice: 1.0185  d1.loss_cls: 0.4599  d1.loss_mask: 0.0877  d1.loss_dice: 0.9623  d2.loss_cls: 0.3757  d2.loss_mask: 0.0861  d2.loss_dice: 0.9183  d3.loss_cls: 0.3183  d3.loss_mask: 0.0877  d3.loss_dice: 0.9306  d4.loss_cls: 0.3356  d4.loss_mask: 0.0720  d4.loss_dice: 0.8993  d5.loss_cls: 0.2596  d5.loss_mask: 0.0694  d5.loss_dice: 0.8607  d6.loss_cls: 0.2537  d6.loss_mask: 0.0682  d6.loss_dice: 0.8750  d7.loss_cls: 0.2494  d7.loss_mask: 0.0695  d7.loss_dice: 0.8556  d8.loss_cls: 0.2592  d8.loss_mask: 0.0684  d8.loss_dice: 0.8121  d9.loss_cls: 0.2279  d9.loss_mask: 0.0669  d9.loss_dice: 0.8500
2025/06/30 13:16:18 - mmengine - INFO - Epoch(train) [3][200/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:14:07  time: 1.7364  data_time: 0.0045  memory: 8230  grad_norm: 3.1524  loss: 14.8799  loss_cls: 0.1993  loss_mask: 0.0689  loss_dice: 0.7482  loss_poly_reg: 1.1287  loss_poly_ang: 0.7107  loss_dp: 0.3705  d0.loss_cls: 0.6637  d0.loss_mask: 0.1098  d0.loss_dice: 0.9819  d1.loss_cls: 0.3950  d1.loss_mask: 0.0797  d1.loss_dice: 0.7639  d2.loss_cls: 0.3544  d2.loss_mask: 0.0800  d2.loss_dice: 0.7956  d3.loss_cls: 0.3651  d3.loss_mask: 0.0787  d3.loss_dice: 0.7031  d4.loss_cls: 0.2719  d4.loss_mask: 0.0859  d4.loss_dice: 0.7439  d5.loss_cls: 0.2488  d5.loss_mask: 0.0763  d5.loss_dice: 0.7371  d6.loss_cls: 0.2568  d6.loss_mask: 0.0735  d6.loss_dice: 0.6976  d7.loss_cls: 0.2350  d7.loss_mask: 0.0708  d7.loss_dice: 0.7243  d8.loss_cls: 0.2181  d8.loss_mask: 0.0727  d8.loss_dice: 0.7536  d9.loss_cls: 0.1993  d9.loss_mask: 0.0689  d9.loss_dice: 0.7482
2025/06/30 13:18:15 - mmengine - INFO - Epoch(train) [3][250/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:15:53  time: 2.3760  data_time: 0.0027  memory: 6339  grad_norm: 2.7995  loss: 16.7967  loss_cls: 0.2095  loss_mask: 0.0741  loss_dice: 0.8349  loss_poly_reg: 1.2239  loss_poly_ang: 0.9216  loss_dp: 0.4115  d0.loss_cls: 1.1307  d0.loss_mask: 0.0981  d0.loss_dice: 0.9275  d1.loss_cls: 0.3720  d1.loss_mask: 0.0803  d1.loss_dice: 0.9041  d2.loss_cls: 0.3252  d2.loss_mask: 0.0763  d2.loss_dice: 0.9177  d3.loss_cls: 0.3218  d3.loss_mask: 0.0743  d3.loss_dice: 0.9066  d4.loss_cls: 0.2570  d4.loss_mask: 0.0823  d4.loss_dice: 0.9099  d5.loss_cls: 0.2376  d5.loss_mask: 0.0783  d5.loss_dice: 0.8948  d6.loss_cls: 0.2062  d6.loss_mask: 0.0760  d6.loss_dice: 0.8670  d7.loss_cls: 0.2226  d7.loss_mask: 0.0751  d7.loss_dice: 0.8432  d8.loss_cls: 0.2019  d8.loss_mask: 0.0761  d8.loss_dice: 0.8400  d9.loss_cls: 0.2095  d9.loss_mask: 0.0741  d9.loss_dice: 0.8349
2025/06/30 13:19:44 - mmengine - INFO - Epoch(train) [3][300/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:08:22  time: 1.6331  data_time: 0.0035  memory: 7251  grad_norm: 4.5573  loss: 13.7999  loss_cls: 0.1714  loss_mask: 0.0801  loss_dice: 0.6168  loss_poly_reg: 1.1881  loss_poly_ang: 0.7569  loss_dp: 0.3766  d0.loss_cls: 0.8383  d0.loss_mask: 0.1059  d0.loss_dice: 0.7812  d1.loss_cls: 0.3010  d1.loss_mask: 0.1342  d1.loss_dice: 0.7741  d2.loss_cls: 0.3083  d2.loss_mask: 0.0853  d2.loss_dice: 0.8007  d3.loss_cls: 0.2346  d3.loss_mask: 0.0773  d3.loss_dice: 0.6688  d4.loss_cls: 0.2018  d4.loss_mask: 0.0832  d4.loss_dice: 0.6548  d5.loss_cls: 0.1743  d5.loss_mask: 0.0958  d5.loss_dice: 0.6952  d6.loss_cls: 0.1718  d6.loss_mask: 0.0828  d6.loss_dice: 0.6805  d7.loss_cls: 0.1584  d7.loss_mask: 0.0831  d7.loss_dice: 0.6298  d8.loss_cls: 0.1919  d8.loss_mask: 0.0823  d8.loss_dice: 0.6462  d9.loss_cls: 0.1714  d9.loss_mask: 0.0801  d9.loss_dice: 0.6168
2025/06/30 13:22:09 - mmengine - INFO - Epoch(train) [3][350/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:16:35  time: 3.6609  data_time: 0.0017  memory: 7365  grad_norm: 3.0662  loss: 19.5319  loss_cls: 0.2753  loss_mask: 0.1455  loss_dice: 0.8541  loss_poly_reg: 1.2888  loss_poly_ang: 1.2748  loss_dp: 0.4186  d0.loss_cls: 1.3427  d0.loss_mask: 0.2037  d0.loss_dice: 1.0304  d1.loss_cls: 0.5046  d1.loss_mask: 0.1697  d1.loss_dice: 0.9759  d2.loss_cls: 0.4607  d2.loss_mask: 0.1650  d2.loss_dice: 0.9807  d3.loss_cls: 0.3789  d3.loss_mask: 0.1589  d3.loss_dice: 0.9457  d4.loss_cls: 0.3291  d4.loss_mask: 0.1553  d4.loss_dice: 0.9417  d5.loss_cls: 0.2894  d5.loss_mask: 0.1492  d5.loss_dice: 0.9258  d6.loss_cls: 0.2962  d6.loss_mask: 0.1520  d6.loss_dice: 0.8625  d7.loss_cls: 0.2781  d7.loss_mask: 0.1460  d7.loss_dice: 0.8676  d8.loss_cls: 0.3096  d8.loss_mask: 0.1483  d8.loss_dice: 0.8322  d9.loss_cls: 0.2753  d9.loss_mask: 0.1455  d9.loss_dice: 0.8541
2025/06/30 13:24:00 - mmengine - INFO - Epoch(train) [3][400/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:14:31  time: 1.8239  data_time: 0.0020  memory: 6875  grad_norm: 3.8611  loss: 15.2492  loss_cls: 0.2000  loss_mask: 0.0887  loss_dice: 0.7019  loss_poly_reg: 1.1747  loss_poly_ang: 0.9355  loss_dp: 0.3702  d0.loss_cls: 0.8207  d0.loss_mask: 0.1029  d0.loss_dice: 0.8519  d1.loss_cls: 0.3000  d1.loss_mask: 0.1041  d1.loss_dice: 0.8630  d2.loss_cls: 0.2563  d2.loss_mask: 0.1070  d2.loss_dice: 0.8965  d3.loss_cls: 0.2411  d3.loss_mask: 0.1018  d3.loss_dice: 0.8595  d4.loss_cls: 0.2498  d4.loss_mask: 0.1051  d4.loss_dice: 0.7877  d5.loss_cls: 0.1794  d5.loss_mask: 0.0942  d5.loss_dice: 0.7636  d6.loss_cls: 0.2007  d6.loss_mask: 0.0974  d6.loss_dice: 0.7628  d7.loss_cls: 0.1859  d7.loss_mask: 0.0951  d7.loss_dice: 0.7325  d8.loss_cls: 0.1887  d8.loss_mask: 0.0931  d8.loss_dice: 0.7468  d9.loss_cls: 0.2000  d9.loss_mask: 0.0887  d9.loss_dice: 0.7019
2025/06/30 13:25:55 - mmengine - INFO - Epoch(train) [3][450/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:13:29  time: 2.7451  data_time: 0.0045  memory: 6753  grad_norm: 2.6748  loss: 18.0265  loss_cls: 0.2786  loss_mask: 0.0865  loss_dice: 0.8318  loss_poly_reg: 1.1508  loss_poly_ang: 0.8427  loss_dp: 0.3962  d0.loss_cls: 1.1351  d0.loss_mask: 0.1141  d0.loss_dice: 0.9489  d1.loss_cls: 0.4968  d1.loss_mask: 0.0989  d1.loss_dice: 0.9697  d2.loss_cls: 0.4782  d2.loss_mask: 0.0926  d2.loss_dice: 0.9380  d3.loss_cls: 0.3934  d3.loss_mask: 0.0889  d3.loss_dice: 0.8990  d4.loss_cls: 0.3429  d4.loss_mask: 0.0940  d4.loss_dice: 0.9399  d5.loss_cls: 0.3007  d5.loss_mask: 0.0916  d5.loss_dice: 0.8963  d6.loss_cls: 0.3167  d6.loss_mask: 0.0854  d6.loss_dice: 0.9369  d7.loss_cls: 0.2954  d7.loss_mask: 0.0820  d7.loss_dice: 0.9257  d8.loss_cls: 0.3082  d8.loss_mask: 0.0861  d8.loss_dice: 0.8879  d9.loss_cls: 0.2786  d9.loss_mask: 0.0865  d9.loss_dice: 0.8318
2025/06/30 13:27:55 - mmengine - INFO - Epoch(train) [3][500/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:13:01  time: 1.9587  data_time: 0.0033  memory: 8090  grad_norm: 3.3943  loss: 13.8867  loss_cls: 0.1705  loss_mask: 0.0730  loss_dice: 0.6822  loss_poly_reg: 1.1636  loss_poly_ang: 0.9266  loss_dp: 0.3978  d0.loss_cls: 0.8332  d0.loss_mask: 0.0869  d0.loss_dice: 0.7494  d1.loss_cls: 0.3487  d1.loss_mask: 0.0777  d1.loss_dice: 0.7681  d2.loss_cls: 0.3204  d2.loss_mask: 0.0764  d2.loss_dice: 0.7251  d3.loss_cls: 0.3047  d3.loss_mask: 0.0738  d3.loss_dice: 0.7118  d4.loss_cls: 0.2567  d4.loss_mask: 0.0717  d4.loss_dice: 0.6716  d5.loss_cls: 0.2091  d5.loss_mask: 0.0725  d5.loss_dice: 0.6887  d6.loss_cls: 0.1834  d6.loss_mask: 0.0725  d6.loss_dice: 0.6579  d7.loss_cls: 0.1935  d7.loss_mask: 0.0683  d7.loss_dice: 0.6519  d8.loss_cls: 0.2002  d8.loss_mask: 0.0712  d8.loss_dice: 0.6439  d9.loss_cls: 0.1705  d9.loss_mask: 0.0730  d9.loss_dice: 0.6822
2025/06/30 13:30:11 - mmengine - INFO - Epoch(train) [3][550/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:14:54  time: 3.2351  data_time: 0.0021  memory: 9237  grad_norm: 2.2948  loss: 16.0826  loss_cls: 0.2296  loss_mask: 0.0684  loss_dice: 0.7753  loss_poly_reg: 1.0829  loss_poly_ang: 0.9534  loss_dp: 0.3756  d0.loss_cls: 1.1504  d0.loss_mask: 0.1034  d0.loss_dice: 0.8973  d1.loss_cls: 0.4482  d1.loss_mask: 0.0776  d1.loss_dice: 0.8721  d2.loss_cls: 0.3682  d2.loss_mask: 0.0764  d2.loss_dice: 0.8110  d3.loss_cls: 0.3761  d3.loss_mask: 0.0815  d3.loss_dice: 0.7849  d4.loss_cls: 0.2875  d4.loss_mask: 0.0738  d4.loss_dice: 0.7971  d5.loss_cls: 0.2780  d5.loss_mask: 0.0686  d5.loss_dice: 0.7683  d6.loss_cls: 0.2701  d6.loss_mask: 0.0671  d6.loss_dice: 0.7624  d7.loss_cls: 0.2368  d7.loss_mask: 0.0684  d7.loss_dice: 0.7440  d8.loss_cls: 0.2322  d8.loss_mask: 0.0684  d8.loss_dice: 0.7544  d9.loss_cls: 0.2296  d9.loss_mask: 0.0684  d9.loss_dice: 0.7753
2025/06/30 13:31:58 - mmengine - INFO - Epoch(train) [3][600/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:11:55  time: 1.6789  data_time: 0.0039  memory: 4763  grad_norm: 2.7029  loss: 16.1387  loss_cls: 0.2528  loss_mask: 0.0556  loss_dice: 0.8103  loss_poly_reg: 0.9803  loss_poly_ang: 0.7510  loss_dp: 0.3733  d0.loss_cls: 0.9104  d0.loss_mask: 0.0828  d0.loss_dice: 0.9196  d1.loss_cls: 0.4081  d1.loss_mask: 0.0661  d1.loss_dice: 0.8715  d2.loss_cls: 0.3652  d2.loss_mask: 0.0654  d2.loss_dice: 0.9124  d3.loss_cls: 0.3328  d3.loss_mask: 0.0669  d3.loss_dice: 0.8797  d4.loss_cls: 0.3140  d4.loss_mask: 0.0640  d4.loss_dice: 0.8846  d5.loss_cls: 0.2840  d5.loss_mask: 0.0581  d5.loss_dice: 0.7924  d6.loss_cls: 0.2788  d6.loss_mask: 0.0579  d6.loss_dice: 0.8426  d7.loss_cls: 0.2594  d7.loss_mask: 0.0581  d7.loss_dice: 0.8371  d8.loss_cls: 0.2847  d8.loss_mask: 0.0586  d8.loss_dice: 0.8413  d9.loss_cls: 0.2528  d9.loss_mask: 0.0556  d9.loss_dice: 0.8103
2025/06/30 13:34:02 - mmengine - INFO - Epoch(train) [3][650/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:11:17  time: 3.6629  data_time: 0.0021  memory: 7082  grad_norm: 3.0695  loss: 18.7781  loss_cls: 0.2749  loss_mask: 0.1056  loss_dice: 0.8107  loss_poly_reg: 1.3043  loss_poly_ang: 1.0695  loss_dp: 0.3975  d0.loss_cls: 1.2868  d0.loss_mask: 0.1516  d0.loss_dice: 1.0467  d1.loss_cls: 0.4918  d1.loss_mask: 0.1230  d1.loss_dice: 1.0159  d2.loss_cls: 0.4758  d2.loss_mask: 0.1232  d2.loss_dice: 0.9524  d3.loss_cls: 0.4145  d3.loss_mask: 0.1125  d3.loss_dice: 0.8663  d4.loss_cls: 0.3778  d4.loss_mask: 0.1458  d4.loss_dice: 0.9190  d5.loss_cls: 0.3183  d5.loss_mask: 0.1213  d5.loss_dice: 0.9126  d6.loss_cls: 0.3070  d6.loss_mask: 0.1121  d6.loss_dice: 0.8357  d7.loss_cls: 0.2641  d7.loss_mask: 0.1113  d7.loss_dice: 0.8848  d8.loss_cls: 0.2820  d8.loss_mask: 0.1117  d8.loss_dice: 0.8606  d9.loss_cls: 0.2749  d9.loss_mask: 0.1056  d9.loss_dice: 0.8107
2025/06/30 13:35:59 - mmengine - INFO - Epoch(train) [3][700/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:09:40  time: 1.4971  data_time: 0.0021  memory: 7671  grad_norm: 3.0446  loss: 15.1424  loss_cls: 0.1879  loss_mask: 0.0859  loss_dice: 0.7365  loss_poly_reg: 1.0760  loss_poly_ang: 0.7661  loss_dp: 0.3770  d0.loss_cls: 0.5956  d0.loss_mask: 0.1201  d0.loss_dice: 0.8580  d1.loss_cls: 0.2571  d1.loss_mask: 0.1107  d1.loss_dice: 0.8582  d2.loss_cls: 0.2564  d2.loss_mask: 0.1084  d2.loss_dice: 0.8121  d3.loss_cls: 0.2231  d3.loss_mask: 0.1138  d3.loss_dice: 0.8927  d4.loss_cls: 0.2313  d4.loss_mask: 0.0988  d4.loss_dice: 0.8311  d5.loss_cls: 0.1735  d5.loss_mask: 0.1123  d5.loss_dice: 0.8872  d6.loss_cls: 0.1700  d6.loss_mask: 0.1053  d6.loss_dice: 0.8251  d7.loss_cls: 0.1673  d7.loss_mask: 0.0947  d7.loss_dice: 0.9165  d8.loss_cls: 0.1721  d8.loss_mask: 0.0897  d8.loss_dice: 0.8216  d9.loss_cls: 0.1879  d9.loss_mask: 0.0859  d9.loss_dice: 0.7365
2025/06/30 13:38:07 - mmengine - INFO - Epoch(train) [3][750/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:09:12  time: 2.5837  data_time: 0.0011  memory: 6088  grad_norm: 3.5685  loss: 17.0493  loss_cls: 0.2418  loss_mask: 0.0996  loss_dice: 0.8585  loss_poly_reg: 1.1752  loss_poly_ang: 0.7388  loss_dp: 0.3841  d0.loss_cls: 0.9959  d0.loss_mask: 0.1228  d0.loss_dice: 0.9305  d1.loss_cls: 0.3592  d1.loss_mask: 0.1135  d1.loss_dice: 0.9444  d2.loss_cls: 0.3506  d2.loss_mask: 0.1133  d2.loss_dice: 0.9117  d3.loss_cls: 0.3458  d3.loss_mask: 0.1036  d3.loss_dice: 0.9390  d4.loss_cls: 0.2820  d4.loss_mask: 0.1067  d4.loss_dice: 0.8953  d5.loss_cls: 0.2828  d5.loss_mask: 0.1031  d5.loss_dice: 0.8809  d6.loss_cls: 0.2366  d6.loss_mask: 0.1007  d6.loss_dice: 0.8384  d7.loss_cls: 0.2313  d7.loss_mask: 0.1045  d7.loss_dice: 0.8791  d8.loss_cls: 0.2316  d8.loss_mask: 0.0996  d8.loss_dice: 0.8481  d9.loss_cls: 0.2418  d9.loss_mask: 0.0996  d9.loss_dice: 0.8585
2025/06/30 13:40:15 - mmengine - INFO - Epoch(train) [3][800/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:08:26  time: 2.0565  data_time: 0.0018  memory: 6562  grad_norm: 3.4206  loss: 16.9979  loss_cls: 0.2777  loss_mask: 0.0774  loss_dice: 0.7603  loss_poly_reg: 1.0627  loss_poly_ang: 0.8006  loss_dp: 0.3687  d0.loss_cls: 0.9709  d0.loss_mask: 0.1842  d0.loss_dice: 0.9829  d1.loss_cls: 0.4781  d1.loss_mask: 0.1312  d1.loss_dice: 0.8590  d2.loss_cls: 0.4339  d2.loss_mask: 0.1177  d2.loss_dice: 0.8090  d3.loss_cls: 0.4297  d3.loss_mask: 0.0993  d3.loss_dice: 0.8915  d4.loss_cls: 0.3626  d4.loss_mask: 0.1092  d4.loss_dice: 0.8736  d5.loss_cls: 0.3404  d5.loss_mask: 0.0930  d5.loss_dice: 0.8194  d6.loss_cls: 0.3354  d6.loss_mask: 0.0919  d6.loss_dice: 0.7953  d7.loss_cls: 0.2975  d7.loss_mask: 0.0886  d7.loss_dice: 0.7500  d8.loss_cls: 0.3296  d8.loss_mask: 0.0912  d8.loss_dice: 0.7701  d9.loss_cls: 0.2777  d9.loss_mask: 0.0774  d9.loss_dice: 0.7603
2025/06/30 13:41:59 - mmengine - INFO - Epoch(train) [3][850/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:05:21  time: 1.4727  data_time: 0.0000  memory: 6852  grad_norm: 3.9761  loss: 15.8928  loss_cls: 0.2402  loss_mask: 0.0886  loss_dice: 0.7258  loss_poly_reg: 1.1909  loss_poly_ang: 0.8671  loss_dp: 0.3780  d0.loss_cls: 0.7959  d0.loss_mask: 0.1167  d0.loss_dice: 0.8297  d1.loss_cls: 0.3932  d1.loss_mask: 0.1237  d1.loss_dice: 0.8804  d2.loss_cls: 0.3614  d2.loss_mask: 0.1186  d2.loss_dice: 0.8228  d3.loss_cls: 0.3746  d3.loss_mask: 0.0974  d3.loss_dice: 0.7964  d4.loss_cls: 0.3132  d4.loss_mask: 0.1105  d4.loss_dice: 0.8181  d5.loss_cls: 0.2633  d5.loss_mask: 0.0943  d5.loss_dice: 0.7813  d6.loss_cls: 0.2448  d6.loss_mask: 0.0887  d6.loss_dice: 0.7373  d7.loss_cls: 0.2565  d7.loss_mask: 0.0900  d7.loss_dice: 0.7335  d8.loss_cls: 0.2443  d8.loss_mask: 0.0911  d8.loss_dice: 0.7699  d9.loss_cls: 0.2402  d9.loss_mask: 0.0886  d9.loss_dice: 0.7258
2025/06/30 13:44:17 - mmengine - INFO - Epoch(train) [3][900/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:05:20  time: 2.0901  data_time: 0.0021  memory: 8631  grad_norm: 2.7611  loss: 18.6131  loss_cls: 0.2912  loss_mask: 0.1103  loss_dice: 0.8811  loss_poly_reg: 1.2054  loss_poly_ang: 0.8740  loss_dp: 0.4057  d0.loss_cls: 0.8352  d0.loss_mask: 0.1935  d0.loss_dice: 1.0196  d1.loss_cls: 0.4726  d1.loss_mask: 0.1858  d1.loss_dice: 0.9949  d2.loss_cls: 0.4261  d2.loss_mask: 0.1514  d2.loss_dice: 0.9778  d3.loss_cls: 0.3937  d3.loss_mask: 0.1849  d3.loss_dice: 0.9368  d4.loss_cls: 0.3738  d4.loss_mask: 0.1685  d4.loss_dice: 0.8717  d5.loss_cls: 0.3582  d5.loss_mask: 0.1459  d5.loss_dice: 0.9318  d6.loss_cls: 0.3341  d6.loss_mask: 0.1266  d6.loss_dice: 0.8720  d7.loss_cls: 0.3066  d7.loss_mask: 0.1198  d7.loss_dice: 0.8629  d8.loss_cls: 0.3312  d8.loss_mask: 0.1173  d8.loss_dice: 0.8703  d9.loss_cls: 0.2912  d9.loss_mask: 0.1103  d9.loss_dice: 0.8811
2025/06/30 13:46:03 - mmengine - INFO - Exp name: gcp_r50_kazgisa-kostanai_20250630_130850
2025/06/30 13:46:04 - mmengine - INFO - Saving checkpoint at 3 epochs
2025/06/30 13:56:05 - mmengine - INFO - Epoch(val) [3][50/53]    eta: 0:00:35  time: 9.9113  data_time: 0.5176  memory: 7908  
2025/06/30 13:56:23 - mmengine - INFO - Evaluating segm...
2025/06/30 13:56:29 - mmengine - INFO - segm_mAP_copypaste: 0.515 0.785 0.570 0.293 0.675 0.826
2025/06/30 13:56:40 - mmengine - INFO - mta: 38.48620516835087
2025/06/30 13:56:44 - mmengine - INFO - iou: 0.7920996971250511, c_iou: 0.6756825681145666, N_ratio: 0.8117978918196664
2025/06/30 13:56:44 - mmengine - INFO - Epoch(val) [3][53/53]    coco/segm_mAP: 0.5150  coco/segm_mAP_50: 0.7850  coco/segm_mAP_75: 0.5700  coco/segm_mAP_s: 0.2930  coco/segm_mAP_m: 0.6750  coco/segm_mAP_l: 0.8260  coco/mta: 38.4862  coco/iou: 0.7921  coco/c_iou: 0.6757  coco/N_ratio: 0.8118  data_time: 0.6778  time: 11.5326
2025/06/30 13:58:43 - mmengine - INFO - Epoch(train) [4][ 50/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:02:32  time: 2.4144  data_time: 0.0037  memory: 6608  grad_norm: 3.2810  loss: 14.9312  loss_cls: 0.1774  loss_mask: 0.0771  loss_dice: 0.7686  loss_poly_reg: 1.1466  loss_poly_ang: 0.8720  loss_dp: 0.3847  d0.loss_cls: 0.8769  d0.loss_mask: 0.0978  d0.loss_dice: 0.7896  d1.loss_cls: 0.3385  d1.loss_mask: 0.0790  d1.loss_dice: 0.7945  d2.loss_cls: 0.2871  d2.loss_mask: 0.0756  d2.loss_dice: 0.7519  d3.loss_cls: 0.3030  d3.loss_mask: 0.0752  d3.loss_dice: 0.7730  d4.loss_cls: 0.2350  d4.loss_mask: 0.0799  d4.loss_dice: 0.7445  d5.loss_cls: 0.2065  d5.loss_mask: 0.0773  d5.loss_dice: 0.7739  d6.loss_cls: 0.2109  d6.loss_mask: 0.0766  d6.loss_dice: 0.7771  d7.loss_cls: 0.2153  d7.loss_mask: 0.0734  d7.loss_dice: 0.7287  d8.loss_cls: 0.2001  d8.loss_mask: 0.0750  d8.loss_dice: 0.7656  d9.loss_cls: 0.1774  d9.loss_mask: 0.0771  d9.loss_dice: 0.7686
2025/06/30 14:00:46 - mmengine - INFO - Epoch(train) [4][100/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:01:01  time: 2.9011  data_time: 0.0006  memory: 8830  grad_norm: 3.0273  loss: 14.7508  loss_cls: 0.2048  loss_mask: 0.0614  loss_dice: 0.6900  loss_poly_reg: 1.0421  loss_poly_ang: 0.8210  loss_dp: 0.3683  d0.loss_cls: 1.0283  d0.loss_mask: 0.0928  d0.loss_dice: 0.9187  d1.loss_cls: 0.4123  d1.loss_mask: 0.0655  d1.loss_dice: 0.7602  d2.loss_cls: 0.3501  d2.loss_mask: 0.0788  d2.loss_dice: 0.7536  d3.loss_cls: 0.3219  d3.loss_mask: 0.0616  d3.loss_dice: 0.7141  d4.loss_cls: 0.2957  d4.loss_mask: 0.0654  d4.loss_dice: 0.7227  d5.loss_cls: 0.2544  d5.loss_mask: 0.0641  d5.loss_dice: 0.7088  d6.loss_cls: 0.2531  d6.loss_mask: 0.0608  d6.loss_dice: 0.7047  d7.loss_cls: 0.2340  d7.loss_mask: 0.0596  d7.loss_dice: 0.6782  d8.loss_cls: 0.2319  d8.loss_mask: 0.0589  d8.loss_dice: 0.6567  d9.loss_cls: 0.2048  d9.loss_mask: 0.0614  d9.loss_dice: 0.6900
2025/06/30 14:02:40 - mmengine - INFO - Epoch(train) [4][150/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:58:47  time: 2.3895  data_time: 0.0034  memory: 5636  grad_norm: 3.6207  loss: 15.3476  loss_cls: 0.1698  loss_mask: 0.0703  loss_dice: 0.7542  loss_poly_reg: 1.1000  loss_poly_ang: 0.8987  loss_dp: 0.3701  d0.loss_cls: 1.0425  d0.loss_mask: 0.0926  d0.loss_dice: 0.9377  d1.loss_cls: 0.3421  d1.loss_mask: 0.0762  d1.loss_dice: 0.8398  d2.loss_cls: 0.3165  d2.loss_mask: 0.0666  d2.loss_dice: 0.8528  d3.loss_cls: 0.2920  d3.loss_mask: 0.0659  d3.loss_dice: 0.8076  d4.loss_cls: 0.2274  d4.loss_mask: 0.0702  d4.loss_dice: 0.8094  d5.loss_cls: 0.2169  d5.loss_mask: 0.0829  d5.loss_dice: 0.7823  d6.loss_cls: 0.1791  d6.loss_mask: 0.0731  d6.loss_dice: 0.7840  d7.loss_cls: 0.1860  d7.loss_mask: 0.0674  d7.loss_dice: 0.7925  d8.loss_cls: 0.1798  d8.loss_mask: 0.0660  d8.loss_dice: 0.7411  d9.loss_cls: 0.1698  d9.loss_mask: 0.0703  d9.loss_dice: 0.7542
2025/06/30 14:03:33 - mmengine - INFO - Exp name: gcp_r50_kazgisa-kostanai_20250630_130850
2025/06/30 14:04:40 - mmengine - INFO - Epoch(train) [4][200/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:56:57  time: 2.5388  data_time: 0.0055  memory: 6176  grad_norm: 2.2641  loss: 19.2895  loss_cls: 0.3345  loss_mask: 0.0826  loss_dice: 0.8574  loss_poly_reg: 1.1617  loss_poly_ang: 1.0428  loss_dp: 0.3775  d0.loss_cls: 1.1603  d0.loss_mask: 0.1071  d0.loss_dice: 1.0795  d1.loss_cls: 0.5887  d1.loss_mask: 0.1080  d1.loss_dice: 1.0579  d2.loss_cls: 0.5130  d2.loss_mask: 0.0947  d2.loss_dice: 0.9939  d3.loss_cls: 0.4894  d3.loss_mask: 0.0861  d3.loss_dice: 0.9869  d4.loss_cls: 0.4258  d4.loss_mask: 0.0906  d4.loss_dice: 0.9480  d5.loss_cls: 0.3787  d5.loss_mask: 0.0886  d5.loss_dice: 0.9669  d6.loss_cls: 0.3411  d6.loss_mask: 0.0865  d6.loss_dice: 0.8956  d7.loss_cls: 0.3512  d7.loss_mask: 0.0849  d7.loss_dice: 0.9045  d8.loss_cls: 0.3728  d8.loss_mask: 0.0821  d8.loss_dice: 0.8755  d9.loss_cls: 0.3345  d9.loss_mask: 0.0826  d9.loss_dice: 0.8574
2025/06/30 14:06:39 - mmengine - INFO - Epoch(train) [4][250/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:55:07  time: 3.1215  data_time: 0.0016  memory: 6273  grad_norm: 3.3691  loss: 18.6695  loss_cls: 0.2707  loss_mask: 0.0793  loss_dice: 0.9639  loss_poly_reg: 1.1055  loss_poly_ang: 0.8230  loss_dp: 0.3557  d0.loss_cls: 1.0588  d0.loss_mask: 0.1148  d0.loss_dice: 1.1556  d1.loss_cls: 0.4047  d1.loss_mask: 0.0941  d1.loss_dice: 1.1283  d2.loss_cls: 0.3695  d2.loss_mask: 0.0951  d2.loss_dice: 1.0717  d3.loss_cls: 0.3898  d3.loss_mask: 0.0775  d3.loss_dice: 0.9750  d4.loss_cls: 0.3248  d4.loss_mask: 0.0863  d4.loss_dice: 1.0583  d5.loss_cls: 0.2950  d5.loss_mask: 0.0857  d5.loss_dice: 0.9941  d6.loss_cls: 0.2786  d6.loss_mask: 0.0804  d6.loss_dice: 0.9653  d7.loss_cls: 0.2439  d7.loss_mask: 0.0817  d7.loss_dice: 1.0098  d8.loss_cls: 0.2590  d8.loss_mask: 0.0807  d8.loss_dice: 0.9792  d9.loss_cls: 0.2707  d9.loss_mask: 0.0793  d9.loss_dice: 0.9639
2025/06/30 14:08:45 - mmengine - INFO - Epoch(train) [4][300/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:53:37  time: 2.7780  data_time: 0.0011  memory: 6238  grad_norm: 3.8657  loss: 18.1234  loss_cls: 0.2245  loss_mask: 0.2424  loss_dice: 0.7743  loss_poly_reg: 1.0698  loss_poly_ang: 0.7608  loss_dp: 0.4072  d0.loss_cls: 0.9351  d0.loss_mask: 0.2990  d0.loss_dice: 1.0755  d1.loss_cls: 0.3994  d1.loss_mask: 0.2339  d1.loss_dice: 0.9079  d2.loss_cls: 0.3615  d2.loss_mask: 0.2849  d2.loss_dice: 0.9471  d3.loss_cls: 0.3270  d3.loss_mask: 0.2102  d3.loss_dice: 0.8651  d4.loss_cls: 0.2648  d4.loss_mask: 0.1973  d4.loss_dice: 0.8000  d5.loss_cls: 0.2637  d5.loss_mask: 0.2554  d5.loss_dice: 0.8162  d6.loss_cls: 0.2369  d6.loss_mask: 0.2654  d6.loss_dice: 0.7884  d7.loss_cls: 0.2267  d7.loss_mask: 0.2435  d7.loss_dice: 0.7931  d8.loss_cls: 0.2351  d8.loss_mask: 0.3209  d8.loss_dice: 0.8492  d9.loss_cls: 0.2245  d9.loss_mask: 0.2424  d9.loss_dice: 0.7743
2025/06/30 14:10:58 - mmengine - INFO - Epoch(train) [4][350/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:52:29  time: 1.9801  data_time: 0.0021  memory: 5601  grad_norm: 2.8586  loss: 15.5990  loss_cls: 0.2641  loss_mask: 0.1042  loss_dice: 0.7045  loss_poly_reg: 1.2294  loss_poly_ang: 0.8076  loss_dp: 0.3884  d0.loss_cls: 0.8769  d0.loss_mask: 0.1244  d0.loss_dice: 0.8577  d1.loss_cls: 0.3825  d1.loss_mask: 0.1219  d1.loss_dice: 0.7630  d2.loss_cls: 0.3514  d2.loss_mask: 0.1241  d2.loss_dice: 0.7840  d3.loss_cls: 0.3218  d3.loss_mask: 0.1198  d3.loss_dice: 0.7757  d4.loss_cls: 0.3226  d4.loss_mask: 0.1193  d4.loss_dice: 0.6938  d5.loss_cls: 0.2381  d5.loss_mask: 0.1107  d5.loss_dice: 0.7207  d6.loss_cls: 0.2373  d6.loss_mask: 0.1065  d6.loss_dice: 0.7136  d7.loss_cls: 0.2495  d7.loss_mask: 0.1065  d7.loss_dice: 0.7121  d8.loss_cls: 0.2791  d8.loss_mask: 0.1087  d8.loss_dice: 0.7063  d9.loss_cls: 0.2641  d9.loss_mask: 0.1042  d9.loss_dice: 0.7045
2025/06/30 14:12:52 - mmengine - INFO - Epoch(train) [4][400/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:50:14  time: 1.8940  data_time: 0.0046  memory: 6956  grad_norm: 3.6392  loss: 14.6625  loss_cls: 0.2157  loss_mask: 0.0621  loss_dice: 0.6630  loss_poly_reg: 0.9970  loss_poly_ang: 0.7970  loss_dp: 0.3998  d0.loss_cls: 0.8789  d0.loss_mask: 0.0887  d0.loss_dice: 0.8635  d1.loss_cls: 0.4068  d1.loss_mask: 0.0814  d1.loss_dice: 0.6874  d2.loss_cls: 0.3194  d2.loss_mask: 0.0895  d2.loss_dice: 0.8800  d3.loss_cls: 0.3043  d3.loss_mask: 0.0746  d3.loss_dice: 0.8012  d4.loss_cls: 0.2496  d4.loss_mask: 0.0756  d4.loss_dice: 0.8727  d5.loss_cls: 0.2135  d5.loss_mask: 0.0728  d5.loss_dice: 0.6753  d6.loss_cls: 0.2266  d6.loss_mask: 0.0693  d6.loss_dice: 0.7063  d7.loss_cls: 0.2287  d7.loss_mask: 0.0635  d7.loss_dice: 0.6816  d8.loss_cls: 0.2133  d8.loss_mask: 0.0645  d8.loss_dice: 0.6987  d9.loss_cls: 0.2157  d9.loss_mask: 0.0621  d9.loss_dice: 0.6630
2025/06/30 14:15:29 - mmengine - INFO - Epoch(train) [4][450/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:50:14  time: 3.2124  data_time: 0.0005  memory: 6565  grad_norm: 2.9550  loss: 17.2196  loss_cls: 0.2897  loss_mask: 0.0639  loss_dice: 0.8601  loss_poly_reg: 0.9715  loss_poly_ang: 0.6262  loss_dp: 0.3558  d0.loss_cls: 1.1259  d0.loss_mask: 0.0839  d0.loss_dice: 1.0037  d1.loss_cls: 0.4702  d1.loss_mask: 0.0792  d1.loss_dice: 1.0108  d2.loss_cls: 0.3913  d2.loss_mask: 0.0783  d2.loss_dice: 0.9911  d3.loss_cls: 0.3818  d3.loss_mask: 0.0713  d3.loss_dice: 0.9117  d4.loss_cls: 0.3417  d4.loss_mask: 0.0654  d4.loss_dice: 0.9104  d5.loss_cls: 0.3391  d5.loss_mask: 0.0678  d5.loss_dice: 0.8763  d6.loss_cls: 0.2930  d6.loss_mask: 0.0624  d6.loss_dice: 0.8704  d7.loss_cls: 0.2884  d7.loss_mask: 0.0675  d7.loss_dice: 0.8618  d8.loss_cls: 0.2970  d8.loss_mask: 0.0667  d8.loss_dice: 0.8315  d9.loss_cls: 0.2897  d9.loss_mask: 0.0639  d9.loss_dice: 0.8601
2025/06/30 14:17:29 - mmengine - INFO - Epoch(train) [4][500/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:48:15  time: 2.6384  data_time: 0.0025  memory: 7152  grad_norm: 2.6058  loss: 18.4009  loss_cls: 0.2556  loss_mask: 0.0886  loss_dice: 0.9268  loss_poly_reg: 1.3756  loss_poly_ang: 0.9067  loss_dp: 0.3804  d0.loss_cls: 1.0822  d0.loss_mask: 0.1106  d0.loss_dice: 1.0775  d1.loss_cls: 0.4580  d1.loss_mask: 0.0979  d1.loss_dice: 1.0025  d2.loss_cls: 0.4008  d2.loss_mask: 0.1030  d2.loss_dice: 0.9481  d3.loss_cls: 0.4045  d3.loss_mask: 0.0953  d3.loss_dice: 0.9880  d4.loss_cls: 0.3494  d4.loss_mask: 0.0963  d4.loss_dice: 0.9391  d5.loss_cls: 0.2964  d5.loss_mask: 0.1004  d5.loss_dice: 0.9475  d6.loss_cls: 0.2597  d6.loss_mask: 0.0913  d6.loss_dice: 0.8831  d7.loss_cls: 0.2478  d7.loss_mask: 0.0914  d7.loss_dice: 0.8817  d8.loss_cls: 0.2467  d8.loss_mask: 0.0901  d8.loss_dice: 0.9071  d9.loss_cls: 0.2556  d9.loss_mask: 0.0886  d9.loss_dice: 0.9268
2025/06/30 14:19:23 - mmengine - INFO - Epoch(train) [4][550/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:46:00  time: 3.4060  data_time: 0.0018  memory: 6290  grad_norm: 3.0589  loss: 17.4691  loss_cls: 0.3048  loss_mask: 0.0864  loss_dice: 0.8286  loss_poly_reg: 1.0309  loss_poly_ang: 0.7791  loss_dp: 0.3694  d0.loss_cls: 1.0881  d0.loss_mask: 0.0961  d0.loss_dice: 0.9887  d1.loss_cls: 0.4697  d1.loss_mask: 0.1016  d1.loss_dice: 0.9236  d2.loss_cls: 0.4173  d2.loss_mask: 0.0969  d2.loss_dice: 0.9575  d3.loss_cls: 0.3672  d3.loss_mask: 0.0984  d3.loss_dice: 0.8991  d4.loss_cls: 0.3604  d4.loss_mask: 0.0970  d4.loss_dice: 0.9114  d5.loss_cls: 0.3143  d5.loss_mask: 0.0915  d5.loss_dice: 0.9019  d6.loss_cls: 0.3006  d6.loss_mask: 0.0918  d6.loss_dice: 0.8215  d7.loss_cls: 0.3018  d7.loss_mask: 0.0899  d7.loss_dice: 0.8360  d8.loss_cls: 0.2961  d8.loss_mask: 0.0880  d8.loss_dice: 0.8438  d9.loss_cls: 0.3048  d9.loss_mask: 0.0864  d9.loss_dice: 0.8286
2025/06/30 14:21:59 - mmengine - INFO - Epoch(train) [4][600/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:45:34  time: 3.7938  data_time: 0.0018  memory: 7924  grad_norm: 2.7217  loss: 16.7137  loss_cls: 0.2660  loss_mask: 0.0674  loss_dice: 0.7808  loss_poly_reg: 1.0908  loss_poly_ang: 0.8922  loss_dp: 0.3747  d0.loss_cls: 1.2084  d0.loss_mask: 0.1038  d0.loss_dice: 0.9750  d1.loss_cls: 0.4695  d1.loss_mask: 0.0823  d1.loss_dice: 0.9126  d2.loss_cls: 0.3866  d2.loss_mask: 0.0832  d2.loss_dice: 0.8781  d3.loss_cls: 0.3195  d3.loss_mask: 0.0820  d3.loss_dice: 0.8784  d4.loss_cls: 0.3014  d4.loss_mask: 0.0870  d4.loss_dice: 0.8360  d5.loss_cls: 0.2804  d5.loss_mask: 0.0706  d5.loss_dice: 0.7931  d6.loss_cls: 0.2629  d6.loss_mask: 0.0750  d6.loss_dice: 0.8097  d7.loss_cls: 0.2605  d7.loss_mask: 0.0711  d7.loss_dice: 0.7999  d8.loss_cls: 0.2539  d8.loss_mask: 0.0702  d8.loss_dice: 0.7765  d9.loss_cls: 0.2660  d9.loss_mask: 0.0674  d9.loss_dice: 0.7808
2025/06/30 14:24:10 - mmengine - INFO - Epoch(train) [4][650/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:44:00  time: 2.2949  data_time: 0.0046  memory: 9655  grad_norm: 3.2709  loss: 16.3757  loss_cls: 0.2600  loss_mask: 0.0924  loss_dice: 0.7802  loss_poly_reg: 1.1533  loss_poly_ang: 0.8198  loss_dp: 0.3841  d0.loss_cls: 0.9338  d0.loss_mask: 0.1248  d0.loss_dice: 0.9251  d1.loss_cls: 0.3784  d1.loss_mask: 0.1062  d1.loss_dice: 0.8403  d2.loss_cls: 0.3128  d2.loss_mask: 0.1104  d2.loss_dice: 0.8568  d3.loss_cls: 0.3486  d3.loss_mask: 0.0990  d3.loss_dice: 0.8830  d4.loss_cls: 0.3139  d4.loss_mask: 0.1024  d4.loss_dice: 0.8600  d5.loss_cls: 0.2746  d5.loss_mask: 0.0964  d5.loss_dice: 0.7917  d6.loss_cls: 0.2771  d6.loss_mask: 0.0938  d6.loss_dice: 0.7821  d7.loss_cls: 0.2481  d7.loss_mask: 0.0938  d7.loss_dice: 0.7822  d8.loss_cls: 0.2614  d8.loss_mask: 0.0946  d8.loss_dice: 0.7617  d9.loss_cls: 0.2600  d9.loss_mask: 0.0924  d9.loss_dice: 0.7802
2025/06/30 14:26:14 - mmengine - INFO - Epoch(train) [4][700/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:42:05  time: 2.1858  data_time: 0.0035  memory: 7715  grad_norm: 3.0515  loss: 16.7780  loss_cls: 0.2337  loss_mask: 0.0994  loss_dice: 0.7746  loss_poly_reg: 1.2215  loss_poly_ang: 0.8138  loss_dp: 0.3824  d0.loss_cls: 0.9040  d0.loss_mask: 0.1505  d0.loss_dice: 1.0291  d1.loss_cls: 0.3303  d1.loss_mask: 0.1368  d1.loss_dice: 0.9267  d2.loss_cls: 0.3123  d2.loss_mask: 0.1383  d2.loss_dice: 0.9387  d3.loss_cls: 0.2592  d3.loss_mask: 0.1291  d3.loss_dice: 0.9039  d4.loss_cls: 0.2914  d4.loss_mask: 0.1131  d4.loss_dice: 0.8733  d5.loss_cls: 0.2390  d5.loss_mask: 0.1308  d5.loss_dice: 0.8778  d6.loss_cls: 0.2543  d6.loss_mask: 0.1147  d6.loss_dice: 0.7807  d7.loss_cls: 0.2385  d7.loss_mask: 0.0987  d7.loss_dice: 0.7807  d8.loss_cls: 0.2424  d8.loss_mask: 0.1047  d8.loss_dice: 0.8456  d9.loss_cls: 0.2337  d9.loss_mask: 0.0994  d9.loss_dice: 0.7746
2025/06/30 14:28:16 - mmengine - INFO - Epoch(train) [4][750/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:40:07  time: 1.7859  data_time: 0.0005  memory: 7504  grad_norm: 2.9061  loss: 14.9799  loss_cls: 0.1807  loss_mask: 0.0631  loss_dice: 0.7774  loss_poly_reg: 1.0608  loss_poly_ang: 0.6282  loss_dp: 0.3228  d0.loss_cls: 0.8550  d0.loss_mask: 0.0852  d0.loss_dice: 0.9075  d1.loss_cls: 0.3034  d1.loss_mask: 0.0844  d1.loss_dice: 0.9241  d2.loss_cls: 0.2614  d2.loss_mask: 0.0708  d2.loss_dice: 0.8582  d3.loss_cls: 0.2594  d3.loss_mask: 0.0781  d3.loss_dice: 0.8502  d4.loss_cls: 0.2299  d4.loss_mask: 0.0841  d4.loss_dice: 0.7991  d5.loss_cls: 0.2119  d5.loss_mask: 0.0721  d5.loss_dice: 0.8418  d6.loss_cls: 0.1910  d6.loss_mask: 0.0654  d6.loss_dice: 0.8036  d7.loss_cls: 0.1759  d7.loss_mask: 0.0666  d7.loss_dice: 0.7744  d8.loss_cls: 0.1754  d8.loss_mask: 0.0659  d8.loss_dice: 0.8306  d9.loss_cls: 0.1807  d9.loss_mask: 0.0631  d9.loss_dice: 0.7774  loss_poly_iou: 0.0000
2025/06/30 14:30:15 - mmengine - INFO - Epoch(train) [4][800/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:38:01  time: 2.6189  data_time: 0.0025  memory: 6274  grad_norm: 2.1313  loss: 18.7287  loss_cls: 0.2827  loss_mask: 0.0998  loss_dice: 0.9011  loss_poly_reg: 1.1741  loss_poly_ang: 0.9690  loss_dp: 0.3858  d0.loss_cls: 1.1020  d0.loss_mask: 0.1246  d0.loss_dice: 1.0516  d1.loss_cls: 0.4993  d1.loss_mask: 0.1011  d1.loss_dice: 0.9926  d2.loss_cls: 0.4419  d2.loss_mask: 0.0995  d2.loss_dice: 0.9619  d3.loss_cls: 0.4177  d3.loss_mask: 0.1082  d3.loss_dice: 0.9829  d4.loss_cls: 0.3512  d4.loss_mask: 0.1054  d4.loss_dice: 0.9796  d5.loss_cls: 0.2978  d5.loss_mask: 0.1045  d5.loss_dice: 0.9872  d6.loss_cls: 0.2737  d6.loss_mask: 0.1007  d6.loss_dice: 0.9658  d7.loss_cls: 0.2638  d7.loss_mask: 0.0998  d7.loss_dice: 0.9206  d8.loss_cls: 0.2825  d8.loss_mask: 0.0987  d8.loss_dice: 0.9180  d9.loss_cls: 0.2827  d9.loss_mask: 0.0998  d9.loss_dice: 0.9011  loss_poly_iou: 0.0000
2025/06/30 14:32:05 - mmengine - INFO - Epoch(train) [4][850/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:35:35  time: 2.7144  data_time: 0.0018  memory: 6867  grad_norm: 2.8864  loss: 16.6189  loss_cls: 0.2354  loss_mask: 0.0678  loss_dice: 0.8003  loss_poly_reg: 1.1568  loss_poly_ang: 0.9453  loss_dp: 0.3838  d0.loss_cls: 1.0916  d0.loss_mask: 0.0969  d0.loss_dice: 0.9768  d1.loss_cls: 0.4459  d1.loss_mask: 0.0742  d1.loss_dice: 0.8720  d2.loss_cls: 0.3552  d2.loss_mask: 0.0780  d2.loss_dice: 0.8754  d3.loss_cls: 0.3183  d3.loss_mask: 0.0738  d3.loss_dice: 0.8479  d4.loss_cls: 0.2922  d4.loss_mask: 0.0796  d4.loss_dice: 0.8575  d5.loss_cls: 0.2454  d5.loss_mask: 0.0725  d5.loss_dice: 0.8662  d6.loss_cls: 0.2298  d6.loss_mask: 0.0711  d6.loss_dice: 0.8471  d7.loss_cls: 0.2194  d7.loss_mask: 0.0689  d7.loss_dice: 0.8225  d8.loss_cls: 0.2341  d8.loss_mask: 0.0709  d8.loss_dice: 0.8426  d9.loss_cls: 0.2354  d9.loss_mask: 0.0678  d9.loss_dice: 0.8003  loss_poly_iou: 0.0000
2025/06/30 14:33:40 - mmengine - INFO - Epoch(train) [4][900/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:32:40  time: 1.5956  data_time: 0.0012  memory: 4609  grad_norm: 3.1008  loss: 15.4908  loss_cls: 0.2029  loss_mask: 0.0783  loss_dice: 0.7813  loss_poly_reg: 1.1586  loss_poly_ang: 0.8151  loss_dp: 0.3814  d0.loss_cls: 0.6936  d0.loss_mask: 0.1122  d0.loss_dice: 0.9304  d1.loss_cls: 0.3267  d1.loss_mask: 0.0839  d1.loss_dice: 0.8663  d2.loss_cls: 0.2760  d2.loss_mask: 0.0926  d2.loss_dice: 0.9304  d3.loss_cls: 0.2976  d3.loss_mask: 0.0842  d3.loss_dice: 0.7974  d4.loss_cls: 0.2354  d4.loss_mask: 0.0817  d4.loss_dice: 0.8750  d5.loss_cls: 0.1983  d5.loss_mask: 0.0834  d5.loss_dice: 0.8374  d6.loss_cls: 0.2099  d6.loss_mask: 0.0773  d6.loss_dice: 0.7992  d7.loss_cls: 0.2021  d7.loss_mask: 0.0761  d7.loss_dice: 0.7747  d8.loss_cls: 0.2010  d8.loss_mask: 0.0777  d8.loss_dice: 0.7903  d9.loss_cls: 0.2029  d9.loss_mask: 0.0783  d9.loss_dice: 0.7813  loss_poly_iou: 0.0000
2025/06/30 14:35:37 - mmengine - INFO - Exp name: gcp_r50_kazgisa-kostanai_20250630_130850
2025/06/30 14:35:38 - mmengine - INFO - Saving checkpoint at 4 epochs
2025/06/30 14:44:29 - mmengine - INFO - Epoch(val) [4][50/53]    eta: 0:00:31  time: 8.7749  data_time: 0.5403  memory: 8421  
2025/06/30 14:44:48 - mmengine - INFO - Evaluating segm...
2025/06/30 14:44:56 - mmengine - INFO - segm_mAP_copypaste: 0.531 0.786 0.590 0.313 0.685 0.830
2025/06/30 14:45:07 - mmengine - INFO - mta: 38.073654223556694
2025/06/30 14:45:12 - mmengine - INFO - iou: 0.7950443470289801, c_iou: 0.6756953837878892, N_ratio: 0.8016034047607264
2025/06/30 14:45:12 - mmengine - INFO - Epoch(val) [4][53/53]    coco/segm_mAP: 0.5310  coco/segm_mAP_50: 0.7860  coco/segm_mAP_75: 0.5900  coco/segm_mAP_s: 0.3130  coco/segm_mAP_m: 0.6850  coco/segm_mAP_l: 0.8300  coco/mta: 38.0737  coco/iou: 0.7950  coco/c_iou: 0.6757  coco/N_ratio: 0.8016  data_time: 0.6771  time: 10.1254
2025/06/30 14:47:08 - mmengine - INFO - Epoch(train) [5][ 50/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:29:23  time: 1.8664  data_time: 0.0049  memory: 6807  grad_norm: 3.3966  loss: 13.1796  loss_cls: 0.1883  loss_mask: 0.0588  loss_dice: 0.6684  loss_poly_reg: 1.2882  loss_poly_ang: 0.9775  loss_dp: 0.3842  d0.loss_cls: 0.8046  d0.loss_mask: 0.0840  d0.loss_dice: 0.7383  d1.loss_cls: 0.2989  d1.loss_mask: 0.0694  d1.loss_dice: 0.7120  d2.loss_cls: 0.2579  d2.loss_mask: 0.0684  d2.loss_dice: 0.6797  d3.loss_cls: 0.2584  d3.loss_mask: 0.0684  d3.loss_dice: 0.6443  d4.loss_cls: 0.2117  d4.loss_mask: 0.0676  d4.loss_dice: 0.6339  d5.loss_cls: 0.1956  d5.loss_mask: 0.0626  d5.loss_dice: 0.6606  d6.loss_cls: 0.2029  d6.loss_mask: 0.0637  d6.loss_dice: 0.6199  d7.loss_cls: 0.1922  d7.loss_mask: 0.0584  d7.loss_dice: 0.6271  d8.loss_cls: 0.1888  d8.loss_mask: 0.0579  d8.loss_dice: 0.5729  d9.loss_cls: 0.1883  d9.loss_mask: 0.0588  d9.loss_dice: 0.6684  loss_poly_iou: 0.0000
2025/06/30 14:49:21 - mmengine - INFO - Epoch(train) [5][100/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:27:45  time: 3.1567  data_time: 0.0042  memory: 7451  grad_norm: 2.7712  loss: 16.1697  loss_cls: 0.2847  loss_mask: 0.0638  loss_dice: 0.7000  loss_poly_reg: 1.0075  loss_poly_ang: 0.8472  loss_dp: 0.3597  d0.loss_cls: 1.1891  d0.loss_mask: 0.1053  d0.loss_dice: 0.9721  d1.loss_cls: 0.4958  d1.loss_mask: 0.0778  d1.loss_dice: 0.8468  d2.loss_cls: 0.4506  d2.loss_mask: 0.0778  d2.loss_dice: 0.8153  d3.loss_cls: 0.3926  d3.loss_mask: 0.0770  d3.loss_dice: 0.7593  d4.loss_cls: 0.3284  d4.loss_mask: 0.0729  d4.loss_dice: 0.7739  d5.loss_cls: 0.3106  d5.loss_mask: 0.0709  d5.loss_dice: 0.7839  d6.loss_cls: 0.3093  d6.loss_mask: 0.0665  d6.loss_dice: 0.7249  d7.loss_cls: 0.2963  d7.loss_mask: 0.0658  d7.loss_dice: 0.7223  d8.loss_cls: 0.2892  d8.loss_mask: 0.0651  d8.loss_dice: 0.7186  d9.loss_cls: 0.2847  d9.loss_mask: 0.0638  d9.loss_dice: 0.7000  loss_poly_iou: 0.0000
2025/06/30 14:51:31 - mmengine - INFO - Epoch(train) [5][150/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:25:59  time: 1.3008  data_time: 0.0030  memory: 6221  grad_norm: 3.4971  loss: 13.7875  loss_cls: 0.1530  loss_mask: 0.0826  loss_dice: 0.7070  loss_poly_reg: 0.9560  loss_poly_ang: 0.6572  loss_dp: 0.3610  d0.loss_cls: 0.6495  d0.loss_mask: 0.1209  d0.loss_dice: 0.8799  d1.loss_cls: 0.2829  d1.loss_mask: 0.0893  d1.loss_dice: 0.7469  d2.loss_cls: 0.2004  d2.loss_mask: 0.0895  d2.loss_dice: 0.8405  d3.loss_cls: 0.2331  d3.loss_mask: 0.0769  d3.loss_dice: 0.6849  d4.loss_cls: 0.2202  d4.loss_mask: 0.0923  d4.loss_dice: 0.7620  d5.loss_cls: 0.1707  d5.loss_mask: 0.0844  d5.loss_dice: 0.7422  d6.loss_cls: 0.1511  d6.loss_mask: 0.0895  d6.loss_dice: 0.7040  d7.loss_cls: 0.1539  d7.loss_mask: 0.0868  d7.loss_dice: 0.7443  d8.loss_cls: 0.1487  d8.loss_mask: 0.0856  d8.loss_dice: 0.7975  d9.loss_cls: 0.1530  d9.loss_mask: 0.0826  d9.loss_dice: 0.7070  loss_poly_iou: 0.0000
2025/06/30 14:53:27 - mmengine - INFO - Epoch(train) [5][200/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:23:50  time: 1.9938  data_time: 0.0019  memory: 6694  grad_norm: 3.1039  loss: 13.4532  loss_cls: 0.1665  loss_mask: 0.0704  loss_dice: 0.6049  loss_poly_reg: 1.0934  loss_poly_ang: 0.9679  loss_dp: 0.3883  d0.loss_cls: 0.8229  d0.loss_mask: 0.0901  d0.loss_dice: 0.7904  d1.loss_cls: 0.3269  d1.loss_mask: 0.0848  d1.loss_dice: 0.7274  d2.loss_cls: 0.2846  d2.loss_mask: 0.0856  d2.loss_dice: 0.7126  d3.loss_cls: 0.2845  d3.loss_mask: 0.0749  d3.loss_dice: 0.5967  d4.loss_cls: 0.2097  d4.loss_mask: 0.0754  d4.loss_dice: 0.6562  d5.loss_cls: 0.1764  d5.loss_mask: 0.0740  d5.loss_dice: 0.6302  d6.loss_cls: 0.1674  d6.loss_mask: 0.0725  d6.loss_dice: 0.6256  d7.loss_cls: 0.1590  d7.loss_mask: 0.0727  d7.loss_dice: 0.6534  d8.loss_cls: 0.1753  d8.loss_mask: 0.0726  d8.loss_dice: 0.6184  d9.loss_cls: 0.1665  d9.loss_mask: 0.0704  d9.loss_dice: 0.6049  loss_poly_iou: 0.0000
2025/06/30 14:54:44 - mmengine - INFO - Exp name: gcp_r50_kazgisa-kostanai_20250630_130850
2025/06/30 14:55:40 - mmengine - INFO - Epoch(train) [5][250/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:22:10  time: 2.8009  data_time: 0.0005  memory: 8039  grad_norm: 3.2131  loss: 15.3569  loss_cls: 0.1931  loss_mask: 0.0760  loss_dice: 0.7623  loss_poly_reg: 0.9189  loss_poly_ang: 0.7986  loss_dp: 0.3615  d0.loss_cls: 1.0029  d0.loss_mask: 0.1011  d0.loss_dice: 0.9102  d1.loss_cls: 0.3624  d1.loss_mask: 0.0938  d1.loss_dice: 0.8671  d2.loss_cls: 0.3562  d2.loss_mask: 0.0973  d2.loss_dice: 0.8419  d3.loss_cls: 0.2609  d3.loss_mask: 0.0908  d3.loss_dice: 0.8446  d4.loss_cls: 0.2694  d4.loss_mask: 0.0955  d4.loss_dice: 0.8044  d5.loss_cls: 0.2413  d5.loss_mask: 0.0942  d5.loss_dice: 0.7958  d6.loss_cls: 0.2162  d6.loss_mask: 0.0774  d6.loss_dice: 0.7477  d7.loss_cls: 0.2054  d7.loss_mask: 0.0771  d7.loss_dice: 0.7428  d8.loss_cls: 0.1910  d8.loss_mask: 0.0775  d8.loss_dice: 0.7501  d9.loss_cls: 0.1931  d9.loss_mask: 0.0760  d9.loss_dice: 0.7623  loss_poly_iou: 0.0000
2025/06/30 14:57:37 - mmengine - INFO - Epoch(train) [5][300/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:20:01  time: 2.4377  data_time: 0.0014  memory: 7091  grad_norm: 2.8765  loss: 16.2893  loss_cls: 0.2303  loss_mask: 0.0827  loss_dice: 0.7911  loss_poly_reg: 1.1514  loss_poly_ang: 0.7938  loss_dp: 0.3779  d0.loss_cls: 1.0380  d0.loss_mask: 0.1111  d0.loss_dice: 0.9321  d1.loss_cls: 0.4063  d1.loss_mask: 0.1035  d1.loss_dice: 0.9013  d2.loss_cls: 0.3730  d2.loss_mask: 0.0950  d2.loss_dice: 0.8598  d3.loss_cls: 0.3490  d3.loss_mask: 0.0824  d3.loss_dice: 0.8250  d4.loss_cls: 0.2972  d4.loss_mask: 0.0969  d4.loss_dice: 0.8000  d5.loss_cls: 0.2481  d5.loss_mask: 0.0855  d5.loss_dice: 0.8347  d6.loss_cls: 0.2230  d6.loss_mask: 0.0882  d6.loss_dice: 0.8155  d7.loss_cls: 0.2247  d7.loss_mask: 0.0777  d7.loss_dice: 0.7833  d8.loss_cls: 0.2332  d8.loss_mask: 0.0760  d8.loss_dice: 0.7975  d9.loss_cls: 0.2303  d9.loss_mask: 0.0827  d9.loss_dice: 0.7911  loss_poly_iou: 0.0000
2025/06/30 14:59:53 - mmengine - INFO - Epoch(train) [5][350/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:18:23  time: 2.8091  data_time: 0.0057  memory: 7504  grad_norm: 2.6225  loss: 17.8448  loss_cls: 0.2623  loss_mask: 0.1039  loss_dice: 0.8141  loss_poly_reg: 1.1301  loss_poly_ang: 1.0099  loss_dp: 0.3997  d0.loss_cls: 1.2095  d0.loss_mask: 0.1120  d0.loss_dice: 0.9726  d1.loss_cls: 0.4847  d1.loss_mask: 0.1125  d1.loss_dice: 0.9423  d2.loss_cls: 0.4374  d2.loss_mask: 0.1129  d2.loss_dice: 0.9160  d3.loss_cls: 0.3528  d3.loss_mask: 0.1055  d3.loss_dice: 0.9150  d4.loss_cls: 0.2937  d4.loss_mask: 0.1117  d4.loss_dice: 0.9212  d5.loss_cls: 0.2754  d5.loss_mask: 0.1063  d5.loss_dice: 0.8976  d6.loss_cls: 0.2341  d6.loss_mask: 0.1095  d6.loss_dice: 0.9174  d7.loss_cls: 0.2483  d7.loss_mask: 0.1060  d7.loss_dice: 0.8540  d8.loss_cls: 0.2670  d8.loss_mask: 0.1073  d8.loss_dice: 0.8219  d9.loss_cls: 0.2623  d9.loss_mask: 0.1039  d9.loss_dice: 0.8141  loss_poly_iou: 0.0000
2025/06/30 15:01:32 - mmengine - INFO - Epoch(train) [5][400/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:15:48  time: 2.6822  data_time: 0.0033  memory: 7887  grad_norm: 2.1892  loss: 15.9357  loss_cls: 0.2549  loss_mask: 0.0833  loss_dice: 0.7009  loss_poly_reg: 1.2002  loss_poly_ang: 0.9608  loss_dp: 0.3766  d0.loss_cls: 1.1038  d0.loss_mask: 0.1397  d0.loss_dice: 0.9108  d1.loss_cls: 0.3968  d1.loss_mask: 0.1053  d1.loss_dice: 0.8192  d2.loss_cls: 0.3242  d2.loss_mask: 0.1027  d2.loss_dice: 0.7938  d3.loss_cls: 0.3130  d3.loss_mask: 0.0934  d3.loss_dice: 0.7725  d4.loss_cls: 0.2694  d4.loss_mask: 0.0920  d4.loss_dice: 0.7919  d5.loss_cls: 0.2356  d5.loss_mask: 0.0946  d5.loss_dice: 0.7494  d6.loss_cls: 0.2245  d6.loss_mask: 0.0903  d6.loss_dice: 0.7486  d7.loss_cls: 0.2518  d7.loss_mask: 0.0928  d7.loss_dice: 0.7582  d8.loss_cls: 0.2293  d8.loss_mask: 0.0862  d8.loss_dice: 0.7303  d9.loss_cls: 0.2549  d9.loss_mask: 0.0833  d9.loss_dice: 0.7009  loss_poly_iou: 0.0000
2025/06/30 15:03:28 - mmengine - INFO - Epoch(train) [5][450/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:13:40  time: 2.4214  data_time: 0.0039  memory: 6510  grad_norm: 2.8686  loss: 17.8463  loss_cls: 0.2712  loss_mask: 0.0894  loss_dice: 0.8405  loss_poly_reg: 1.2627  loss_poly_ang: 0.9487  loss_dp: 0.3794  d0.loss_cls: 1.0555  d0.loss_mask: 0.1192  d0.loss_dice: 1.0057  d1.loss_cls: 0.4415  d1.loss_mask: 0.1118  d1.loss_dice: 0.9652  d2.loss_cls: 0.4155  d2.loss_mask: 0.1180  d2.loss_dice: 0.9205  d3.loss_cls: 0.3455  d3.loss_mask: 0.1079  d3.loss_dice: 0.9227  d4.loss_cls: 0.3221  d4.loss_mask: 0.1038  d4.loss_dice: 0.8736  d5.loss_cls: 0.2740  d5.loss_mask: 0.0962  d5.loss_dice: 0.9007  d6.loss_cls: 0.2638  d6.loss_mask: 0.1004  d6.loss_dice: 0.9066  d7.loss_cls: 0.2664  d7.loss_mask: 0.0920  d7.loss_dice: 0.8819  d8.loss_cls: 0.2899  d8.loss_mask: 0.0901  d8.loss_dice: 0.8629  d9.loss_cls: 0.2712  d9.loss_mask: 0.0894  d9.loss_dice: 0.8405  loss_poly_iou: 0.0000
2025/06/30 15:05:28 - mmengine - INFO - Epoch(train) [5][500/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:11:39  time: 1.5514  data_time: 0.0020  memory: 9193  grad_norm: 4.3381  loss: 11.8870  loss_cls: 0.1573  loss_mask: 0.0711  loss_dice: 0.5477  loss_poly_reg: 0.9437  loss_poly_ang: 0.7875  loss_dp: 0.3664  d0.loss_cls: 0.7393  d0.loss_mask: 0.0926  d0.loss_dice: 0.6538  d1.loss_cls: 0.2279  d1.loss_mask: 0.0864  d1.loss_dice: 0.6541  d2.loss_cls: 0.2194  d2.loss_mask: 0.0900  d2.loss_dice: 0.6402  d3.loss_cls: 0.2012  d3.loss_mask: 0.0866  d3.loss_dice: 0.5532  d4.loss_cls: 0.1795  d4.loss_mask: 0.0900  d4.loss_dice: 0.5508  d5.loss_cls: 0.1734  d5.loss_mask: 0.0921  d5.loss_dice: 0.5052  d6.loss_cls: 0.1799  d6.loss_mask: 0.0683  d6.loss_dice: 0.5978  d7.loss_cls: 0.1639  d7.loss_mask: 0.0713  d7.loss_dice: 0.5394  d8.loss_cls: 0.1574  d8.loss_mask: 0.0736  d8.loss_dice: 0.5500  d9.loss_cls: 0.1573  d9.loss_mask: 0.0711  d9.loss_dice: 0.5477  loss_poly_iou: 0.0000
2025/06/30 15:07:06 - mmengine - INFO - Epoch(train) [5][550/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:09:07  time: 1.7374  data_time: 0.0008  memory: 6297  grad_norm: 3.1751  loss: 16.2088  loss_cls: 0.2135  loss_mask: 0.0697  loss_dice: 0.8365  loss_poly_reg: 1.1559  loss_poly_ang: 0.8347  loss_dp: 0.3677  d0.loss_cls: 0.7530  d0.loss_mask: 0.1209  d0.loss_dice: 0.9157  d1.loss_cls: 0.3264  d1.loss_mask: 0.0797  d1.loss_dice: 0.9137  d2.loss_cls: 0.3233  d2.loss_mask: 0.0796  d2.loss_dice: 0.8509  d3.loss_cls: 0.3158  d3.loss_mask: 0.0703  d3.loss_dice: 0.8999  d4.loss_cls: 0.2697  d4.loss_mask: 0.0767  d4.loss_dice: 0.9594  d5.loss_cls: 0.2146  d5.loss_mask: 0.0740  d5.loss_dice: 0.8580  d6.loss_cls: 0.2570  d6.loss_mask: 0.0718  d6.loss_dice: 0.8665  d7.loss_cls: 0.2252  d7.loss_mask: 0.0704  d7.loss_dice: 0.8743  d8.loss_cls: 0.2344  d8.loss_mask: 0.0709  d8.loss_dice: 0.8391  d9.loss_cls: 0.2135  d9.loss_mask: 0.0697  d9.loss_dice: 0.8365  loss_poly_iou: 0.0000
2025/06/30 15:09:08 - mmengine - INFO - Epoch(train) [5][600/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:07:09  time: 2.5146  data_time: 0.0022  memory: 8158  grad_norm: 2.8866  loss: 17.3548  loss_cls: 0.3173  loss_mask: 0.0757  loss_dice: 0.8170  loss_poly_reg: 1.0780  loss_poly_ang: 0.8653  loss_dp: 0.3838  d0.loss_cls: 1.1631  d0.loss_mask: 0.1008  d0.loss_dice: 0.9885  d1.loss_cls: 0.5001  d1.loss_mask: 0.0883  d1.loss_dice: 0.8852  d2.loss_cls: 0.4700  d2.loss_mask: 0.0912  d2.loss_dice: 0.9139  d3.loss_cls: 0.3890  d3.loss_mask: 0.0849  d3.loss_dice: 0.8452  d4.loss_cls: 0.3511  d4.loss_mask: 0.0877  d4.loss_dice: 0.8723  d5.loss_cls: 0.3107  d5.loss_mask: 0.0890  d5.loss_dice: 0.8547  d6.loss_cls: 0.2925  d6.loss_mask: 0.0788  d6.loss_dice: 0.8183  d7.loss_cls: 0.2821  d7.loss_mask: 0.0776  d7.loss_dice: 0.7895  d8.loss_cls: 0.3238  d8.loss_mask: 0.0758  d8.loss_dice: 0.7836  d9.loss_cls: 0.3173  d9.loss_mask: 0.0757  d9.loss_dice: 0.8170  loss_poly_iou: 0.0000
2025/06/30 15:11:21 - mmengine - INFO - Epoch(train) [5][650/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:05:24  time: 2.7952  data_time: 0.0025  memory: 8966  grad_norm: 3.0875  loss: 17.8172  loss_cls: 0.2889  loss_mask: 0.0590  loss_dice: 0.8459  loss_poly_reg: 1.0695  loss_poly_ang: 0.9239  loss_dp: 0.3789  d0.loss_cls: 1.2628  d0.loss_mask: 0.1003  d0.loss_dice: 1.0131  d1.loss_cls: 0.4898  d1.loss_mask: 0.0797  d1.loss_dice: 1.0029  d2.loss_cls: 0.4414  d2.loss_mask: 0.0754  d2.loss_dice: 0.9621  d3.loss_cls: 0.4274  d3.loss_mask: 0.0697  d3.loss_dice: 0.8928  d4.loss_cls: 0.3970  d4.loss_mask: 0.0649  d4.loss_dice: 0.8350  d5.loss_cls: 0.3408  d5.loss_mask: 0.0628  d5.loss_dice: 0.8506  d6.loss_cls: 0.2928  d6.loss_mask: 0.0635  d6.loss_dice: 0.8681  d7.loss_cls: 0.2849  d7.loss_mask: 0.0632  d7.loss_dice: 0.8658  d8.loss_cls: 0.3159  d8.loss_mask: 0.0625  d8.loss_dice: 0.8723  d9.loss_cls: 0.2889  d9.loss_mask: 0.0590  d9.loss_dice: 0.8459  loss_poly_iou: 0.0000
2025/06/30 15:13:10 - mmengine - INFO - Epoch(train) [5][700/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:03:10  time: 3.3250  data_time: 0.0029  memory: 6972  grad_norm: 2.6281  loss: 17.2425  loss_cls: 0.2704  loss_mask: 0.0739  loss_dice: 0.8030  loss_poly_reg: 1.1142  loss_poly_ang: 0.9170  loss_dp: 0.3681  d0.loss_cls: 1.1967  d0.loss_mask: 0.1185  d0.loss_dice: 0.9843  d1.loss_cls: 0.4548  d1.loss_mask: 0.0978  d1.loss_dice: 0.9466  d2.loss_cls: 0.4353  d2.loss_mask: 0.0967  d2.loss_dice: 0.8915  d3.loss_cls: 0.3712  d3.loss_mask: 0.0849  d3.loss_dice: 0.8590  d4.loss_cls: 0.3047  d4.loss_mask: 0.0838  d4.loss_dice: 0.8933  d5.loss_cls: 0.2764  d5.loss_mask: 0.0840  d5.loss_dice: 0.8627  d6.loss_cls: 0.2707  d6.loss_mask: 0.0803  d6.loss_dice: 0.8085  d7.loss_cls: 0.2315  d7.loss_mask: 0.0774  d7.loss_dice: 0.8453  d8.loss_cls: 0.2482  d8.loss_mask: 0.0770  d8.loss_dice: 0.8674  d9.loss_cls: 0.2704  d9.loss_mask: 0.0739  d9.loss_dice: 0.8030  loss_poly_iou: 0.0000
2025/06/30 15:15:39 - mmengine - INFO - Epoch(train) [5][750/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:01:42  time: 2.8483  data_time: 0.0011  memory: 7128  grad_norm: 2.5066  loss: 16.9420  loss_cls: 0.2461  loss_mask: 0.0955  loss_dice: 0.7444  loss_poly_reg: 1.1352  loss_poly_ang: 0.7639  loss_dp: 0.3585  d0.loss_cls: 1.2987  d0.loss_mask: 0.1273  d0.loss_dice: 1.0004  d1.loss_cls: 0.4562  d1.loss_mask: 0.0996  d1.loss_dice: 0.9305  d2.loss_cls: 0.4480  d2.loss_mask: 0.1148  d2.loss_dice: 0.8905  d3.loss_cls: 0.3381  d3.loss_mask: 0.0940  d3.loss_dice: 0.8533  d4.loss_cls: 0.2842  d4.loss_mask: 0.1090  d4.loss_dice: 0.8493  d5.loss_cls: 0.2558  d5.loss_mask: 0.1040  d5.loss_dice: 0.8459  d6.loss_cls: 0.2364  d6.loss_mask: 0.0990  d6.loss_dice: 0.7771  d7.loss_cls: 0.2300  d7.loss_mask: 0.0980  d7.loss_dice: 0.7857  d8.loss_cls: 0.2685  d8.loss_mask: 0.0977  d8.loss_dice: 0.8206  d9.loss_cls: 0.2461  d9.loss_mask: 0.0955  d9.loss_dice: 0.7444  loss_poly_iou: 0.0000
2025/06/30 15:17:27 - mmengine - INFO - Epoch(train) [5][800/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:59:27  time: 2.5716  data_time: 0.0022  memory: 6230  grad_norm: 3.0971  loss: 15.7293  loss_cls: 0.2365  loss_mask: 0.0659  loss_dice: 0.6952  loss_poly_reg: 1.0779  loss_poly_ang: 0.8716  loss_dp: 0.3739  d0.loss_cls: 1.0506  d0.loss_mask: 0.1183  d0.loss_dice: 0.9286  d1.loss_cls: 0.4390  d1.loss_mask: 0.0787  d1.loss_dice: 0.8088  d2.loss_cls: 0.4187  d2.loss_mask: 0.0801  d2.loss_dice: 0.7990  d3.loss_cls: 0.3509  d3.loss_mask: 0.0737  d3.loss_dice: 0.8168  d4.loss_cls: 0.2841  d4.loss_mask: 0.0784  d4.loss_dice: 0.7785  d5.loss_cls: 0.2832  d5.loss_mask: 0.0694  d5.loss_dice: 0.7362  d6.loss_cls: 0.2691  d6.loss_mask: 0.0671  d6.loss_dice: 0.7124  d7.loss_cls: 0.2594  d7.loss_mask: 0.0671  d7.loss_dice: 0.7576  d8.loss_cls: 0.2711  d8.loss_mask: 0.0678  d8.loss_dice: 0.7462  d9.loss_cls: 0.2365  d9.loss_mask: 0.0659  d9.loss_dice: 0.6952  loss_poly_iou: 0.0000
2025/06/30 15:19:11 - mmengine - INFO - Epoch(train) [5][850/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:57:08  time: 1.8565  data_time: 0.0021  memory: 7672  grad_norm: 3.3404  loss: 17.2299  loss_cls: 0.2201  loss_mask: 0.0887  loss_dice: 0.8558  loss_poly_reg: 1.1863  loss_poly_ang: 0.7357  loss_dp: 0.3755  d0.loss_cls: 0.8926  d0.loss_mask: 0.1667  d0.loss_dice: 0.9901  d1.loss_cls: 0.4199  d1.loss_mask: 0.0989  d1.loss_dice: 0.9610  d2.loss_cls: 0.3902  d2.loss_mask: 0.0958  d2.loss_dice: 0.9315  d3.loss_cls: 0.3205  d3.loss_mask: 0.1093  d3.loss_dice: 0.9764  d4.loss_cls: 0.2975  d4.loss_mask: 0.1054  d4.loss_dice: 0.9286  d5.loss_cls: 0.2209  d5.loss_mask: 0.0959  d5.loss_dice: 0.9306  d6.loss_cls: 0.2325  d6.loss_mask: 0.0915  d6.loss_dice: 0.8676  d7.loss_cls: 0.2319  d7.loss_mask: 0.0934  d7.loss_dice: 0.9613  d8.loss_cls: 0.2354  d8.loss_mask: 0.0875  d8.loss_dice: 0.8705  d9.loss_cls: 0.2201  d9.loss_mask: 0.0887  d9.loss_dice: 0.8558  loss_poly_iou: 0.0000
2025/06/30 15:21:12 - mmengine - INFO - Epoch(train) [5][900/942]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:55:08  time: 2.5552  data_time: 0.0044  memory: 5967  grad_norm: 2.8158  loss: 14.8035  loss_cls: 0.2413  loss_mask: 0.1180  loss_dice: 0.6196  loss_poly_reg: 1.0141  loss_poly_ang: 0.7974  loss_dp: 0.3820  d0.loss_cls: 0.9296  d0.loss_mask: 0.1644  d0.loss_dice: 0.7921  d1.loss_cls: 0.3781  d1.loss_mask: 0.1457  d1.loss_dice: 0.7035  d2.loss_cls: 0.3634  d2.loss_mask: 0.1426  d2.loss_dice: 0.6733  d3.loss_cls: 0.3200  d3.loss_mask: 0.1299  d3.loss_dice: 0.6836  d4.loss_cls: 0.2725  d4.loss_mask: 0.1365  d4.loss_dice: 0.7187  d5.loss_cls: 0.2604  d5.loss_mask: 0.1396  d5.loss_dice: 0.6717  d6.loss_cls: 0.2143  d6.loss_mask: 0.1244  d6.loss_dice: 0.6531  d7.loss_cls: 0.2439  d7.loss_mask: 0.1225  d7.loss_dice: 0.6616  d8.loss_cls: 0.2338  d8.loss_mask: 0.1110  d8.loss_dice: 0.6621  d9.loss_cls: 0.2413  d9.loss_mask: 0.1180  d9.loss_dice: 0.6196  loss_poly_iou: 0.0000
2025/06/30 15:23:15 - mmengine - INFO - Exp name: gcp_r50_kazgisa-kostanai_20250630_130850
2025/06/30 15:23:16 - mmengine - INFO - Saving checkpoint at 5 epochs
